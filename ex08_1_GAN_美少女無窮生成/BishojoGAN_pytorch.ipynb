{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 美少女無窮生成 pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 測試於pytorch 1.0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "生成式對抗模型可以說是近期深度學習中最有創意的發想，他利用一個生成器負責作假圖片，用一個鑑別器負責識別輸入的照片是真還是假? 生成器從來沒看過真照片，但是他必須從鑑別器的反應來推斷逐步修正，一直到可以生成以假亂真的照片。這也意味著，生成模型將照片裡的所有知識細節都已經學習到了。在這個實作中，我去AKB48以及早安少女的Hellow家族官網，以及透過爬蟲利用關鍵字美少女收集了大量日系美少女頭像，我們可以透過GAN來無窮生成美少女嗎??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![md_images](../Images/gan.jpg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在本次實例中，我們將會使用RaSGAN，這篇論文最大的特色就是提出了在傳統GAN中只關注真假，卻沒有注意到真與假之間的相對差異，舉例來說，下圖第一層是很正常的麵包與柯基，因為差異很大所以沒有問題，但是當真實圖片出現了像麵包的柯基屁屁，或者是像狗的麵包，這時候真與假之間的相對差異就沒那麼明顯了，所以這篇論文中在損失函數的部分，將真實圖片的鑑別機率要減去假圖片平均的鑑別機率後再做一次sigmoid將差異拉開至0~1之間，反之亦然，這樣就可以緩解圖片中出現看起來沒那麼真的問題。會採用RaSGAN的原因在於這是我目前使用過最快就能產生有效影像的GAN方法。\n",
    "\n",
    "The relativistic discriminator: a key element missing from standard GAN\n",
    "https://arxiv.org/pdf/1807.00734.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![md_images](../Images/rasgan.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "首先我們引用相關的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import glob\n",
    "import pylab\n",
    "import cv2\n",
    "import math\n",
    "import string\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 是否使用GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在此定義圖片大小為64*64，以及設計產生噪音(長度為100)的函數作為生成模型的輸入值以及讀取所有的圖片(都已經縮放為128*128，目前僅提供給付費學員)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_h = 64\n",
    "img_w = 64\n",
    "img_c = 3\n",
    "\n",
    "#產生高斯分布噪音\n",
    "def noise_sample(num_samples, g_input_dim=100):\n",
    "    return np.random.normal(\n",
    "        size=[num_samples, g_input_dim]\n",
    "    ).astype(np.float32)\n",
    "\n",
    "s =  glob.glob('../Data/ex08_train/resized_images/' + '*.jpg')\n",
    "print('{0}張圖片'.format(len(s)))\n",
    "random.shuffle(s)\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "定義讀取圖片的reader以及讀取圖片時數劇增強的操作函數，請注意，輸入圖片應該利用(im-127.5)/127.5調整成-1~+1之間，而不要是除以255.變成0~+1之間。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#圖片轉向量\n",
    "def img2array(img: Image):\n",
    "    arr = np.array(img).astype(np.float32)\n",
    "    arr=arr.transpose(2, 0, 1) #轉成CHW\n",
    "    arr=np.ascontiguousarray(arr)\n",
    "    return arr[::-1] #顏色排序為BGR\n",
    "\n",
    "#向量轉圖片\n",
    "def array2img(arr: np.ndarray):\n",
    "    arr =arr[::-1]#轉成RGB\n",
    "    sanitized_img = np.maximum(0, np.minimum(255, np.transpose(arr, (1, 2, 0))))#轉成HWC\n",
    "    img = Image.fromarray(sanitized_img.astype(np.uint8))\n",
    "    return img\n",
    "\n",
    "#隨機加入標準常態分配的噪聲\n",
    "def add_noise(image):\n",
    "    noise=np.random.standard_normal(image.shape)*np.random.choice(np.arange(-5,5))\n",
    "    image=np.clip(image+noise,0,255)\n",
    "    return image\n",
    "\n",
    "#調整明暗\n",
    "def adjust_gamma(image,gamma=1.2):\n",
    "    image = image.transpose([1, 2, 0])\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    cv2.LUT(image.astype(np.uint8), table)\n",
    "    image = image.transpose([2, 0, 1])\n",
    "    return image\n",
    "\n",
    "#模糊\n",
    "def adjust_blur(image):\n",
    "    image = image.transpose([1, 2, 0])\n",
    "    image=cv2.blur(image, (3, 3))\n",
    "    image = image.transpose([2, 0, 1])\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "def next_minibatch(minibatch_size,is_train=True):\n",
    "    global s, idx\n",
    "    features = []\n",
    "    while len(features) < minibatch_size:\n",
    "        try:\n",
    "            im = Image.open(s[idx]).convert('RGB').resize((64,64),Image.ANTIALIAS) \n",
    "            im = img2array(im).astype(np.float32)\n",
    "            #加入數據增強以確保圖片輸入的多元性，避免鑑別模型記憶樣本過擬合\n",
    "            if is_train:\n",
    "                im=add_noise(im)\n",
    "                if random.randint(0,10)%2==0:\n",
    "                    gamma=np.random.choice(np.arange(0.6, 1.5, 0.05))\n",
    "                    img=adjust_gamma(im,gamma)\n",
    "                if random.randint(0,10)%5<=1:\n",
    "                    im=adjust_blur(im)\n",
    "                    \n",
    "            features.append((im-127.5)/127.5)\n",
    "        except OSError as e:\n",
    "            print(e)\n",
    "        idx += 1\n",
    "        if idx >= len(s):\n",
    "            random.shuffle(s)\n",
    "            idx = 0\n",
    "    return np.asarray(features).astype(np.float32)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "為了能夠讓生成的效果更好，我們在網絡結構上面做了一些設計。首先是在生成器與鑑別器中都同時加入了自注意力(self attention)機制，顧名思義，自注意力的注意力權值分布表示衍生自卷積本身的transpose相乘(相乘前將卷積reshape成(h*w,c))，因此它可以處理圖片每個對應位置的重要性權重分布。加入自注意力機制，生成器可以處理周遭以及遠端之間的相互依賴，以確保生成細節的合理性，而鑑別器也能夠更好的根據全局的圖像結構來做更精確的集合約束判定。\n",
    "\n",
    "在自注意力機制中，最後透過gamma這個參數來與原輸入相加(x = gamma * o + x)，請注意gamma的初始值應該要為零，這樣確保第一次執行時是沒有納入注意力機制的，應該要在訓練後才逐步增加gamma的大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![md_images](../Images/self_attention_module.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#自注意力機制\n",
    "class Self_Attn(nn.Module):\n",
    "    \"\"\" Self attention Layer\"\"\"\n",
    "    def __init__(self,in_dim,activation):\n",
    "        super(Self_Attn,self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        self.activation = activation\n",
    "        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
    "        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
    "        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax  = nn.Softmax(dim=-1) #\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X W X H)\n",
    "            returns :\n",
    "                out : self attention value + input feature \n",
    "                attention: B X N X N (N is Width*Height)\n",
    "        \"\"\"\n",
    "        m_batchsize,C,width ,height = x.size()\n",
    "        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\n",
    "        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\n",
    "        energy =  torch.bmm(proj_query,proj_key) # transpose check\n",
    "        attention = self.softmax(energy) # BX (N) X (N) \n",
    "        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n",
    "        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n",
    "        out = out.view(m_batchsize,C,width,height)\n",
    "        out = self.gamma*out + x\n",
    "        return out,attention\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "至於生成器中的圖片上採樣，強烈建議使用pixel shuffle的方式，以取代掉transpose convolution，主要是可以減少訓練參數，避免生成器過度弱勢。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![md_images](../Images/pixelshuffle.jpg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "設計生成器以及鑑別器的結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv5x5(in_planes, out_planes, stride=1,dilation=1,padding=2):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=5, stride=stride,padding=padding, dilation =dilation, bias=False)\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1,dilation=1,padding=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,padding=padding,dilation =dilation, bias=False)\n",
    "def conv1x1(in_planes, out_planes, stride=1,dilation=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,dilation =dilation, bias=False)\n",
    "\n",
    "class resnet_basic(nn.Module):\n",
    "    def __init__(self, inplanes, stride=1):\n",
    "        super(resnet_basic, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, inplanes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 =conv3x3(inplanes, inplanes, stride)\n",
    "        self.bn2 = nn.BatchNorm2d(inplanes)\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = residual+0.2*out\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "def weights_init(m): \n",
    "    if isinstance(m, nn.Conv2d): \n",
    "        nn.init.kaiming_uniform_(m.weight) \n",
    "\n",
    "\n",
    "class generator(nn.Module):\n",
    "    def __init__(self, input_dim=100,input_size=64):\n",
    "        super(generator, self).__init__()\n",
    "        self.input_dim=input_dim\n",
    "        self.input_size=input_size\n",
    "        self.att= Self_Attn(64, 'relu')\n",
    "        self.fc = nn.Linear(self.input_dim, 256 * (self.input_size//16) * (self.input_size//16))\n",
    "        self.ps1=nn.PixelShuffle(2)\n",
    "        self.tu1 = nn.Sequential(\n",
    "            conv5x5(256//4, 256, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.ps2=nn.PixelShuffle(2)\n",
    "        self.tu2 = nn.Sequential(\n",
    "            conv5x5(256//4, 128, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            resnet_basic(128, 1),\n",
    "            conv3x3(128, 128, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.ps3=nn.PixelShuffle(2)\n",
    "        self.tu3 = nn.Sequential(\n",
    "            conv3x3(128//4, 128, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            resnet_basic(128,1),\n",
    "            conv3x3(128, 64, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "\n",
    "        )\n",
    "        self.ps4=nn.PixelShuffle(2)\n",
    "        self.tu4 = nn.Sequential(\n",
    "            conv3x3(64//4, 64, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            conv3x3(64, 64, stride=1,dilation=2,padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            conv3x3(64, 64, stride=1,dilation=4,padding=4),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            conv1x1(64, 3, stride=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.fc(input)\n",
    "        x = x.view(x.size(0),256 ,(self.input_size//16) ,(self.input_size//16))\n",
    "        x = self.ps1(x)\n",
    "        x = self.tu1(x)\n",
    "        x = self.ps2(x)\n",
    "        x = self.tu2(x)\n",
    "        x = self.ps3(x)\n",
    "        x = self.tu3(x)\n",
    "        x,attention=self.att(x)\n",
    "        x = self.ps4(x)\n",
    "        x =self.tu4(x)\n",
    "        return x\n",
    "\n",
    "class discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(discriminator, self).__init__()\n",
    "        self.td1 = nn.Sequential(\n",
    "            conv5x5(3, 32, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            conv3x3(32, 32, stride=1,dilation=2,padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            conv5x5(32, 64, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2)\n",
    " \n",
    "        )\n",
    "        self.td2 = nn.Sequential(\n",
    "            conv3x3(64, 64, stride=1,dilation=4,padding=4),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            conv3x3(64, 128, stride=1,dilation=8,padding=8),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            resnet_basic(128,1),\n",
    "            conv3x3(128, 128, stride=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            resnet_basic(128,1),\n",
    "            conv3x3(128, 128, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            conv1x1(128, 64, stride=1),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        self.fc = nn.Linear(64, 1)\n",
    "        self.att= Self_Attn(64, 'relu')\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.td1(input)\n",
    "        x,attention=self.att(x)\n",
    "        x = self.td2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "接下來我們可以利用以下函數將多張生成圖片拼貼成為一張，以便於檢視"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tile_rgb_images(x, row=2, col=2):\n",
    "    fig = pylab.gcf()\n",
    "    fig.set_size_inches(col * 2, row * 2)\n",
    "    pylab.clf()\n",
    "    pylab.ioff()\n",
    "    for m in range(row * col):\n",
    "        pylab.subplot(row, col, m + 1)\n",
    "        img = array2img(x[m]*127.5+127.5)\n",
    "        pylab.imshow(img, interpolation=\"nearest\", animated=True)\n",
    "        pylab.axis(\"off\")\n",
    "    filename='Pytorch_Results/RaSGAN_{}.png'.format(\n",
    "    str(datetime.datetime.fromtimestamp(time.time())).replace(' ', '').replace(':', '').replace('-', '').replace(\n",
    "        '.', ''))\n",
    "    pylab.savefig(filename, bbox_inches='tight')\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "設計對抗網路，生成器的輸入為隨機噪音。鑑別器各自輸入真圖片與假圖片。然後設計各自損失函數以及訓練器。在此我只將訓練minibatch數量設為200，只是為了快速展示，若是希望跑出來的效果更好，請把這數字加大(數萬)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G = generator(100).to(device)\n",
    "D = discriminator().to(device)\n",
    "\n",
    "G.apply(weights_init)\n",
    "D.apply(weights_init)\n",
    "\n",
    "if os.path.exists('Models/RaSGAN_X_fake_pytorch.cnn'):\n",
    "    G=torch.load('Models/RaSGAN_X_fake_pytorch.cnn')\n",
    "    print('G recovered!!')\n",
    "    \n",
    "if os.path.exists('Models/RaSGAN_D_real_pytorch.cnn'):\n",
    "    model=torch.load('Models/RaSGAN_D_real_pytorch.cnn')\n",
    "    print('D recovered!!')\n",
    "    \n",
    "\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=2e-4,betas=(0.0, 0.999), weight_decay=5e-5)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=2e-4,betas=(0.9, 0.999), weight_decay=5e-5)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "記得先訓練分類器，在訓練生成器，否則分類器會不具鑑別能力而無法供應生成器有效的梯度指引而造成訓練失敗"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "以下是我針對訓練GAN模型給予的一些小建議\n",
    "\n",
    "輸入數據\n",
    "    輸入的噪聲值使用高斯分布而非是均等分布，可以避免生成內容質性過高\n",
    "    輸入的真圖片需要加入噪聲以及進行數據增強，以避免模型記憶單一圖片結果\n",
    "    輸入圖像使用(img-127.5)/127.5標準化到-1與1之間，生成器最後一層使用tanh，而非是標準化到0與1之間，生成器最後一層使用sigmoid\n",
    "\n",
    "模型結構\n",
    "    使用pixel_shuffle作為上採樣方法\n",
    "    不要使用max pooling，改用strides=2來作為下採樣方法\n",
    "    生成器最後一層不要使用batch normalization，會容易產生偽影\n",
    "    生成器噪聲層後一定「不要」使用batch normalization，否則噪聲差異就被normalization掉了\n",
    "    鑑別器最後一層活化函數為None而非sigmoid，這樣才能產生夠大的loss，迫使生成器進步\n",
    "    加入dilation來強化獲取較大範圍的上下文細節\n",
    "    初始化使用xavier，初始化方差盡量要小\n",
    "    resnet block使用殘差縮放(本案例縮放尺度使用0.2)\n",
    "    建議可以先做初步測試，如果鑑別器太強，則酌量將鑑別器通道數減少，或者是增加生成器的resnet block使用次數，以確保兩者實力接近。\n",
    "    適時使用dropout，建議用在鑑別器，因為訓練初期可給鑑別器增加難度而不會過強，訓練後期則會增加建模器的泛用性。但不太特別建議用在生成器\n",
    "    加入self-attention以獲取周遭以及遠端之間的相互依賴細節\n",
    "優化方法\n",
    "    生成器的優化器中將動量設為0.0 (沒錯，不要懷疑)\n",
    "    先優化鑑別器再優化生成器，這樣生成器訓練時才會獲得梯度的指引\n",
    "    如果你要在訓練過程中加入梯度懲罰，那麼鑑別器模型設計時就不能使用Batch Normalization，而建議改用instance Normalization\n",
    "    n篇文章以及我個人的經驗都一致`,學習速率初始值是2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minibatch_size=32\n",
    "num_epochs=300\n",
    "print('epoch start')\n",
    "D.train()\n",
    "for epoch in range(num_epochs):\n",
    "    mbs=0\n",
    "    G.train()\n",
    "    while mbs <1000:\n",
    "        Z_data = noise_sample(minibatch_size)\n",
    "        X_data = next_minibatch(minibatch_size)\n",
    "        \n",
    "        \n",
    "        Z_data, X_data = torch.from_numpy(Z_data), torch.from_numpy(X_data)\n",
    "     \n",
    "        Z_data, X_data = Variable(Z_data).to(device), Variable(X_data).to(device)\n",
    "        \n",
    "        D_optimizer.zero_grad()\n",
    "\n",
    "        D_real = D(X_data)\n",
    "\n",
    "        G_ = G(Z_data)\n",
    "        D_fake = D(G_)\n",
    "        \n",
    "        epsilon=1e-10\n",
    "        D_r_tilde = torch.sigmoid(D_real -D_fake.mean())\n",
    "        D_f_tilde = torch.sigmoid(D_fake - D_real.mean())\n",
    "        D_loss = - ((D_r_tilde + epsilon).log()).mean() - ((1 - D_f_tilde + epsilon).log()).mean()\n",
    "        \n",
    "        #在倒傳導階段是不允許兩個計算圖同時更新梯度，所以要設定retain_graph=True\n",
    "        D_loss.backward(retain_graph=True)\n",
    "        D_optimizer.step()\n",
    "\n",
    "        G_optimizer.zero_grad()\n",
    "\n",
    "        G_ = G(Z_data)\n",
    "        D_fake = D(G_)\n",
    "         \n",
    "        G_loss =- ((D_f_tilde + epsilon).log()).mean() - ((1 - D_r_tilde + epsilon).log()).mean()\n",
    "      \n",
    "        G_loss.backward(retain_graph=True)\n",
    "        G_optimizer.step()\n",
    "        \n",
    "        \n",
    "\n",
    "        if (mbs+1)%50==0:\n",
    "            print(\"Epoch: {}/{} \".format(epoch+1, num_epochs),\n",
    "                                      \"Step: {} \".format(mbs+1),\n",
    "                                      \"D Loss: {:.4f} (D_real:{:.3%})  \".format(D_loss.data.item(),D_real.cpu().detach().numpy().mean()),\n",
    "                                      \"G Loss: {:.4f} (D_fake:{:.3%})  \".format(G_loss.data.item(),D_fake.cpu().detach().numpy().mean()))\n",
    "            tile_rgb_images(G_.cpu().detach().numpy(), 4, 4)\n",
    "            torch.save(G, 'Models/RaSGAN_X_fake_pytorch.cnn')\n",
    "            torch.save(D, 'Models/RaSGAN_D_real_pytorch.cnn')\n",
    "        mbs+=1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
