{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 美少女無窮生成 cntk"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "生成式對抗模型可以說是近期深度學習中最有創意的發想，他利用一個生成器負責作假圖片，用一個鑑別器負責識別輸入的照片是真還是假? 生成器從來沒看過真照片，但是他必須從鑑別器的反應來推斷逐步修正，一直到可以生成以假亂真的照片。這也意味著，生成模型將照片裡的所有知識細節都已經學習到了。在這個實作中，我去AKB48以及早安少女的Hellow家族官網，以及透過爬蟲利用關鍵字美少女收集了大量日系美少女頭像，我們可以透過GAN來無窮生成美少女嗎??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![md_images](../Images/gan.jpg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在本次實例中，我們將會使用一篇很新RaSGAN，這篇論文最大的特色就是提出了在傳統GAN中只關注真假，卻沒有注意到真與假之間的相對差異，舉例來說，下圖第一層是很正常的麵包與柯基，因為差異很大所以沒有問題，但是當真實圖片出現了像麵包的柯基屁屁，或者是像狗的麵包，這時候真與假之間的相對差異就沒那麼明顯了，所以這篇論文中在損失函數的部分，將真實圖片的鑑別機率要減去假圖片平均的鑑別機率後再做一次sigmoid將差異拉開至0~1之間，反之亦然，這樣就可以緩解圖片中出現看起來沒那麼真的問題。會採用RaSGAN的原因在於這是我目前使用過最快就能產生有效影像的GAN方法。\n",
    "\n",
    "The relativistic discriminator: a key element missing from standard GAN\n",
    "https://arxiv.org/pdf/1807.00734.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![md_images](../Images/rasgan.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "首先我們引用相關的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import glob\n",
    "import pylab\n",
    "import cv2\n",
    "import math\n",
    "import string\n",
    "import cntk as C\n",
    "from cntk.ops import *\n",
    "from cntk.initializer import *\n",
    "from cntk.ops.functions import *\n",
    "from cntk.layers import *\n",
    "from cntk.losses import *\n",
    "from cntk.train import *\n",
    "from cntk.learners import *\n",
    "from cntk.metrics import *\n",
    "from cntk.device import *\n",
    "import random \n",
    "\n",
    "\n",
    "\n",
    "# 是否使用GPU\n",
    "is_gpu = True\n",
    "if is_gpu:\n",
    "    try_set_default_device(gpu(0))\n",
    "else:\n",
    "    try_set_default_device(cpu())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在此定義圖片大小為128*128，以及設計產生噪音(長度為100)的函數作為生成模型的輸入值以及讀取所有的圖片(都已經縮放為128*128，目前僅提供給付費學員)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_h = 64\n",
    "img_w = 64\n",
    "img_c = 3\n",
    "\n",
    "#產生高斯分布噪音\n",
    "def noise_sample(num_samples, g_input_dim=100):\n",
    "    return np.random.normal(\n",
    "        size=[num_samples, g_input_dim]\n",
    "    ).astype(np.float32)\n",
    "\n",
    "s =  glob.glob('../Data/ex08_train/resized_images/' + '*.jpg')\n",
    "print('{0}張圖片'.format(len(s)))\n",
    "random.shuffle(s)\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "定義讀取圖片的reader以及讀取圖片時數劇增強的操作函數，請注意，輸入圖片應該利用(im-127.5)/127.5調整成-1~+1之間，而不要是除以255.變成0~+1之間。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#圖片轉向量\n",
    "def img2array(img: Image):\n",
    "    arr = np.array(img).astype(np.float32)\n",
    "    arr=arr.transpose(2, 0, 1) #轉成CHW\n",
    "    arr=np.ascontiguousarray(arr)\n",
    "    return arr[::-1] #顏色排序為BGR\n",
    "\n",
    "#向量轉圖片\n",
    "def array2img(arr: np.ndarray):\n",
    "    arr =arr[::-1]#轉成RGB\n",
    "    sanitized_img = np.maximum(0, np.minimum(255, np.transpose(arr, (1, 2, 0))))#轉成HWC\n",
    "    img = Image.fromarray(sanitized_img.astype(np.uint8))\n",
    "    return img\n",
    "\n",
    "#隨機加入標準常態分配的噪聲\n",
    "def add_noise(image):\n",
    "    noise=np.random.standard_normal(image.shape)*np.random.choice(np.arange(-5,5))\n",
    "    image=np.clip(image+noise,0,255)\n",
    "    return image\n",
    "\n",
    "#調整明暗\n",
    "def adjust_gamma(image,gamma=1.2):\n",
    "    image = image.transpose([1, 2, 0])\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    cv2.LUT(image.astype(np.uint8), table)\n",
    "    image = image.transpose([2, 0, 1])\n",
    "    return image\n",
    "\n",
    "#模糊\n",
    "def adjust_blur(image):\n",
    "    image = image.transpose([1, 2, 0])\n",
    "    image=cv2.blur(image, (3, 3))\n",
    "    image = image.transpose([2, 0, 1])\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "def next_minibatch(minibatch_size,is_train=True):\n",
    "    global s, idx\n",
    "    features = []\n",
    "    while len(features) < minibatch_size:\n",
    "        try:\n",
    "            im = Image.open(s[idx]).convert('RGB').resize((64,64),Image.ANTIALIAS) \n",
    "            im = img2array(im).astype(np.float32)\n",
    "            #加入數據增強以確保圖片輸入的多元性，避免鑑別模型記憶樣本過擬合\n",
    "            if is_train:\n",
    "                im=add_noise(im)\n",
    "                if random.randint(0,10)%2==0:\n",
    "                    gamma=np.random.choice(np.arange(0.6, 1.5, 0.05))\n",
    "                    img=adjust_gamma(im,gamma)\n",
    "                if random.randint(0,10)%5<=1:\n",
    "                    im=adjust_blur(im)\n",
    "                    \n",
    "            features.append((im-127.5)/127.5)\n",
    "        except OSError as e:\n",
    "            print(e)\n",
    "        idx += 1\n",
    "        if idx >= len(s):\n",
    "            random.shuffle(s)\n",
    "            idx = 0\n",
    "    return np.asarray(features).astype(np.float32)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "為了能夠讓生成的效果更好，我們在網絡結構上面做了一些設計。首先是在生成器與鑑別器中都同時加入了自注意力(self attention)機制，顧名思義，自注意力的注意力權值分布表示衍生自卷積本身的transpose相乘(相乘前將卷積reshape成(h*w,c))，因此它可以處理圖片每個對應位置的重要性權重分布。加入自注意力機制，生成器可以處理周遭以及遠端之間的相互依賴，以確保生成細節的合理性，而鑑別器也能夠更好的根據全局的圖像結構來做更精確的集合約束判定。\n",
    "\n",
    "在自注意力機制中，最後透過gamma這個參數來與原輸入相加(x = gamma * o + x)，請注意gamma的初始值應該要為零，這樣確保第一次執行時是沒有納入注意力機制的，應該要在訓練後才逐步增加gamma的大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![md_images](../Images/self_attention_module.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def self_attn_block(x,num_filters, squeeze_factor=8):\n",
    "    '''\n",
    "    代碼來自於 https://github.com/taki0112/Self-Attention-GAN-Tensorflow\n",
    "    '''\n",
    "    f = Convolution((1, 1), num_filters // squeeze_factor, pad=True, name='f_conv')(x)\n",
    "    g = Convolution((1, 1), num_filters // squeeze_factor, pad=True, name='g_conv')(x)\n",
    "    h = Convolution((1, 1), num_filters, pad=True, name='h_conv')(x)\n",
    "\n",
    "    h_shape = h.shape\n",
    "\n",
    "    flat_f = reshape(f, (-1,f.shape[0])) \n",
    "    flat_g = reshape(g, (-1,g.shape[0])) \n",
    "    flat_h = reshape(h, (-1,h.shape[0]))  \n",
    "\n",
    "    s = times_transpose(flat_g, flat_f)  #(1024,1024) [N,N]  N = h * w\n",
    "\n",
    "    beta = softmax(s,-1)\n",
    "    o = times(beta, flat_h)##(1024,128)  [ N, C]\n",
    "    o = reshape(o, (h_shape[1], h_shape[2], h_shape[0]))\n",
    "    o = transpose(o, [2, 0, 1])\n",
    "    gamma = Parameter(1, init=0.0)\n",
    "    x = gamma * o + x\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def pixel_shuffle(x,scale):\n",
    "    h=x.shape[1]\n",
    "    w=x.shape[2]\n",
    "    x = C.reshape(x, (x.shape[0]//(scale*scale),scale,scale, x.shape[1], x.shape[2]))\n",
    "    x = transpose(x,[0,3,4,1,2])\n",
    "    \n",
    "    slicelist = [x[:,i,:,:,:] for i in range(h)]\n",
    "    x=splice(*slicelist,axis=3)\n",
    "    slicelist = [x[:,:,i,:,:] for i in range(w)]\n",
    "    x=splice(*slicelist,axis=4)\n",
    "    x = squeeze(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![md_images](../Images/pixelshuffle.jpg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "設計生成器以及鑑別器的結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def conv_bn_leaky_relu(input, filter_size, num_filters, strides=(1, 1), init=C.xavier(0.0005),dilation=1, bias=False):\n",
    "    c = Convolution2D(filter_size, num_filters, activation=None, init=init, pad=True, strides=strides,dilation=dilation, bias=bias)(input)\n",
    "    r = BatchNormalization()(c)\n",
    "    return leaky_relu(r)\n",
    "\n",
    "def conv_bn_relu(input, filter_size, num_filters, strides=(1, 1), init=C.xavier(0.0005),dilation=1, bias=False):\n",
    "    c = Convolution2D(filter_size, num_filters, activation=None, init=init, pad=True, strides=strides,dilation=dilation, bias=bias)(input)\n",
    "    r = BatchNormalization()(c)\n",
    "    return relu(r)\n",
    "\n",
    "\n",
    "def conv_leaky_relu(input, filter_size, num_filters, strides=(1, 1), init=C.xavier(0.0005),dilation=1, bias=False):\n",
    "    r = Convolution2D(filter_size, num_filters, activation=leaky_relu, init=init, pad=True, strides=strides,dilation=dilation, bias=bias)(input)\n",
    "    return r\n",
    "\n",
    "\n",
    "def resnet_basic(input, num_filters,dilation=1):\n",
    "    c1 = conv_bn_relu(input, (3, 3), num_filters,dilation=dilation, bias=False)\n",
    "    c2 = Convolution2D((3,3), num_filters, activation=None, pad=True, strides=1,init=C.xavier(0.0005),dilation=dilation, bias=False)(c1)\n",
    "    c2 = BatchNormalization()(c2)\n",
    "    p = 0.2*c2 + input\n",
    "    return relu(p)\n",
    "\n",
    "\n",
    "#生成器\n",
    "def generator(z):\n",
    "    x = Dense([256, img_h // 16, img_w // 16], activation=None, init=xavier(0.02), bias=False)(z) \n",
    "    x = pixel_shuffle(x,2)\n",
    "    x = conv_bn_relu(x,(5, 5),256)\n",
    "\n",
    "    x = pixel_shuffle(x,2)\n",
    "    x = conv_bn_relu(x,(5, 5),128)\n",
    "    x = resnet_basic(x,128)\n",
    "    x = conv_bn_relu(x,(3, 3),128)\n",
    "    \n",
    "    x = pixel_shuffle(x,2)\n",
    "    x = conv_bn_relu(x,(3, 3),128)\n",
    "    x = resnet_basic(x,128)\n",
    "    x = conv_bn_relu(x,(3, 3),64)\n",
    "    \n",
    "    x = self_attn_block(x,64)\n",
    "    \n",
    "    x = pixel_shuffle(x,2)\n",
    "    x = conv_bn_relu(x,(3, 3),64,dilation=2)\n",
    "    x = conv_bn_relu(x,(3, 3),64,dilation=4)\n",
    "    x = conv_bn_relu(x,(3, 3),64)\n",
    "    #通道數降維為3，使用sigmoid(控制向量介於0~1)\n",
    "    x = Convolution2D((1,1), 3, activation=tanh, init=xavier(0.0005), pad=True, strides=1, bias=False)(x)\n",
    "    return x\n",
    "\n",
    "#鑑別器ˇ\n",
    "def discriminator(x):\n",
    "    x = conv_bn_leaky_relu(x,(5, 5), 32, strides=2)\n",
    "    x = conv_bn_leaky_relu(x,(3, 3), 32, strides=1,dilation=2)\n",
    "\n",
    "    x = conv_bn_leaky_relu(x,(5, 5), 64, strides=2)\n",
    "    x = self_attn_block(x,64)\n",
    "    \n",
    "    x = conv_bn_leaky_relu(x,(3, 3), 64, strides=1,dilation=4)\n",
    "    x = conv_bn_leaky_relu(x,(3, 3), 128, strides=1,dilation=8)\n",
    "    x = dropout(x,0.5)\n",
    "    x = resnet_basic(x,128)\n",
    "    x = conv_bn_leaky_relu(x,(3, 3), 128, strides=2)\n",
    "    x = resnet_basic(x,128)\n",
    "    x = conv_bn_leaky_relu(x,(3, 3), 128, strides=1)\n",
    "    x = conv_bn_leaky_relu(x,(1, 1), 64, strides=1)\n",
    "    x = GlobalAveragePooling()(x)\n",
    "    x = squeeze(x)\n",
    "    x = Dense(1, activation=None, bias=False)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "接下來我們可以利用以下函數將多張生成圖片拼貼成為一張，以便於檢視"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tile_rgb_images(x, row=2, col=2):\n",
    "    fig = pylab.gcf()\n",
    "    fig.set_size_inches(col * 2, row * 2)\n",
    "    pylab.clf()\n",
    "    pylab.ioff()\n",
    "    for m in range(row * col):\n",
    "        pylab.subplot(row, col, m + 1)\n",
    "        img = array2img(x[m]*127.5+127.5)\n",
    "        pylab.imshow(img, interpolation=\"nearest\", animated=True)\n",
    "        pylab.axis(\"off\")\n",
    "    filename='Results/RaSGAN_{}.png'.format(\n",
    "    str(datetime.datetime.fromtimestamp(time.time())).replace(' ', '').replace(':', '').replace('-', '').replace(\n",
    "        '.', ''))\n",
    "    pylab.savefig(filename, bbox_inches='tight')\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "設計對抗網路，生成器的輸入為隨機噪音。鑑別器各自輸入真圖片與假圖片。然後設計各自損失函數以及訓練器。在此我只將訓練minibatch數量設為200，只是為了快速展示，若是希望跑出來的效果更好，請把這數字加大(數萬)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minibatch_size = 32\n",
    "num_minibatches = 50000\n",
    "lr = 2e-4\n",
    "epsilon=1e-10\n",
    "\n",
    "def build_RaSGAN_graph(noise_shape, image_shape, generator, discriminator):\n",
    "    input_dynamic_axes = [C.Axis.default_batch_axis()]\n",
    "    Z = C.input_variable(noise_shape, dynamic_axes=input_dynamic_axes)\n",
    "    X_real = C.input_variable(image_shape, dynamic_axes=input_dynamic_axes)\n",
    "    \n",
    "    #生成器\n",
    "    X_fake = generator(Z)\n",
    "    if os.path.exists('Models\\RaSGAN_X_fake_64.cnn'):\n",
    "        generator = C.Function.load('Models\\RaSGAN_X_fake_64.cnn')\n",
    "        X_fake = generator(Z)\n",
    "        print(\"Loading existing X_fake\")\n",
    "\n",
    "    #鑑別器(真圖片)\n",
    "    D_real = discriminator(X_real)\n",
    "    if os.path.exists('Models\\RaSGAN_D_real_64.cnn'):\n",
    "        D_real = C.Function.load('Models\\RaSGAN_D_real_64.cnn')\n",
    "        D_real = D_real(X_real)\n",
    "        print(\"Loading existing D_real\")\n",
    "    \n",
    "    #鑑別器(假圖片)\n",
    "    D_fake = D_real.clone(\n",
    "        method='share',\n",
    "        substitutions={X_real: X_fake.output})\n",
    "    \n",
    "    clipped_D_params = [C.clip(p, -0.5, 0.5) for p in D_real.parameters]\n",
    "\n",
    "    \n",
    "    #設計損失函數\n",
    "    D_r_tilde = sigmoid(D_real - reduce_mean(D_fake))\n",
    "    D_f_tilde = sigmoid(D_fake - reduce_mean(D_real))\n",
    "    D_loss = - reduce_mean(log(D_r_tilde + epsilon)) - reduce_mean(log(1 - D_f_tilde + epsilon))\n",
    "    G_loss = - reduce_mean(log(D_f_tilde + epsilon)) - reduce_mean(log(1 - D_r_tilde + epsilon))\n",
    "    \n",
    "    #D_r_tilde = sigmoid(D_real - reduce_mean(D_fake))\n",
    "    #D_f_tilde = sigmoid(D_fake - reduce_mean(D_real))\n",
    "    #D_loss = - reduce_mean(log(D_r_tilde + epsilon)) - reduce_mean(log(1 - D_f_tilde + epsilon))\n",
    "    #G_loss = - reduce_mean(log(D_f_tilde + epsilon))\n",
    "    \n",
    "    \n",
    "    pp_G = C.logging.ProgressPrinter(50)\n",
    "    pp_D = C.logging.ProgressPrinter(50)\n",
    "    G_learner = C.adam(\n",
    "        parameters=X_fake.parameters,\n",
    "        lr=C.learning_rate_schedule(lr, C.UnitType.sample),\n",
    "        momentum= momentum_schedule(0.0),\n",
    "        l2_regularization_weight=5e-5,\n",
    "        unit_gain =False,\n",
    "        use_mean_gradient=True,epsilon=epsilon)\n",
    "\n",
    "    D_learner = C.adam(\n",
    "        parameters=D_real.parameters,\n",
    "        lr=C.learning_rate_schedule(lr, C.UnitType.sample),\n",
    "        momentum=momentum_schedule(0.9),\n",
    "        l2_regularization_weight=5e-5, \n",
    "        unit_gain =False,\n",
    "        use_mean_gradient=True,epsilon=epsilon)\n",
    "\n",
    "    G_trainer = C.Trainer(X_fake,(G_loss, reduce_mean(D_fake)),G_learner,pp_G)\n",
    "    D_trainer = C.Trainer(D_real,(D_loss, reduce_mean(D_real)),D_learner,pp_D)\n",
    "\n",
    "    return X_real, X_fake, D_real,clipped_D_params, Z, G_trainer, D_trainer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "記得先訓練分類器，在訓練生成器，否則分類器會不具鑑別能力而造成訓練失敗"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "以下是我針對訓練GAN模型給予的一些小建議\n",
    "\n",
    "輸入數據\n",
    "    輸入的噪聲值使用高斯分布而非是均等分布，可以避免生成內容質性過高\n",
    "    輸入的真圖片需要加入噪聲以及進行數據增強，以避免模型記憶單一圖片結果\n",
    "    輸入圖像使用(img-127.5)/127.5標準化到-1與1之間，生成器最後一層使用tanh，而非是標準化到0與1之間，生成器最後一層使用sigmoid\n",
    "\n",
    "模型結構\n",
    "    使用pixel_shuffle作為上採樣方法\n",
    "    不要使用max pooling，改用strides=2來作為下採樣方法\n",
    "    生成器最後一層不要使用batch normalization，會容易產生偽影\n",
    "    生成器噪聲層後一定「不要」使用batch normalization，否則噪聲差異就被normalization掉了\n",
    "    鑑別器最後一層活化函數為None而非sigmoid，這樣才能產生夠大的loss，迫使生成器進步\n",
    "    加入dilation來強化獲取較大範圍的上下文細節\n",
    "    初始化使用xavier，初始化方差盡量要小\n",
    "    resnet block使用殘差縮放(本案例縮放尺度使用0.2)\n",
    "    建議可以先做初步測試，如果鑑別器太強，則酌量將鑑別器通道數減少，或者是增加生成器的resnet block使用次數，以確保兩者實力接近。\n",
    "    適時使用dropout，建議用在鑑別器，因為訓練初期可給鑑別器增加難度而不會過強，訓練後期則會增加建模器的泛用性。但不太特別建議用在生成器\n",
    "    加入self-attention以獲取周遭以及遠端之間的相互依賴細節\n",
    "優化方法\n",
    "    生成器的優化器中將動量設為0.0 (沒錯，不要懷疑)\n",
    "    先優化鑑別器再優化生成器，這樣生成器訓練時才會獲得梯度的指引\n",
    "    如果你要在訓練過程中加入梯度懲罰，那麼鑑別器模型設計時就不能使用Batch Normalization，而建議改用instance Normalization\n",
    "    n篇文章以及我個人的經驗都一致`,學習速率初始值是2e-4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_real, X_fake, D_real,clipped_D_params, Z, G_trainer, D_trainer = build_RaSGAN_graph(100, (3, 64, 64), generator, discriminator)\n",
    "    \n",
    "print(\"第一筆是鑑別器損失，metrics為D_real值；第二筆是生成器損失，metrics為D_fake值，兩者都是越大越被判定為真圖片\")\n",
    "\n",
    "\n",
    "for mbs in range(num_minibatches):\n",
    "    Z_data = noise_sample(minibatch_size)\n",
    "    X_data = next_minibatch(minibatch_size)\n",
    "\n",
    "    #梯度懲罰\n",
    "    #if (mbs+1)>50:\n",
    "    #    for parameter, clipped in zip(D_real.parameters, clipped_D_params):\n",
    "    #        C.assign(parameter, clipped).eval()\n",
    "\n",
    "    D_trainer.train_minibatch({X_real: X_data, Z: Z_data})\n",
    "\n",
    "    G_trainer.train_minibatch({Z: Z_data,X_real: X_data})\n",
    "\n",
    "\n",
    "    if (mbs<10000 and (mbs+1) % 20 == 0) or (mbs>=10000 and(mbs+1) % 50 == 0) :\n",
    "        tile_rgb_images(X_fake(Z_data), 4, 4)\n",
    "        X_fake.save('Models\\RaSGAN_X_fake_64.cnn')\n",
    "        D_real.save('Models\\RaSGAN_D_real_64.cnn')\n",
    "        \n",
    "               \n",
    "    #if (mbs+1) % 5000 == 0 :#每隔1000，學習速率變為75%\n",
    "    #    lr*=0.75\n",
    "                \n",
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
