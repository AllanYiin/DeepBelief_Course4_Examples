{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepFake 數據準備"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "DeekFake人工智能換臉應該可以說是最受矚目的深度學習落地應用了，透過深度學習可以讓我們任意的換臉，在這個實作中，我們要來練習的是將蔡依林與蔡英文臉部互換。一開始，我們首先要介紹的是如何準備數據，你也可以照著這邊介紹的方法來準備任何你想要換臉的對象。這一階段與深度學習框架無關，我們將介紹如何從影片中擷取圖片，以及如何從圖片中擷取人臉以及進行臉部對齊。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![deepfake](../Images/deepfaketheme.jpg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "引用需要的包，最重要的就是opencv (cv2)以及dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "import dlib\n",
    "import cv2\n",
    "import imutils\n",
    "from imutils.face_utils import *\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "宣告dlib的人臉偵測器以及特徵點模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#宣告臉部偵測器，以及載入預訓練的臉部特徵點模型\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "landmark_predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "利用cv2.VideoCapture可以直接幫我們讀取指定影片檔案，然後將裡面的每一禎讀取成圖檔。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('videos/jolin.mp4')\n",
    "i=0\n",
    "while (cap.isOpened()):#影片轉為每禎圖片\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        cv2.imwrite('images data/jolin frames/frame_{0:0>4d}.jpg'.format(i),frame)\n",
    "    else:\n",
    "        break\n",
    "    i+=1\n",
    "    if i%1000==0:\n",
    "        print('已處理{0}張'.format(i))\n",
    "print('共計處理{0}張'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已處理2000張\n",
      "已處理3000張\n",
      "共計處理3915張\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture('videos/president2.mp4')\n",
    "i=1300\n",
    "while (cap.isOpened()):#影片轉為每禎圖片\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        cv2.imwrite('images data/president frames/frame_{0:0>4d}.jpg'.format(i),frame)\n",
    "    else:\n",
    "        break\n",
    "    i+=1\n",
    "    if i%1000==0:\n",
    "        print('已處理{0}張'.format(i))\n",
    "print('共計處理{0}張'.format(i))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "接下來，我們需要從圖片中挖取出人臉的部分。我們是利用dlib的人臉檢測機制，並且根據人臉框位置切割圖片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共計擷取出3217張臉\n"
     ]
    }
   ],
   "source": [
    "imgs=glob.glob('images data/president frames/frame_*.jpg')\n",
    "n=0\n",
    "for im_path in imgs:\n",
    "    img = cv2.imread(im_path)\n",
    "    #產生臉部識別\n",
    "    face_rects = detector(img, 1)\n",
    "    for i, d in enumerate(face_rects):\n",
    "        #讀取框左上右下座標\n",
    "        x1 = d.left()\n",
    "        y1 = d.top()\n",
    "        x2 = d.right()\n",
    "        y2 = d.bottom()\n",
    "        \n",
    "        #在臉部位置打框\n",
    "        crop_img = img[y1:y2, x1:x2, :]\n",
    "        #要顯示圖形前，需要將BGR轉RGB\n",
    "        cv2.imwrite(im_path.replace('president frames','president faces'),crop_img)\n",
    "        n+=1\n",
    "print('共計擷取出{0}張臉'.format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgs=glob.glob('images data/jolin frames/frame_*.jpg')\n",
    "i=4393\n",
    "for im_path in imgs[4393:]:\n",
    "    img = cv2.imread(im_path)\n",
    "    #產生臉部識別\n",
    "    face_rects = detector(img, 1)\n",
    "    for i, d in enumerate(face_rects):\n",
    "        #讀取框左上右下座標\n",
    "        x1 = d.left()\n",
    "        y1 = d.top()\n",
    "        x2 = d.right()\n",
    "        y2 = d.bottom()\n",
    "        \n",
    "        #在臉部位置打框\n",
    "        crop_img = img[y1:y2, x1:x2, :]\n",
    "        #要顯示圖形前，需要將BGR轉RGB\n",
    "        cv2.imwrite(im_path.replace('jolin frames','jolin faces'),crop_img)\n",
    "        i+=1\n",
    "print('共計擷取出{0}張臉'.format(i))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "除了單純的臉部識別外，我們還需要將臉部區域進一步的傳入特徵點模型中產出68個臉部特徵點，其特徵點定義如下圖。我們也將與臉部對齊有關的特徵點整理如下代碼。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FACE_POINTS = range(17, 68) #臉的輪廓\n",
    "MOUTH_POINTS = range(48, 68) #嘴\n",
    "RIGHT_BROW_POINTS =range(17, 22) #右眉毛\n",
    "LEFT_BROW_POINTS = range(22, 27) #左眉毛\n",
    "RIGHT_EYE_POINTS = range(36, 42) #右眼\n",
    "LEFT_EYE_POINTS =range(42, 48) #左眼\n",
    "NOSE_POINTS = range(27, 35) #鼻子\n",
    "#JAW_POINTS = range(0, 17) #下巴\n",
    "ALIGN_POINTS = [LEFT_BROW_POINTS , RIGHT_EYE_POINTS ,LEFT_EYE_POINTS,RIGHT_BROW_POINTS ,NOSE_POINTS , MOUTH_POINTS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![deepfake](../Images/landmark.jpg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "所謂的人臉對齊，是要將各種角度的人臉擺正，因此我們會需要一個對照組，Get_landmarks_2D函數就是基於臉部特徵點統計量來生成對照組座標。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Get_landmarks_2D(image):\n",
    "    w=image.shape[1]\n",
    "    h=image.shape[0]\n",
    "    padding=5\n",
    "    scale=(128-2*padding)/max([image.shape[0],image.shape[1]])\n",
    "\n",
    "    mean_face_x_ratio = np.array([\n",
    "        0.000213256, 0.0752622, 0.18113, 0.29077, 0.393397, 0.586856, 0.689483, 0.799124,\n",
    "        0.904991, 0.98004, 0.490127, 0.490127, 0.490127, 0.490127, 0.36688, 0.426036,\n",
    "        0.490127, 0.554217, 0.613373, 0.121737, 0.187122, 0.265825, 0.334606, 0.260918,\n",
    "        0.182743, 0.645647, 0.714428, 0.793132, 0.858516, 0.79751, 0.719335, 0.254149,\n",
    "        0.340985, 0.428858, 0.490127, 0.551395, 0.639268, 0.726104, 0.642159, 0.556721,\n",
    "        0.490127, 0.423532, 0.338094, 0.290379, 0.428096, 0.490127, 0.552157, 0.689874,\n",
    "        0.553364, 0.490127, 0.42689])\n",
    "\n",
    "    mean_face_y_ratio = np.array([\n",
    "        0.106454, 0.038915, 0.0187482, 0.0344891, 0.0773906, 0.0773906, 0.0344891,\n",
    "        0.0187482, 0.038915, 0.106454, 0.203352, 0.307009, 0.409805, 0.515625, 0.587326,\n",
    "        0.609345, 0.628106, 0.609345, 0.587326, 0.216423, 0.178758, 0.179852, 0.231733,\n",
    "        0.245099, 0.244077, 0.231733, 0.179852, 0.178758, 0.216423, 0.244077, 0.245099,\n",
    "        0.780233, 0.745405, 0.727388, 0.742578, 0.727388, 0.745405, 0.780233, 0.864805,\n",
    "        0.902192, 0.909281, 0.902192, 0.864805, 0.784792, 0.778746, 0.785343, 0.778746,\n",
    "        0.784792, 0.824182, 0.831803, 0.824182])\n",
    "\n",
    "    mean_face_x=mean_face_x_ratio*w*scale+padding\n",
    "    mean_face_y = mean_face_y_ratio*h*scale+padding\n",
    "    landmarks_2D = np.stack([mean_face_x, mean_face_y], axis=1)\n",
    "    return landmarks_2D"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Umeyama是一種PCL演算法，簡單點來理解就是產生將源點集合(人臉特徵點)變換對齊到目標點集合(landmarks_2D)至相同的坐標系下所需之轉換矩陣。有了這個轉換矩陣，就可以再利用opencv的cv2.warpAffine函數進行仿射變換。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def umeyama(src, dst, estimate_scale):\n",
    "    \"\"\"估計N維相似性轉換參數(可處理有無縮放之狀況) \n",
    "    ----------\n",
    "    src : (M, N) array\n",
    "        來源點座標\n",
    "    dst : (M, N) array\n",
    "        參考點座標\n",
    "    estimate_scale : bool\n",
    "        是否估計縮放因子\n",
    "    Returns\n",
    "    -------\n",
    "    T : (N + 1, N + 1)\n",
    "        The homogeneous similarity transformation matrix. The matrix contains\n",
    "        NaN values only if the problem is not well-conditioned.\n",
    "    \n",
    "    參考文獻\n",
    "    ----------\n",
    "    .. [1] \"Least-squares estimation of transformation parameters between two\n",
    "            point patterns\", Shinji Umeyama, PAMI 1991, DOI: 10.1109/34.88573\n",
    "    \"\"\"\n",
    "\n",
    "    num = src.shape[0]\n",
    "    dim = src.shape[1]\n",
    "\n",
    "    # 計算src 以及 dst的均值\n",
    "    src_mean = src.mean(axis=0)\n",
    "    dst_mean = dst.mean(axis=0)\n",
    "\n",
    "    #將 src 與 dst 減去均值\n",
    "    src_demean = src - src_mean\n",
    "    dst_demean = dst  - dst_mean\n",
    "\n",
    "    # Eq. (38).\n",
    "    A = np.dot(dst_demean.T, src_demean) / num\n",
    "\n",
    "    # Eq. (39).\n",
    "    d = np.ones((dim,), dtype=np.double)\n",
    "    if np.linalg.det(A) < 0:\n",
    "        d[dim - 1] = -1\n",
    "\n",
    "    T = np.eye(dim + 1, dtype=np.double)\n",
    "\n",
    "    U, S, V = np.linalg.svd(A)\n",
    "\n",
    "    # Eq. (40) and (43).\n",
    "    rank = np.linalg.matrix_rank(A)\n",
    "    if rank == 0:\n",
    "        return np.nan * T\n",
    "    elif rank == dim - 1:\n",
    "        if np.linalg.det(U) * np.linalg.det(V) > 0:\n",
    "            T[:dim, :dim] = np.dot(U, V)\n",
    "        else:\n",
    "            s = d[dim - 1]\n",
    "            d[dim - 1] = -1\n",
    "            T[:dim, :dim] = np.dot(U, np.dot(np.diag(d), V))\n",
    "            d[dim - 1] = s\n",
    "    else:\n",
    "        T[:dim, :dim] = np.dot(U, np.dot(np.diag(d), V.T))\n",
    "\n",
    "    if estimate_scale:\n",
    "        # Eq. (41) and (42).\n",
    "        scale = 1.0 / src_demean.var(axis=0).sum() * np.dot(S, d)\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    T[:dim, dim] = dst_mean - scale * np.dot(T[:dim, :dim], src_mean.T)\n",
    "    T[:dim, :dim] *= scale\n",
    "\n",
    "    return T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![deepfake](../Images/face_preprocessed.jpg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get_image_mask是產生五官mask，他是利用臉部特徵點來標定五官區域。get_align_face則是根據臉部特徵點以及特徵點對照組產生的轉換矩陣(利用umeyama算法)，再透過opencv中的仿設變換(cv2.warpAffine)來將臉部對齊。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_mask(image,landmarks):\n",
    "    image_mask=np.zeros(image.shape,dtype=float)\n",
    "    for points in ALIGN_POINTS:\n",
    "        try:\n",
    "            \n",
    "            hull =cv2.convexHull( np.array(landmarks[points,:] ).reshape((-1,2)).astype(int) ).flatten().reshape( (-1,2) )\n",
    "            cv2.fillConvexPoly(image_mask,hull,(255,255,255) )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return image_mask\n",
    "    \n",
    "def get_align_face(image,mask,landmarks,size):\n",
    "    landmarks_2D=Get_landmarks_2D(image)\n",
    "    \n",
    "    src_tmp = np.asarray([(int(xy[0]), int(xy[1])) for xy in landmarks[17:]])\n",
    "    tar_tmp = np.asarray([(int(xy[0]), int(xy[1])) for xy in landmarks_2D])\n",
    "\n",
    "    M = umeyama(src_tmp,tar_tmp, True)[:2,:]\n",
    "    align_face = cv2.warpAffine(image, M, (size[1],size[0]), borderMode=cv2.BORDER_REPLICATE)\n",
    "    align_mask = cv2.warpAffine(mask, M, (size[1], size[0]), borderMode=cv2.BORDER_REPLICATE)\n",
    "    return align_face,align_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![deepfake](../Images/warpAffine.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "最後，我們針對擷取的人臉圖片逐一進行臉部對其以及五官mask的生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已處理200張\n",
      "已處理400張\n",
      "已處理600張\n",
      "已處理800張\n",
      "已處理1000張\n",
      "已處理1200張\n",
      "已處理1400張\n",
      "已處理1600張\n",
      "已處理1800張\n",
      "已處理2000張\n",
      "已處理2200張\n",
      "已處理2400張\n",
      "已處理2600張\n",
      "已處理2800張\n",
      "已處理3000張\n",
      "已處理3200張\n",
      "已處理3400張\n",
      "已處理3600張\n",
      "已處理3800張\n",
      "已處理4000張\n",
      "已處理4200張\n",
      "已處理4400張\n",
      "已處理4600張\n",
      "已處理4800張\n",
      "已處理5000張\n",
      "共計擷取出5057張臉\n"
     ]
    }
   ],
   "source": [
    "imgs=glob.glob('images data/jolin faces/frame_*.jpg')\n",
    "i=0\n",
    "for im_path in imgs:\n",
    "    faceimg=cv2.imread(im_path)\n",
    "    d=dlib.rectangle(0, 0,faceimg.shape[0], faceimg.shape[1])\n",
    "    shape=landmark_predictor(faceimg, d)\n",
    "    landmark = shape_to_np(shape)\n",
    "    \n",
    "    try:\n",
    "        image_mask=get_image_mask(faceimg,landmark)\n",
    "        cv2.imwrite(im_path.replace('jolin faces','jolin masks'),image_mask)\n",
    "\n",
    "        align_face,align_mask=get_align_face(faceimg,image_mask,landmark,(128,128))\n",
    "        cv2.imwrite(im_path.replace('jolin faces','jolin align faces'),align_face)\n",
    "        cv2.imwrite(im_path.replace('jolin faces','jolin align masks'),align_mask)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    i+=1\n",
    "    if i%200==0:\n",
    "        print('已處理{0}張'.format(i))\n",
    "print('共計擷取出{0}張臉'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已處理200張\n",
      "已處理400張\n",
      "已處理600張\n",
      "已處理800張\n",
      "已處理1000張\n",
      "已處理1200張\n",
      "已處理1400張\n",
      "已處理1600張\n",
      "已處理1800張\n",
      "已處理2000張\n",
      "已處理2200張\n",
      "已處理2400張\n",
      "已處理2600張\n",
      "已處理2800張\n",
      "已處理3000張\n",
      "已處理3200張\n",
      "共計擷取出3203張臉\n"
     ]
    }
   ],
   "source": [
    "imgs=glob.glob('images data/president faces/frame_*.jpg')\n",
    "i=0\n",
    "for im_path in imgs:\n",
    "    faceimg=cv2.imread(im_path)\n",
    "    d=dlib.rectangle(0, 0,faceimg.shape[0], faceimg.shape[1])\n",
    "    shape=landmark_predictor(faceimg, d)\n",
    "    landmark = shape_to_np(shape)\n",
    "    \n",
    "    try:\n",
    "        image_mask=get_image_mask(faceimg,landmark)\n",
    "        cv2.imwrite(im_path.replace('president faces','president masks'),image_mask)\n",
    "\n",
    "        align_face,align_mask=get_align_face(faceimg,image_mask,landmark,(128,128))\n",
    "        cv2.imwrite(im_path.replace('president faces','president align faces'),align_face)\n",
    "        cv2.imwrite(im_path.replace('president faces','president align masks'),align_mask)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    i+=1\n",
    "    if i%200==0:\n",
    "        print('已處理{0}張'.format(i))\n",
    "print('共計擷取出{0}張臉'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![deepfake](../Images/deepfake-processed.jpg)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
