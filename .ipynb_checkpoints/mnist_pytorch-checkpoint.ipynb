{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge pytorch -y\n",
    "#如果你沒安裝過pytorch，請移除#執行一次即可"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "引用相關的包，如果你的azure notebooks還沒有安裝過pytorch，請注意按照上面指示安裝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "將mnist數據進行轉換，請注意，在pytorch不需要將output轉為onehot，而且需要將他轉型為int64，以配合生成LongTensor。對於feature除以255，是為了將像素值控制在0~1之間，這樣收斂比較快。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(1, 3)\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "arr=np.array([1,2,3])\n",
    "print(arr.shape)\n",
    "\n",
    "arr=np.array([[1,2,3]])\n",
    "print(arr.shape)\n",
    "\n",
    "arr=np.array([[1,2,3],[4,5,6]])\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n",
      "(60000, 784)\n",
      "(60000,)\n",
      "[19329 10291 39040 ... 14293 44639 45787]\n"
     ]
    }
   ],
   "source": [
    "train_data=None\n",
    "test_data=None\n",
    "\n",
    "with open('mnist_train.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "    print(len(train_data))\n",
    "with open('mnist_test.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "    print(len(test_data))\n",
    "    \n",
    "def parse_mnist(data):\n",
    "    features=[]\n",
    "    labels=[]\n",
    "    for row in data:\n",
    "        labels.append(row[-1].astype(np.int64))\n",
    "        features.append(row[:-1].astype(np.float32)/255)\n",
    "    return np.asarray(features),np.asarray(labels)\n",
    "\n",
    "features,labels=parse_mnist(train_data)\n",
    "print(features.shape)\n",
    "print(labels.shape)\n",
    "idxs=np.arange(0,train_data.shape[0])\n",
    "random.shuffle(idxs)\n",
    "print(idxs)\n",
    "idx=0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "定義多層神經網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 500)\n",
    "        self.fc2 = nn.Linear(500, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_minibatch(minibatch_size):\n",
    "    global idxs,idx\n",
    "    x_features=[]\n",
    "    y_labels=[]\n",
    "    while len(x_features)<minibatch_size:\n",
    "        x_features.append(features[idxs[idx]])\n",
    "        y_labels.append(labels[idxs[idx]])\n",
    "        idx+=1\n",
    "        if idx>=len(idxs):\n",
    "            idx=0\n",
    "            random.shuffle(idxs)\n",
    "    return np.asarray(x_features),np.asarray(y_labels)\n",
    "            "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "執行模型訓練過程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100... Step: 20... Loss: 2.2517... Accuracy:0.3125\n",
      "Epoch: 1/100... Step: 40... Loss: 2.0917... Accuracy:0.40625\n",
      "Epoch: 1/100... Step: 60... Loss: 1.7774... Accuracy:0.65625\n",
      "Epoch: 1/100... Step: 80... Loss: 1.1504... Accuracy:0.84375\n",
      "Epoch: 1/100... Step: 100... Loss: 0.7150... Accuracy:0.78125\n",
      "Epoch: 1/100... Step: 120... Loss: 0.7809... Accuracy:0.65625\n",
      "Epoch: 1/100... Step: 140... Loss: 0.6486... Accuracy:0.75\n",
      "Epoch: 1/100... Step: 160... Loss: 0.6548... Accuracy:0.78125\n",
      "Epoch: 1/100... Step: 180... Loss: 0.4001... Accuracy:0.875\n",
      "Epoch: 1/100... Step: 200... Loss: 0.4868... Accuracy:0.8125\n",
      "Epoch: 1/100... Step: 220... Loss: 0.4496... Accuracy:0.9375\n",
      "Epoch: 1/100... Step: 240... Loss: 0.3333... Accuracy:0.9375\n",
      "Epoch: 1/100... Step: 260... Loss: 0.4846... Accuracy:0.90625\n",
      "Epoch: 1/100... Step: 280... Loss: 0.2237... Accuracy:0.9375\n",
      "Epoch: 1/100... Step: 300... Loss: 0.3394... Accuracy:0.875\n",
      "Epoch: 1/100... Step: 320... Loss: 0.4921... Accuracy:0.84375\n",
      "Epoch: 1/100... Step: 340... Loss: 0.5002... Accuracy:0.8125\n",
      "Epoch: 1/100... Step: 360... Loss: 0.3125... Accuracy:0.875\n",
      "Epoch: 1/100... Step: 380... Loss: 0.3323... Accuracy:0.90625\n",
      "Epoch: 1/100... Step: 400... Loss: 0.3674... Accuracy:0.9375\n",
      "Epoch: 1/100... Step: 420... Loss: 0.1829... Accuracy:0.9375\n",
      "Epoch: 1/100... Step: 440... Loss: 0.4639... Accuracy:0.8125\n",
      "Epoch: 1/100... Step: 460... Loss: 0.3331... Accuracy:0.875\n",
      "Epoch: 1/100... Step: 480... Loss: 0.7416... Accuracy:0.84375\n",
      "Epoch: 1/100... Step: 500... Loss: 0.4540... Accuracy:0.875\n",
      "Epoch: 1/100... Step: 520... Loss: 0.2310... Accuracy:0.9375\n",
      "Epoch: 1/100... Step: 540... Loss: 0.2851... Accuracy:0.875\n",
      "Epoch: 1/100... Step: 560... Loss: 0.4636... Accuracy:0.8125\n",
      "Epoch: 1/100... Step: 580... Loss: 0.1227... Accuracy:1.0\n",
      "Epoch: 1/100... Step: 600... Loss: 0.2050... Accuracy:0.9375\n",
      "Epoch: 1/100... Step: 620... Loss: 0.4209... Accuracy:0.875\n",
      "Epoch: 1/100... Step: 640... Loss: 0.5356... Accuracy:0.90625\n",
      "Epoch: 1/100... Step: 660... Loss: 0.8067... Accuracy:0.78125\n",
      "Epoch: 1/100... Step: 680... Loss: 0.1810... Accuracy:0.9375\n",
      "Epoch: 1/100... Step: 700... Loss: 0.2305... Accuracy:0.90625\n",
      "Epoch: 1/100... Step: 720... Loss: 0.4112... Accuracy:0.90625\n",
      "Epoch: 1/100... Step: 740... Loss: 0.1314... Accuracy:0.96875\n",
      "Epoch: 1/100... Step: 760... Loss: 0.0760... Accuracy:0.96875\n",
      "Epoch: 1/100... Step: 780... Loss: 0.3335... Accuracy:0.90625\n",
      "Epoch: 1/100... Step: 800... Loss: 0.4111... Accuracy:0.90625\n",
      "Epoch: 1/100... Step: 820... Loss: 0.1199... Accuracy:1.0\n",
      "Epoch: 1/100... Step: 840... Loss: 0.2383... Accuracy:0.90625\n",
      "Epoch: 1/100... Step: 860... Loss: 0.3862... Accuracy:0.90625\n",
      "Epoch: 1/100... Step: 880... Loss: 0.4343... Accuracy:0.84375\n",
      "Epoch: 1/100... Step: 900... Loss: 0.2392... Accuracy:0.875\n",
      "Epoch: 1/100... Step: 920... Loss: 0.0668... Accuracy:1.0\n",
      "Epoch: 1/100... Step: 940... Loss: 0.2268... Accuracy:0.96875\n",
      "Epoch: 1/100... Step: 960... Loss: 0.2745... Accuracy:0.90625\n",
      "Epoch: 1/100... Step: 980... Loss: 0.2554... Accuracy:0.875\n",
      "Epoch: 1/100... Step: 1000... Loss: 0.0634... Accuracy:1.0\n",
      "Epoch: 1/100... Step: 1020... Loss: 0.3512... Accuracy:0.84375\n",
      "Epoch: 1/100... Step: 1040... Loss: 0.4090... Accuracy:0.875\n",
      "Epoch: 1/100... Step: 1060... Loss: 0.2181... Accuracy:0.90625\n",
      "Epoch: 1/100... Step: 1080... Loss: 0.1051... Accuracy:0.96875\n",
      "Epoch: 1/100... Step: 1100... Loss: 0.1624... Accuracy:0.9375\n",
      "Epoch: 1/100... Step: 1120... Loss: 0.0823... Accuracy:1.0\n",
      "Epoch: 1/100... Step: 1140... Loss: 0.1117... Accuracy:0.96875\n",
      "Epoch: 1/100... Step: 1160... Loss: 0.2927... Accuracy:0.84375\n",
      "Epoch: 1/100... Step: 1180... Loss: 0.5715... Accuracy:0.875\n",
      "Epoch: 1/100... Step: 1200... Loss: 0.0414... Accuracy:1.0\n",
      "Epoch: 1/100... Step: 1220... Loss: 0.2847... Accuracy:0.90625\n",
      "Epoch: 1/100... Step: 1240... Loss: 0.1380... Accuracy:0.96875\n",
      "Epoch: 1/100... Step: 1260... Loss: 0.3406... Accuracy:0.875\n",
      "Epoch: 1/100... Step: 1280... Loss: 0.4771... Accuracy:0.875\n",
      "Epoch: 1/100... Step: 1300... Loss: 0.3631... Accuracy:0.90625\n",
      "Epoch: 1/100... Step: 1320... Loss: 0.2510... Accuracy:0.90625\n",
      "Epoch: 1/100... Step: 1340... Loss: 0.2274... Accuracy:0.9375\n",
      "Epoch: 1/100... Step: 1360... Loss: 0.1671... Accuracy:0.9375\n",
      "Epoch: 1/100... Step: 1380... Loss: 0.1108... Accuracy:1.0\n",
      "Epoch: 1/100... Step: 1400... Loss: 0.2863... Accuracy:0.9375\n",
      "Epoch: 1/100... Step: 1420... Loss: 0.1418... Accuracy:0.9375\n",
      "Epoch: 1/100... Step: 1440... Loss: 0.1160... Accuracy:0.9375\n",
      "Epoch: 1/100... Step: 1460... Loss: 0.1290... Accuracy:0.9375\n",
      "Epoch: 1/100... Step: 1480... Loss: 0.3608... Accuracy:0.90625\n",
      "Epoch: 1/100... Step: 1500... Loss: 0.3890... Accuracy:0.90625\n",
      "Epoch: 1/100... Step: 1520... Loss: 0.1191... Accuracy:0.96875\n",
      "Epoch: 1/100... Step: 1540... Loss: 0.0505... Accuracy:1.0\n",
      "Epoch: 1/100... Step: 1560... Loss: 0.0943... Accuracy:0.96875\n",
      "Epoch: 1/100... Step: 1580... Loss: 0.1625... Accuracy:0.9375\n",
      "Epoch: 1/100... Step: 1600... Loss: 0.1529... Accuracy:0.9375\n",
      "Epoch: 1/100... Step: 1620... Loss: 0.1395... Accuracy:0.9375\n",
      "Epoch: 1/100... Step: 1640... Loss: 0.1666... Accuracy:0.90625\n",
      "Epoch: 1/100... Step: 1660... Loss: 0.1686... Accuracy:0.90625\n",
      "Epoch: 1/100... Step: 1680... Loss: 0.1222... Accuracy:0.96875\n",
      "Epoch: 1/100... Step: 1700... Loss: 0.1304... Accuracy:0.9375\n",
      "Epoch: 1/100... Step: 1720... Loss: 0.3059... Accuracy:0.875\n",
      "Epoch: 1/100... Step: 1740... Loss: 0.1079... Accuracy:0.96875\n",
      "Epoch: 1/100... Step: 1760... Loss: 0.3007... Accuracy:0.875\n",
      "Epoch: 1/100... Step: 1780... Loss: 0.0396... Accuracy:1.0\n",
      "Epoch: 1/100... Step: 1800... Loss: 0.0548... Accuracy:0.96875\n",
      "Epoch: 1/100... Step: 1820... Loss: 0.2205... Accuracy:0.9375\n",
      "Epoch: 1/100... Step: 1840... Loss: 0.2858... Accuracy:0.90625\n",
      "Epoch: 1/100... Step: 1860... Loss: 0.0759... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 20... Loss: 0.2823... Accuracy:0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:250: UserWarning: Couldn't retrieve source code for container of type MLPNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/100... Step: 40... Loss: 0.3307... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 60... Loss: 0.1257... Accuracy:0.9375\n",
      "Epoch: 2/100... Step: 80... Loss: 0.1160... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 100... Loss: 0.1225... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 120... Loss: 0.3217... Accuracy:0.9375\n",
      "Epoch: 2/100... Step: 140... Loss: 0.1068... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 160... Loss: 0.1255... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 180... Loss: 0.2490... Accuracy:0.9375\n",
      "Epoch: 2/100... Step: 200... Loss: 0.0938... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 220... Loss: 0.1237... Accuracy:0.9375\n",
      "Epoch: 2/100... Step: 240... Loss: 0.2059... Accuracy:0.90625\n",
      "Epoch: 2/100... Step: 260... Loss: 0.0373... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 280... Loss: 0.1924... Accuracy:0.9375\n",
      "Epoch: 2/100... Step: 300... Loss: 0.2955... Accuracy:0.84375\n",
      "Epoch: 2/100... Step: 320... Loss: 0.0731... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 340... Loss: 0.1893... Accuracy:0.9375\n",
      "Epoch: 2/100... Step: 360... Loss: 0.0656... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 380... Loss: 0.1539... Accuracy:0.9375\n",
      "Epoch: 2/100... Step: 400... Loss: 0.0503... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 420... Loss: 0.6345... Accuracy:0.875\n",
      "Epoch: 2/100... Step: 440... Loss: 0.0770... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 460... Loss: 0.2228... Accuracy:0.9375\n",
      "Epoch: 2/100... Step: 480... Loss: 0.0844... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 500... Loss: 0.1344... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 520... Loss: 0.2070... Accuracy:0.9375\n",
      "Epoch: 2/100... Step: 540... Loss: 0.1117... Accuracy:0.9375\n",
      "Epoch: 2/100... Step: 560... Loss: 0.0553... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 580... Loss: 0.0491... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 600... Loss: 0.0265... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 620... Loss: 0.2866... Accuracy:0.9375\n",
      "Epoch: 2/100... Step: 640... Loss: 0.2210... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 660... Loss: 0.1237... Accuracy:0.90625\n",
      "Epoch: 2/100... Step: 680... Loss: 0.2064... Accuracy:0.9375\n",
      "Epoch: 2/100... Step: 700... Loss: 0.1106... Accuracy:0.9375\n",
      "Epoch: 2/100... Step: 720... Loss: 0.0450... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 740... Loss: 0.1114... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 760... Loss: 0.1050... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 780... Loss: 0.1840... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 800... Loss: 0.2564... Accuracy:0.875\n",
      "Epoch: 2/100... Step: 820... Loss: 0.0224... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 840... Loss: 0.0175... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 860... Loss: 0.0809... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 880... Loss: 0.2915... Accuracy:0.875\n",
      "Epoch: 2/100... Step: 900... Loss: 0.0846... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 920... Loss: 0.0560... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 940... Loss: 0.0815... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 960... Loss: 0.1118... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 980... Loss: 0.2241... Accuracy:0.9375\n",
      "Epoch: 2/100... Step: 1000... Loss: 0.0820... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 1020... Loss: 0.0255... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 1040... Loss: 0.0439... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 1060... Loss: 0.0336... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 1080... Loss: 0.1299... Accuracy:0.90625\n",
      "Epoch: 2/100... Step: 1100... Loss: 0.0400... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 1120... Loss: 0.1548... Accuracy:0.9375\n",
      "Epoch: 2/100... Step: 1140... Loss: 0.0219... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 1160... Loss: 0.0506... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 1180... Loss: 0.1276... Accuracy:0.9375\n",
      "Epoch: 2/100... Step: 1200... Loss: 0.1994... Accuracy:0.9375\n",
      "Epoch: 2/100... Step: 1220... Loss: 0.1181... Accuracy:0.9375\n",
      "Epoch: 2/100... Step: 1240... Loss: 0.1011... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 1260... Loss: 0.1735... Accuracy:0.9375\n",
      "Epoch: 2/100... Step: 1280... Loss: 0.1618... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 1300... Loss: 0.0233... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 1320... Loss: 0.2122... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 1340... Loss: 0.0138... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 1360... Loss: 0.0516... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 1380... Loss: 0.0570... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 1400... Loss: 0.0342... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 1420... Loss: 0.0388... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 1440... Loss: 0.2333... Accuracy:0.90625\n",
      "Epoch: 2/100... Step: 1460... Loss: 0.1103... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 1480... Loss: 0.1340... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 1500... Loss: 0.0818... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 1520... Loss: 0.0114... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 1540... Loss: 0.0389... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 1560... Loss: 0.0252... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 1580... Loss: 0.1025... Accuracy:0.9375\n",
      "Epoch: 2/100... Step: 1600... Loss: 0.0791... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 1620... Loss: 0.3692... Accuracy:0.90625\n",
      "Epoch: 2/100... Step: 1640... Loss: 0.0814... Accuracy:0.9375\n",
      "Epoch: 2/100... Step: 1660... Loss: 0.0270... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 1680... Loss: 0.0522... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 1700... Loss: 0.0817... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 1720... Loss: 0.0709... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 1740... Loss: 0.0095... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 1760... Loss: 0.0089... Accuracy:1.0\n",
      "Epoch: 2/100... Step: 1780... Loss: 0.0686... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 1800... Loss: 0.0590... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 1820... Loss: 0.0487... Accuracy:0.96875\n",
      "Epoch: 2/100... Step: 1840... Loss: 0.1304... Accuracy:0.90625\n",
      "Epoch: 2/100... Step: 1860... Loss: 0.1471... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 20... Loss: 0.1434... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 40... Loss: 0.1747... Accuracy:0.9375\n",
      "Epoch: 3/100... Step: 60... Loss: 0.1632... Accuracy:0.9375\n",
      "Epoch: 3/100... Step: 80... Loss: 0.4382... Accuracy:0.90625\n",
      "Epoch: 3/100... Step: 100... Loss: 0.1107... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 120... Loss: 0.1315... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 140... Loss: 0.0783... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 160... Loss: 0.0218... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 180... Loss: 0.0805... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 200... Loss: 0.2956... Accuracy:0.9375\n",
      "Epoch: 3/100... Step: 220... Loss: 0.1042... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 240... Loss: 0.1071... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 260... Loss: 0.0082... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 280... Loss: 0.0089... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 300... Loss: 0.1162... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 320... Loss: 0.0631... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 340... Loss: 0.0685... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 360... Loss: 0.0179... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 380... Loss: 0.0478... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 400... Loss: 0.0637... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 420... Loss: 0.0201... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 440... Loss: 0.0870... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 460... Loss: 0.2282... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 480... Loss: 0.4062... Accuracy:0.875\n",
      "Epoch: 3/100... Step: 500... Loss: 0.1160... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 520... Loss: 0.0890... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 540... Loss: 0.0218... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 560... Loss: 0.2861... Accuracy:0.9375\n",
      "Epoch: 3/100... Step: 580... Loss: 0.0040... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 600... Loss: 0.0515... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 620... Loss: 0.1056... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 640... Loss: 0.1413... Accuracy:0.9375\n",
      "Epoch: 3/100... Step: 660... Loss: 0.1139... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 680... Loss: 0.0326... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 700... Loss: 0.0047... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 720... Loss: 0.0370... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 740... Loss: 0.0047... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 760... Loss: 0.0714... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 780... Loss: 0.0278... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 800... Loss: 0.0572... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 820... Loss: 0.1576... Accuracy:0.9375\n",
      "Epoch: 3/100... Step: 840... Loss: 0.0553... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 860... Loss: 0.0197... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 880... Loss: 0.0822... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 900... Loss: 0.0149... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 920... Loss: 0.1000... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 940... Loss: 0.0708... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 960... Loss: 0.2702... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 980... Loss: 0.0492... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 1000... Loss: 0.0121... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 1020... Loss: 0.0476... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 1040... Loss: 0.0915... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 1060... Loss: 0.0408... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 1080... Loss: 0.0184... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 1100... Loss: 0.0778... Accuracy:0.9375\n",
      "Epoch: 3/100... Step: 1120... Loss: 0.0240... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 1140... Loss: 0.0272... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 1160... Loss: 0.1235... Accuracy:0.9375\n",
      "Epoch: 3/100... Step: 1180... Loss: 0.0764... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 1200... Loss: 0.0059... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 1220... Loss: 0.1705... Accuracy:0.9375\n",
      "Epoch: 3/100... Step: 1240... Loss: 0.0426... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 1260... Loss: 0.2276... Accuracy:0.9375\n",
      "Epoch: 3/100... Step: 1280... Loss: 0.0184... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 1300... Loss: 0.0220... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 1320... Loss: 0.0109... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 1340... Loss: 0.0559... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 1360... Loss: 0.0920... Accuracy:0.9375\n",
      "Epoch: 3/100... Step: 1380... Loss: 0.1367... Accuracy:0.9375\n",
      "Epoch: 3/100... Step: 1400... Loss: 0.0761... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 1420... Loss: 0.0915... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 1440... Loss: 0.0185... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 1460... Loss: 0.0180... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 1480... Loss: 0.0297... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 1500... Loss: 0.1127... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 1520... Loss: 0.0123... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 1540... Loss: 0.0292... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 1560... Loss: 0.0811... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 1580... Loss: 0.0285... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 1600... Loss: 0.1517... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 1620... Loss: 0.0145... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 1640... Loss: 0.0210... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 1660... Loss: 0.1309... Accuracy:0.9375\n",
      "Epoch: 3/100... Step: 1680... Loss: 0.3260... Accuracy:0.9375\n",
      "Epoch: 3/100... Step: 1700... Loss: 0.0586... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 1720... Loss: 0.0138... Accuracy:1.0\n",
      "Epoch: 3/100... Step: 1740... Loss: 0.0946... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 1760... Loss: 0.0680... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 1780... Loss: 0.0900... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 1800... Loss: 0.0560... Accuracy:0.96875\n",
      "Epoch: 3/100... Step: 1820... Loss: 0.0855... Accuracy:0.9375\n",
      "Epoch: 3/100... Step: 1840... Loss: 0.3474... Accuracy:0.875\n",
      "Epoch: 3/100... Step: 1860... Loss: 0.0117... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 20... Loss: 0.1029... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 40... Loss: 0.0267... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 60... Loss: 0.0764... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 80... Loss: 0.0075... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 100... Loss: 0.0147... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 120... Loss: 0.1429... Accuracy:0.9375\n",
      "Epoch: 4/100... Step: 140... Loss: 0.0418... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 160... Loss: 0.0247... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 180... Loss: 0.0924... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 200... Loss: 0.0649... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 220... Loss: 0.0773... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 240... Loss: 0.0412... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 260... Loss: 0.1791... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 280... Loss: 0.0028... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 300... Loss: 0.0880... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 320... Loss: 0.0402... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 340... Loss: 0.0236... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 360... Loss: 0.0461... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 380... Loss: 0.0101... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 400... Loss: 0.0553... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 420... Loss: 0.0142... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 440... Loss: 0.0579... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 460... Loss: 0.0681... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 480... Loss: 0.0313... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 500... Loss: 0.0085... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 520... Loss: 0.0066... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 540... Loss: 0.0108... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 560... Loss: 0.0283... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 580... Loss: 0.0222... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 600... Loss: 0.0384... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 620... Loss: 0.0395... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 640... Loss: 0.0432... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 660... Loss: 0.0958... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 680... Loss: 0.0722... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 700... Loss: 0.0241... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 720... Loss: 0.0068... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 740... Loss: 0.2148... Accuracy:0.9375\n",
      "Epoch: 4/100... Step: 760... Loss: 0.1070... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 780... Loss: 0.0073... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 800... Loss: 0.0060... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 820... Loss: 0.0534... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 840... Loss: 0.1211... Accuracy:0.9375\n",
      "Epoch: 4/100... Step: 860... Loss: 0.0889... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 880... Loss: 0.1191... Accuracy:0.9375\n",
      "Epoch: 4/100... Step: 900... Loss: 0.0403... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 920... Loss: 0.0385... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 940... Loss: 0.0372... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 960... Loss: 0.3301... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 980... Loss: 0.0491... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 1000... Loss: 0.1403... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 1020... Loss: 0.0207... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 1040... Loss: 0.0756... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 1060... Loss: 0.0235... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 1080... Loss: 0.0782... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 1100... Loss: 0.0146... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 1120... Loss: 0.0159... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 1140... Loss: 0.0047... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 1160... Loss: 0.0368... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 1180... Loss: 0.0420... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 1200... Loss: 0.0800... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 1220... Loss: 0.0120... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 1240... Loss: 0.0629... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 1260... Loss: 0.0255... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 1280... Loss: 0.0260... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 1300... Loss: 0.0054... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 1320... Loss: 0.0891... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 1340... Loss: 0.0598... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 1360... Loss: 0.1522... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 1380... Loss: 0.0255... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 1400... Loss: 0.0101... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 1420... Loss: 0.0241... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 1440... Loss: 0.0148... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 1460... Loss: 0.0035... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 1480... Loss: 0.0229... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 1500... Loss: 0.0143... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 1520... Loss: 0.0543... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 1540... Loss: 0.0021... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 1560... Loss: 0.0373... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 1580... Loss: 0.0661... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 1600... Loss: 0.0027... Accuracy:1.0\n",
      "Epoch: 4/100... Step: 1620... Loss: 0.0819... Accuracy:0.96875\n",
      "Epoch: 4/100... Step: 1640... Loss: 0.1967... Accuracy:0.9375\n",
      "Epoch: 4/100... Step: 1660... Loss: 0.0885... Accuracy:0.9375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-873ea79086d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mceriation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    902\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 904\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    905\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1968\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1969\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1970\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1788\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[0;32m   1789\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1790\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1791\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1792\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MLPNet()\n",
    "#if use_cuda:\n",
    "model = model.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "ceriation = nn.CrossEntropyLoss()\n",
    "num_epochs=100\n",
    "minibatch_size=32\n",
    "for epoch in range(num_epochs):\n",
    "    mbs=0\n",
    "    rows=0\n",
    "    ave_loss = 0\n",
    "    while rows < train_data.shape[0]:\n",
    "        optimizer.zero_grad()\n",
    "        x,y=get_next_minibatch(minibatch_size)\n",
    "        x, target = torch.from_numpy(x), torch.from_numpy(y)\n",
    "        #if use_cuda:\n",
    "        x, target = x.cuda(), target.cuda()\n",
    "        x, target = Variable(x), Variable(target)\n",
    "        out = model(x)\n",
    "        loss = ceriation(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc=np.mean(np.equal(np.argmax(out.cpu().detach().numpy(),-1).astype(np.int64),y.astype(np.int64)))\n",
    "        \n",
    "        if mbs%20==0 and mbs>0:\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, num_epochs),\n",
    "                          \"Step: {}...\".format(mbs),\n",
    "                          \"Loss: {:.4f}...\".format(loss.data.item()),\n",
    "                          \"Accuracy:{}\".format(acc))\n",
    "            \n",
    "        mbs+=1\n",
    "        rows+=minibatch_size\n",
    "    torch.save(model, 'Models/Mnist_pytorch{0}.cnn'.format(epoch))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
