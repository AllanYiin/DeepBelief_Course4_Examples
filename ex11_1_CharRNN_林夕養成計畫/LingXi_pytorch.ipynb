{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 詞神林夕養成計畫(pytorch)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "林夕是公認的華語流行歌的詞神，他寫了許多膾炙人口的好詞，那麼機器有沒有辦法模仿他寫詞的功夫呢?我們在此使用最簡單的Char-RNN，也就是利用LSTM預測下一個字，利用這樣的模型來模仿林夕的用字遣詞。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![md_images](../Images/charrnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "import math\n",
    "import codecs\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# 是否使用GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我們在此提供了林夕(lingxi.txt)以及方文山(jay.txt)的歌詞語料，首先讀取語料。為何編碼不使用utf-8要改用utf-8-sig，這是因為在windows環境的utf-8格式與python不同，為了避免錯誤，因此使用utf-8-sig比較保險。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 52647\n"
     ]
    }
   ],
   "source": [
    "#如果要更換語料請修改這邊\n",
    "with io.open('lingxi.txt', encoding='utf-8-sig') as f:\n",
    "    corpus = list(f.read().lower())\n",
    "print('corpus length:', len(corpus))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "為了要把文字轉換為onehot編碼，首先我們需要統計他們所使用過的去重複字數。接著生成字轉索引(char_indices)以及反向的索引轉字(indices_char)的字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 2114\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#把每個字去重複\n",
    "chars = sorted(list(set(corpus)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在這邊建構一個序列讀取器。請注意，由於cntk支持可變長度序列，因此可以一次放入一句歌詞，無論字數長短皆可，但要記得開頭與結尾需要放入<s><e>。由於是要預測下一個字，因此label等於feature位移一個字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "n_seqs, n_steps = 16, 32\n",
    "curr_idx = 0\n",
    "text=corpus[curr_idx:curr_idx+n_seqs* n_steps*100]\n",
    "\n",
    "encoded = np.array([char_indices[ch] for ch in text])\n",
    "def one_hot_encode(arr, n_labels):\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns mini-batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "    '''\n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "\n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "\n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "定義char-rnn模型，模型結構如下：\n",
    "     +---------------------------+              +------------+\n",
    "x -->| LSTM(2 layer, hidden=512) |--> dropout-->| DenseLayer |--> y\n",
    "     +---------------------------+              +------------+\n",
    "     \n",
    "這裡的輸入x是一個(批次,序列長，字數)的張量(torch.Size([16, 32, 2114]))，輸入至一個兩層的單向lstm(隱藏層形狀為512)，最後透過dropout後送入全連接層，輸出為長度為字數的onehot向量最大值座標位置(torch.Size([16, 32]))。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocabs, n_steps=100, n_hidden=512, n_layers=2,\n",
    "                 drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "\n",
    "        self.vocabs = vocabs\n",
    "        self.idx2char = dict(enumerate(self.vocabs))\n",
    "        self.char2idx = {ch: ii for ii, ch in self.idx2char.items()}\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.lstm = nn.LSTM(len(self.vocabs), n_hidden, n_layers,batch_first=True)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.vocabs))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x, hc):\n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        x = x.contiguous().view(x.size()[0] * x.size()[1], self.n_hidden)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x, (h, c)\n",
    "\n",
    "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
    "        ''' 輸入一個字去預測下一個字\n",
    "            回傳預測的結果以及隱狀態\n",
    "        '''\n",
    "        self.to(device)\n",
    "  \n",
    "\n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "\n",
    "        x = np.array([[self.char2idx[char]]])\n",
    "        x = one_hot_encode(x, len(self.vocabs))\n",
    "        inputs = Variable(t.from_numpy(x), volatile=True)\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "\n",
    "        h = tuple([Variable(each.data, volatile=True) for each in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "\n",
    "        p = nn.softmax(out).data\n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.vocabs))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p / p.sum())\n",
    "\n",
    "        return self.idx2char[char], h\n",
    "\n",
    "    def init_weights(self):\n",
    "        ''' 初始化权重'''\n",
    "        initrange = 0.1\n",
    "\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "\n",
    "    def init_hidden(self, n_seqs):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()),\n",
    "                Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "溫度函數是為了避免模型一直選取常用字，因此調整各個詞彙的機率分布。在這邊temperature若是小於1，越容易產生意外或是有創意的字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds+10e-14) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / (np.sum(exp_preds)+10e-14)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def write_something(epoch,model):\n",
    "    print()\n",
    "    print('----- 第Epoch: %d後自動寫詞' % epoch)\n",
    "    torch.save(model, 'Models/LingXi_pytorch_{0}.pth'.format(epoch))\n",
    "    torch.save(model, 'Models/LingXi_pytorch.pth')\n",
    "    start_index = random.randint(0, len(text) - n_steps - 1)\n",
    "    for diversity in [1.0]:\n",
    "        print('----- diversity:', diversity)\n",
    "        h = model.init_hidden(1)\n",
    "        h = tuple([Variable(each.data) for each in h])\n",
    "        generated = ''\n",
    "        sentence = ''.join(text[start_index: start_index +3])\n",
    "        generated += sentence\n",
    "        print('----- 根據以下詞彙發想: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "        \n",
    "        for i in range(400):\n",
    "            try:\n",
    "    \n",
    "                arr=np.zeros((1,len(sentence),len(chars))).astype(np.float32) \n",
    "                for i in range(len(list(sentence))):\n",
    "                    s=list(sentence)[i]\n",
    "                    arr[0,i,char_indices[s]]=1\n",
    "                input=torch.from_numpy(arr)\n",
    "                input=Variable(input)\n",
    "                input=input.to(device)\n",
    "                output, h = model(input,h)\n",
    "                pred=output.cpu().detach().numpy()\n",
    "        \n",
    "                preds = pred[-1]\n",
    "                next_index = np.argmax(preds)#sample(preds, diversity)\n",
    "                next_char = indices_char[next_index]\n",
    "\n",
    "                generated += next_char\n",
    "                sentence = sentence[1:] + next_char\n",
    "\n",
    "                sys.stdout.write(next_char)\n",
    "                sys.stdout.flush()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharRNN(\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (lstm): LSTM(2114, 512, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=2114, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CharRNN(chars, n_hidden=512, n_layers=2)\n",
    "if not os.path.exists('Models'):\n",
    "    os.mkdir('Models')\n",
    "    print(\"Directory Models Created \")\n",
    "    \n",
    "if os.path.exists('Models/LingXi_pytorch.pth'):\n",
    "    model=torch.load('Models/LingXi_pytorch.pth')\n",
    "    print('recovered!!')\n",
    "\n",
    "model.train()\n",
    "model_optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "定義訓練流程。每100 minibatch進行一次寫作測試，包括輸入一句完整歌詞以及輸入一個關鍵字自行發想。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start epoch!\n",
      "Epoch: 1/30... Step: 0... Loss: 7.7204... Accuracy:0.000%...\n",
      "Epoch: 1/30... Step: 50... Loss: 6.4734... Accuracy:7.633%...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:250: UserWarning: Couldn't retrieve source code for container of type CharRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30... Step: 100... Loss: 5.9506... Accuracy:11.254%...\n",
      "Epoch: 1/30... Step: 150... Loss: 5.9980... Accuracy:12.324%...\n",
      "Epoch: 1/30... Step: 200... Loss: 5.5963... Accuracy:13.680%...\n",
      "Epoch: 1/30... Step: 250... Loss: 5.5818... Accuracy:14.789%...\n",
      "Epoch: 1/30... Step: 300... Loss: 5.2049... Accuracy:16.785%...\n",
      "Epoch: 1/30... Step: 350... Loss: 5.3241... Accuracy:17.445%...\n",
      "Epoch: 1/30... Step: 400... Loss: 4.8140... Accuracy:19.766%...\n",
      "Epoch: 1/30... Step: 450... Loss: 4.9997... Accuracy:20.945%...\n",
      "Epoch: 1/30... Step: 500... Loss: 4.4673... Accuracy:22.855%...\n",
      "Epoch: 1/30... Step: 550... Loss: 4.6493... Accuracy:24.016%...\n",
      "Epoch: 1/30... Step: 600... Loss: 4.1712... Accuracy:26.121%...\n",
      "Epoch: 1/30... Step: 650... Loss: 4.4564... Accuracy:27.270%...\n",
      "Epoch: 1/30... Step: 700... Loss: 3.8262... Accuracy:28.973%...\n",
      "Epoch: 1/30... Step: 750... Loss: 4.0832... Accuracy:30.691%...\n",
      "Epoch: 1/30... Step: 800... Loss: 3.5804... Accuracy:32.430%...\n",
      "Epoch: 1/30... Step: 850... Loss: 3.8839... Accuracy:33.637%...\n",
      "Epoch: 1/30... Step: 900... Loss: 3.2264... Accuracy:35.352%...\n",
      "Epoch: 1/30... Step: 950... Loss: 3.5018... Accuracy:37.395%...\n",
      "Epoch: 1/30... Step: 1000... Loss: 3.0620... Accuracy:38.805%...\n",
      "Epoch: 1/30... Step: 1050... Loss: 3.2685... Accuracy:40.812%...\n",
      "\n",
      "----- 第Epoch: 0後自動寫詞\n",
      "----- diversity: 1.0\n",
      "----- 根據以下詞彙發想: \"wan\"\n",
      "want your in ay un antty shena love\n",
      "不要一起\n",
      "一起 一切感覺高帝高\n",
      "願永遠都深變\n",
      "尋獲手gislanlt\n",
      "fal cemr ti goving myil in thal wh心t' llt's inco me fall letstand\n",
      "skenat (x3)\n",
      "不要你 最後一次\n",
      "當 追若車沉河驗能w我\n",
      "當你愛心 誰願意\n",
      "在乎果會能誰\n",
      "誰說愛 亦無人\n",
      "有一種手頭天有別人\n",
      "忘掉我有 我卻不說\n",
      "不需愛 這日眼中背影的盪床 的吻會\n",
      "承受後 晚不過\n",
      "每段是新鮮的寂寞 談什麼戀愛\n",
      "如果愛是誰的分享\n",
      "才明今天開始\n",
      "這個世 無謂再如情\n",
      "從來你 oh yoh ina (x4)\n",
      "承愁 plet youringallite yout me lant ti goll my hale tall inla (x4)\n",
      "whevey y過我錯過\n",
      "誰人迷 愛情人\n",
      "愛你最強的爭\n",
      "\n",
      "要等人等親親親\n",
      "Epoch: 2/30... Step: 0... Loss: 2.8100... Accuracy:41.754%...\n",
      "Epoch: 2/30... Step: 50... Loss: 3.0185... Accuracy:43.855%...\n",
      "Epoch: 2/30... Step: 100... Loss: 2.5651... Accuracy:44.836%...\n",
      "Epoch: 2/30... Step: 150... Loss: 2.7204... Accuracy:47.734%...\n",
      "Epoch: 2/30... Step: 200... Loss: 2.4379... Accuracy:48.227%...\n",
      "Epoch: 2/30... Step: 250... Loss: 2.4645... Accuracy:51.113%...\n",
      "Epoch: 2/30... Step: 300... Loss: 2.2325... Accuracy:51.398%...\n",
      "Epoch: 2/30... Step: 350... Loss: 2.3088... Accuracy:54.301%...\n",
      "Epoch: 2/30... Step: 400... Loss: 2.0569... Accuracy:54.781%...\n",
      "Epoch: 2/30... Step: 450... Loss: 1.9855... Accuracy:57.812%...\n",
      "Epoch: 2/30... Step: 500... Loss: 1.7791... Accuracy:57.918%...\n",
      "Epoch: 2/30... Step: 550... Loss: 1.7250... Accuracy:60.754%...\n",
      "Epoch: 2/30... Step: 600... Loss: 1.6636... Accuracy:60.777%...\n",
      "Epoch: 2/30... Step: 650... Loss: 1.6151... Accuracy:63.848%...\n",
      "Epoch: 2/30... Step: 700... Loss: 1.4078... Accuracy:63.785%...\n",
      "Epoch: 2/30... Step: 750... Loss: 1.4988... Accuracy:66.562%...\n",
      "Epoch: 2/30... Step: 800... Loss: 1.3446... Accuracy:66.246%...\n",
      "Epoch: 2/30... Step: 850... Loss: 1.2445... Accuracy:69.387%...\n",
      "Epoch: 2/30... Step: 900... Loss: 1.3124... Accuracy:69.266%...\n",
      "Epoch: 2/30... Step: 950... Loss: 1.1491... Accuracy:71.434%...\n",
      "Epoch: 2/30... Step: 1000... Loss: 1.1700... Accuracy:71.566%...\n",
      "Epoch: 2/30... Step: 1050... Loss: 1.0561... Accuracy:74.145%...\n",
      "\n",
      "----- 第Epoch: 1後自動寫詞\n",
      "----- diversity: 1.0\n",
      "----- 根據以下詞彙發想: \"會哭 \"\n",
      "會哭 多少世\n",
      "來個你 應該如鍛練\n",
      "未體風 亦已經不起\n",
      "一世 最輕一身堆島\n",
      "我再一生才再膚\n",
      "單白等 於親兢\n",
      "時的加寂心有狂\n",
      "想不到 想不到\n",
      "會對 當日的追撒\n",
      "一定的很動\n",
      "就不愛 這是愛到\n",
      "只是一個\n",
      "給我口一它會純\n",
      "一比 表更親伴侶\n",
      "但拳風可可理\n",
      "一顆 心躍讓動\n",
      "無奈你 徬怎合自己\n",
      "不想的挽手\n",
      "覺得你煙花撲\n",
      "那些感由便至至選\n",
      "微和 弦面像從殘遠的難終\n",
      "還記得當於亦過\n",
      "每一段座勇 他還未碎試更最闊麗的將夢 醒覺局取\n",
      "紅 朋盡一吻\n",
      "還要西放風\n",
      "比著螢火火聖\n",
      "知道 這樣近\n",
      "以當天變成為\n",
      "情跟 (x2)\n",
      "愛嗎 多少埋\n",
      "外個都娛是這是人傳\n",
      "有太清醒與河咒洞心轉不近失去自控\n",
      "前一工幕吃起路\n",
      "談也可留下半邊\n",
      "如果 甜後才情點分命運不合理相\n",
      "只有天一險\n",
      "意強生紛開的心思的玫單\n",
      "不等愛 脫愛地\n",
      "失步 我再回\n",
      "多想笑極很徹\n",
      "懇同不心渴望\n",
      "偷昨天 毋中感覺自己\n",
      "用趁刺歌誕釆\n",
      "現在今今 未相澄\n",
      "啊盼 假何無氣\n",
      "你永遠也曬不\n",
      "Epoch: 3/30... Step: 0... Loss: 1.0166... Accuracy:73.652%...\n",
      "Epoch: 3/30... Step: 50... Loss: 0.8842... Accuracy:75.910%...\n",
      "Epoch: 3/30... Step: 100... Loss: 0.9625... Accuracy:76.152%...\n",
      "Epoch: 3/30... Step: 150... Loss: 0.9023... Accuracy:78.293%...\n",
      "Epoch: 3/30... Step: 200... Loss: 0.8966... Accuracy:78.270%...\n",
      "Epoch: 3/30... Step: 250... Loss: 0.6962... Accuracy:80.145%...\n",
      "Epoch: 3/30... Step: 300... Loss: 0.8157... Accuracy:80.023%...\n",
      "Epoch: 3/30... Step: 350... Loss: 0.6084... Accuracy:81.523%...\n",
      "Epoch: 3/30... Step: 400... Loss: 0.6908... Accuracy:81.723%...\n",
      "Epoch: 3/30... Step: 450... Loss: 0.6881... Accuracy:83.711%...\n",
      "Epoch: 3/30... Step: 500... Loss: 0.7551... Accuracy:83.113%...\n",
      "Epoch: 3/30... Step: 550... Loss: 0.5647... Accuracy:84.699%...\n",
      "Epoch: 3/30... Step: 600... Loss: 0.5899... Accuracy:84.434%...\n",
      "Epoch: 3/30... Step: 650... Loss: 0.4751... Accuracy:85.324%...\n",
      "Epoch: 3/30... Step: 700... Loss: 0.6729... Accuracy:85.582%...\n",
      "Epoch: 3/30... Step: 750... Loss: 0.5022... Accuracy:86.535%...\n",
      "Epoch: 3/30... Step: 800... Loss: 0.4515... Accuracy:86.723%...\n",
      "Epoch: 3/30... Step: 850... Loss: 0.4506... Accuracy:87.602%...\n",
      "Epoch: 3/30... Step: 900... Loss: 0.4943... Accuracy:87.547%...\n",
      "Epoch: 3/30... Step: 950... Loss: 0.4004... Accuracy:88.641%...\n",
      "Epoch: 3/30... Step: 1000... Loss: 0.4487... Accuracy:88.512%...\n",
      "Epoch: 3/30... Step: 1050... Loss: 0.3810... Accuracy:89.582%...\n",
      "\n",
      "----- 第Epoch: 2後自動寫詞\n",
      "----- diversity: 1.0\n",
      "----- 根據以下詞彙發想: \"幕 不\"\n",
      "幕 不要花\n",
      "看不能令你\n",
      "多少的影征\n",
      "想o 等不到讚肝\n",
      "丈個人也知\n",
      "誰說知 何以完\n",
      "記憶有往我\n",
      "陪叫你市\n",
      "偏生命一次\n",
      "如果要到非他驚定\n",
      "單個甜不要緊理\n",
      "想天一笑亦會奴呼\n",
      "是靠世界很遠妄想\n",
      "卻可念旅與吐\n",
      "是生滋吧的我還好\n",
      "如果細慣\n",
      "等不愛 〔來〕自必一遠進處\n",
      "甚麼好 連手跟再擁抱\n",
      "一烈情 第一等\n",
      "你是我吧\n",
      "可歌可泣\n",
      "結果甚衷 或沒陪離\n",
      "就當天漆中覺四酒春誰會艷\n",
      "情走在化做息\n",
      "來若太高\n",
      "進旁口紙住你\n",
      "錯了怎麼算進當\n",
      "演你的早\n",
      "面對這份油負\n",
      "相對不發 繼直兩形\n",
      "容給愛 s也我會更加道\n",
      "whever 堂太碰\n",
      "令開 看著你\n",
      "看 這個愛\n",
      "無避兩樣的單在\n",
      "啊 假使不想自己\n",
      "為是眉目什麼都好\n",
      "我 愛的一樣\n",
      "溫親你 到講我對\n",
      "你的情世 比時過真應煙沒有花結\n",
      "若能怎麼可擔生 與多內\n",
      "移結你 已失戀 何沒有吵\n",
      "隨風狂逛\n",
      "展覽買遍發我\n",
      "不要孤單的下撲\n",
      "明明戀心 請房口車苦演當你原來\n",
      "及你間地老\n",
      "想放世 仍然是你\n",
      "仍然愛人\n",
      "Epoch: 4/30... Step: 0... Loss: 0.4873... Accuracy:89.402%...\n",
      "Epoch: 4/30... Step: 50... Loss: 0.2934... Accuracy:90.301%...\n",
      "Epoch: 4/30... Step: 100... Loss: 0.4195... Accuracy:89.949%...\n",
      "Epoch: 4/30... Step: 150... Loss: 0.3318... Accuracy:90.633%...\n",
      "Epoch: 4/30... Step: 200... Loss: 0.3406... Accuracy:90.625%...\n",
      "Epoch: 4/30... Step: 250... Loss: 0.2984... Accuracy:91.090%...\n",
      "Epoch: 4/30... Step: 300... Loss: 0.3390... Accuracy:91.102%...\n",
      "Epoch: 4/30... Step: 350... Loss: 0.2991... Accuracy:91.539%...\n",
      "Epoch: 4/30... Step: 400... Loss: 0.3372... Accuracy:91.812%...\n",
      "Epoch: 4/30... Step: 450... Loss: 0.2433... Accuracy:92.008%...\n",
      "Epoch: 4/30... Step: 500... Loss: 0.3213... Accuracy:92.188%...\n",
      "Epoch: 4/30... Step: 550... Loss: 0.2337... Accuracy:92.402%...\n",
      "Epoch: 4/30... Step: 600... Loss: 0.2890... Accuracy:92.359%...\n",
      "Epoch: 4/30... Step: 650... Loss: 0.2389... Accuracy:93.023%...\n",
      "Epoch: 4/30... Step: 700... Loss: 0.2856... Accuracy:93.020%...\n",
      "Epoch: 4/30... Step: 750... Loss: 0.2016... Accuracy:93.094%...\n",
      "Epoch: 4/30... Step: 800... Loss: 0.2795... Accuracy:92.883%...\n",
      "Epoch: 4/30... Step: 850... Loss: 0.1959... Accuracy:93.434%...\n",
      "Epoch: 4/30... Step: 900... Loss: 0.1955... Accuracy:93.418%...\n",
      "Epoch: 4/30... Step: 950... Loss: 0.2587... Accuracy:93.648%...\n",
      "Epoch: 4/30... Step: 1000... Loss: 0.2626... Accuracy:93.434%...\n",
      "Epoch: 4/30... Step: 1050... Loss: 0.1954... Accuracy:93.742%...\n",
      "\n",
      "----- 第Epoch: 3後自動寫詞\n",
      "----- diversity: 1.0\n",
      "----- 根據以下詞彙發想: \"到 你\"\n",
      "到 你的感情\n",
      "哼不出半聲\n",
      "承受的歌共\n",
      "罷也敗 也在心\n",
      "就算世界沒燈花\n",
      "會枯萎 多得歌\n",
      "多麼面對你\n",
      "再亂的 我是誰\n",
      "或辛 苦荷寶\n",
      "走光 像馬路\n",
      "同樣想像謊慰\n",
      "難e答只b電了 懷念在終\n",
      "路娃還錯意\n",
      "麼不起 在你那麼紅\n",
      "你能美 bacsim\n",
      "(ht give youg inever you\"\n",
      "情著秀麗\n",
      "閉報警 leame\n",
      "ditd yeu愛你\n",
      "據說只愛來回迴\n",
      "所暗中 也光陰\n",
      "在卻旁躍下暢\n",
      "就這於靜地\n",
      "你遊 給\n",
      "一個 怎相信\n",
      "男人感慨\n",
      "從前過花生知死\n",
      "請造經又要錯〕\n",
      "控訴我都告\n",
      "開始 先的離\n",
      "\n",
      "每天都世什\n",
      "無二個今天\n",
      "可太愛 oh a島ce力bd 傳d\n",
      "飄黑 我會平好\n",
      "oh p生ing whe dams wing it?\n",
      "wi gortime mels e節是輕的浪漫\n",
      "不如功長\n",
      "了生命的分紅\n",
      "像不 陪我出天\n",
      "從來煙花升初\n",
      "他的現在\n",
      "在當議的中吧\n",
      "分手 沒有蠟班 不再跳\n",
      "難道要包後擁再起\n",
      "愛你不可\n",
      "毫無的\n",
      "Epoch: 5/30... Step: 0... Loss: 0.1968... Accuracy:94.008%...\n",
      "Epoch: 5/30... Step: 50... Loss: 0.1550... Accuracy:93.988%...\n",
      "Epoch: 5/30... Step: 100... Loss: 0.2404... Accuracy:94.109%...\n",
      "Epoch: 5/30... Step: 150... Loss: 0.1890... Accuracy:94.531%...\n",
      "Epoch: 5/30... Step: 200... Loss: 0.2005... Accuracy:94.258%...\n",
      "Epoch: 5/30... Step: 250... Loss: 0.1956... Accuracy:94.672%...\n",
      "Epoch: 5/30... Step: 300... Loss: 0.1679... Accuracy:94.789%...\n",
      "Epoch: 5/30... Step: 350... Loss: 0.1568... Accuracy:94.652%...\n",
      "Epoch: 5/30... Step: 400... Loss: 0.1817... Accuracy:94.648%...\n",
      "Epoch: 5/30... Step: 450... Loss: 0.2062... Accuracy:95.203%...\n",
      "Epoch: 5/30... Step: 500... Loss: 0.1612... Accuracy:95.082%...\n",
      "Epoch: 5/30... Step: 550... Loss: 0.1658... Accuracy:94.953%...\n",
      "Epoch: 5/30... Step: 600... Loss: 0.1951... Accuracy:94.863%...\n",
      "Epoch: 5/30... Step: 650... Loss: 0.1414... Accuracy:95.230%...\n",
      "Epoch: 5/30... Step: 700... Loss: 0.1714... Accuracy:95.293%...\n",
      "Epoch: 5/30... Step: 750... Loss: 0.1399... Accuracy:95.492%...\n",
      "Epoch: 5/30... Step: 800... Loss: 0.1861... Accuracy:95.406%...\n",
      "Epoch: 5/30... Step: 850... Loss: 0.1391... Accuracy:95.270%...\n",
      "Epoch: 5/30... Step: 900... Loss: 0.1708... Accuracy:95.387%...\n",
      "Epoch: 5/30... Step: 950... Loss: 0.1155... Accuracy:95.719%...\n",
      "Epoch: 5/30... Step: 1000... Loss: 0.1583... Accuracy:95.637%...\n",
      "Epoch: 5/30... Step: 1050... Loss: 0.1289... Accuracy:95.598%...\n",
      "\n",
      "----- 第Epoch: 4後自動寫詞\n",
      "----- diversity: 1.0\n",
      "----- 根據以下詞彙發想: \"要\n",
      "少\"\n",
      "要\n",
      "少聚惹來\n",
      "回頭窄更場遺人\n",
      "單歌揭露 風光\n",
      "忘掉天 變得可\n",
      "同可同樣影生\n",
      "也聽 一晚會離累 也也感覺到\n",
      "誰說過份何看不到盡亮\n",
      "床代表情擁抱同個明天光陰\n",
      "眼問掉城麻\n",
      "就當我聽過\n",
      "而我只欣望著美如 說不話說都不離\n",
      "就是一種愛做\n",
      "合我一世\n",
      "未來進瞧了\n",
      "也未 這幸運\n",
      "到 你一世\n",
      "比這些年的地愛\n",
      "你不要感激厚加天路人\n",
      "從見錯貼著買密結地 竊望真偷\n",
      "算你 如在旁愈\n",
      "明日開心懂得親麼\n",
      "怎麼像看不慣\n",
      "只卻一場\n",
      "就如今天開得多\n",
      "亦無講 我聲年\n",
      "意要感動人\n",
      "才發生 知道花會oeo 笑著隱居你沒樣\n",
      "你當我說慨\n",
      "來年時跟他\n",
      "天考地我過甚麼\n",
      "無數首抵爭歌\n",
      "若我說你沒有\n",
      "可以 安用逝\n",
      "如果結束 溫疏的影\n",
      "\n",
      "情歌 一光滋味\n",
      "還是 這是風身雨\n",
      "責底是你神髮\n",
      "那是你的追中\n",
      "capin esres, miss cing thisg you\"\n",
      "i'爛 不早流言\n",
      "幸熱了善得\n",
      "淚 有拋得\n",
      "〔也許〕\n",
      "化再證 我一世\n",
      "沒有空 若無法\n",
      "停遇\n",
      "Epoch: 6/30... Step: 0... Loss: 0.1549... Accuracy:95.445%...\n",
      "Epoch: 6/30... Step: 50... Loss: 0.1478... Accuracy:95.844%...\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "epochs=30\n",
    "n_chars = len(chars)\n",
    "print('start epoch!')\n",
    "h = model.init_hidden(n_seqs)\n",
    "metrics=[]\n",
    "for epoch in range(epochs):\n",
    "    while counter<=1000:\n",
    "        for x, y in get_batches(encoded, n_seqs, n_steps):\n",
    "            x = one_hot_encode(x, n_chars).astype(np.float32)\n",
    "            x, y = torch.from_numpy(x), torch.from_numpy(y.astype(np.int64))\n",
    "\n",
    "            inputs, targets = Variable(x), Variable(y)\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            h = tuple([Variable(each.data) for each in h])\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            output, h = model.forward(inputs, h)\n",
    "\n",
    "            loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "            accu = 1-np.mean(np.not_equal(np.argmax(output.cpu().detach().numpy(), -1).astype(np.int64), targets.view(n_seqs*n_steps).cpu().detach().numpy().astype(np.int64)))\n",
    "            metrics.append(accu)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # 梯度截斷\n",
    "            #nn.utils.clip_grad_norm(model.parameters(), 5)\n",
    "\n",
    "            model_optimizer.step()\n",
    "            if counter % 50 == 0:\n",
    "                #列印訓練狀態\n",
    "                print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "                          \"Step: {}...\".format(counter),\n",
    "                          \"Loss: {:.4f}...\".format(loss.data.item()),\n",
    "                          \"Accuracy:{:.3%}...\".format(np.asarray(metrics).mean()))\n",
    "                metrics=[]\n",
    "            if (counter+1) % 100 == 0:\n",
    "                torch.save(model, 'Models/LingXi_pytorch_{0}.pth'.format(epoch))\n",
    "                torch.save(model, 'Models/LingXi_pytorch.pth')\n",
    "            counter += 1\n",
    "    write_something(epoch,model)\n",
    "    counter = 0\n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
