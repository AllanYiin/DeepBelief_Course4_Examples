{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 詞神林夕養成計畫(cntk)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "林夕是公認的華語流行歌的詞神，他寫了許多膾炙人口的好詞，那麼機器有沒有辦法模仿他寫詞的功夫呢?我們在此使用最簡單的Char-RNN，也就是利用LSTM預測下一個字，利用這樣的模型來模仿林夕的用字遣詞。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![md_images](../Images/charrnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "import math\n",
    "import codecs\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from cntk.initializer import *\n",
    "from cntk.layers import *\n",
    "from cntk.layers.models.attention import *\n",
    "from cntk.layers.typing import *\n",
    "from cntk.learners import adam, momentum_as_time_constant_schedule, learning_rate_schedule, UnitType\n",
    "from cntk.logging import log_number_of_parameters, ProgressPrinter\n",
    "from cntk.losses import *\n",
    "from cntk.metrics import classification_error\n",
    "from cntk.ops import *\n",
    "from cntk.train import Trainer\n",
    "from cntk.device import try_set_default_device, cpu,gpu\n",
    "\n",
    "# 是否使用GPU\n",
    "is_gpu = True\n",
    "\n",
    "if is_gpu:\n",
    "    try_set_default_device(gpu(0))\n",
    "else:\n",
    "    try_set_default_device(cpu())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我們在此提供了林夕(lingxi.txt)以及方文山(jay.txt)的歌詞語料，首先讀取語料。為何編碼不使用utf-8要改用utf-8-sig，這是因為在windows環境的utf-8格式與python不同，為了避免錯誤，因此使用utf-8-sig比較保險。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 52647\n"
     ]
    }
   ],
   "source": [
    "with io.open('lingxi.txt', encoding='utf-8-sig') as f:\n",
    "    text = f.read().lower()\n",
    "print('corpus length:', len(text))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "為了要把文字轉換為onehot編碼，首先我們需要統計他們所使用過的去重複字數。接著生成字轉索引(char_indices)以及反向的索引轉字(indices_char)的字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 2114\n"
     ]
    }
   ],
   "source": [
    "#把每個字去重複\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在這邊建構一個序列讀取器。請注意，由於cntk支持可變長度序列，因此可以一次放入一句歌詞，無論字數長短皆可，但要記得開頭與結尾需要放入<s><e>。由於是要預測下一個字，因此label等於feature位移一個字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 17536\n",
      "['你說你 從來未愛戀過\\n但很珍惜 跟我在消磨\\n我笑我 原來是我的錯\\n裂開的心 還未', ' 從來未愛戀過\\n但很珍惜 跟我在消磨\\n我笑我 原來是我的錯\\n裂開的心 還未算清楚', '未愛戀過\\n但很珍惜 跟我在消磨\\n我笑我 原來是我的錯\\n裂開的心 還未算清楚\\n如此']\n",
      "['說你 從來未愛戀過\\n但很珍惜 跟我在消磨\\n我笑我 原來是我的錯\\n裂開的心 還未算', '從來未愛戀過\\n但很珍惜 跟我在消磨\\n我笑我 原來是我的錯\\n裂開的心 還未算清楚\\n', '愛戀過\\n但很珍惜 跟我在消磨\\n我笑我 原來是我的錯\\n裂開的心 還未算清楚\\n如此天']\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen-1, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i+1:i + maxlen+1])\n",
    "print('nb sequences:', len(sentences))\n",
    "print(sentences[:3])\n",
    "print(next_chars[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx=0\n",
    "def word2onehot(w):\n",
    "    idx=char_indices[w]\n",
    "    arr=np.zeros(len(chars),dtype=np.float32)\n",
    "    arr[idx]=1\n",
    "    return arr\n",
    "\n",
    "\n",
    "def get_next_minibatch(minibatch_size=16):\n",
    "    global idx\n",
    "    features=[]\n",
    "    labels=[]\n",
    "    groundtruths=[]\n",
    "    while len(features)<minibatch_size:\n",
    "        features.append([word2onehot(s) for s in list(sentences[idx])]) \n",
    "        labels.append([word2onehot(s) for s in list(next_chars[idx])])\n",
    "        groundtruths.append( list(sentences[idx]))\n",
    "        idx+=1\n",
    "        if idx>len(sentences)-1:\n",
    "            idx=0\n",
    "    return features,labels,groundtruths\n",
    "\n",
    "#print(get_next_minibatch(3))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "為了避免模型柿子挑軟的吃，總是預測高頻率字(像是：你我他的了)，因此在這邊定義focal loss來促使模型預測非高頻字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def focal_loss(output, target, gamma=2, axis=-1):\n",
    "    return negate(reduce_sum(target * C.pow(1 - output, gamma) * log(output), axis))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "定義模型，一個嵌入層，連結兩層lstm，然後是dropout，以及兩個全連接層。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(input_sequence, vocab_dim=len(chars),num_layers=2, hidden_dim=512):\n",
    "    with default_options(enable_self_stabilization=True,init=he_uniform(0.02)):\n",
    "        rnn = Sequential([\n",
    "            C.layers.Embedding(hidden_dim),\n",
    "            For(range(num_layers), lambda: \n",
    "                Sequential([Stabilizer(), \n",
    "                            Recurrence(LSTM(hidden_dim), go_backwards=False),\n",
    "                           C.layers.BatchNormalization()])),\n",
    "            Dropout(0.5),\n",
    "            Dense(vocab_dim,activation=softmax)\n",
    "            ])\n",
    "    return rnn(input_sequence)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "溫度函數是為了避免模型一直選取常用字，因此調整各個詞彙的機率分布。在這邊temperature若是小於1，越容易產生意外或是有創意的字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds+10e-14) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / (np.sum(exp_preds)+10e-14)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_something(epoch,z):\n",
    "    print()\n",
    "    print('----- 第Epoch: %d後自動寫詞' % epoch)\n",
    "    z.save(\"Models/LingXi_%d.lstm\" % epoch)\n",
    "    z.save(\"Models/LingXi.lstm\")\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.8,1.0]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- 根據以下詞彙發想: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "        \n",
    "        for i in range(200):\n",
    "            try:\n",
    "                x_pred =[word2onehot(s) for s in list(sentence)]   \n",
    "                preds = z([x_pred])[0][-1]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_char = indices_char[next_index]\n",
    "\n",
    "                generated += next_char\n",
    "                sentence = sentence[1:] + next_char\n",
    "\n",
    "                sys.stdout.write(next_char)\n",
    "                sys.stdout.flush()\n",
    "            except:\n",
    "                generated+=''\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "定義訓練流程。每100 minibatch進行一次寫作測試，包括輸入一句完整歌詞以及輸入一個關鍵字自行發想。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "num_epochs=10\n",
    "minibatch_size=32\n",
    "#訓練漢字層級的下一個字預測模型\n",
    "def train():\n",
    "    global learning_rate,minibatch_size\n",
    "\n",
    "    #定義序列軸\n",
    "    input_seq_axis = Axis('inputAxis')\n",
    "    #定義輸入變數\n",
    "    input_sequence = sequence.input_variable(shape=len(chars), sequence_axis=input_seq_axis)\n",
    "    label_sequence = sequence.input_variable(shape=len(chars), sequence_axis=input_seq_axis)\n",
    "    \n",
    "    #定義模型輸出\n",
    "    z = create_model(input_sequence)\n",
    "    if os.path.exists(\"Models/LingXi.lstm\"):\n",
    "        model=Function.load(\"Models/LingXi.lstm\")\n",
    "        z = model(input_sequence)\n",
    "\n",
    "    #定義進行訓練的損失函數以及錯誤率計算\n",
    "    #loss=cross_entropy_with_softmax(z,label_sequence)\n",
    "    loss= cross_entropy_with_softmax(z,label_sequence)#+0.1*focal_loss(z,label_sequence)\n",
    "    errs = classification_error(z, label_sequence)\n",
    "    \n",
    "    # 列印模型參數\n",
    "    log_number_of_parameters(z);\n",
    "    print()\n",
    "    \n",
    "    # 定義訓練器\n",
    "    num_trained_samples_since_last_report = 0\n",
    "    progress_printer = ProgressPrinter(freq=20, tag='Training', num_epochs=300)\n",
    "    learner = adam(z.parameters,\n",
    "                    lr=learning_rate_schedule([learning_rate], UnitType.sample, 300),\n",
    "                    momentum=momentum_as_time_constant_schedule([minibatch_size / -math.log(0.95)], epoch_size=300),\n",
    "                    l2_regularization_weight=5e-4)\n",
    "    trainer = Trainer(z, (loss, errs), learner, progress_printer)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        mbs = 0\n",
    "        progress_printer.update_with_trainer(trainer, with_metric=True)\n",
    "        num_trained_samples = 0\n",
    "        while mbs<1000:\n",
    "            features, labels, truths=get_next_minibatch(minibatch_size)\n",
    "            #進行訓練\n",
    "            trainer.train_minibatch({input_sequence: features, label_sequence: labels})\n",
    " \n",
    "            if mbs%200==0 and mbs>0:\n",
    "                write_something(epoch,z)\n",
    "                learning_rate*=0.75\n",
    "            mbs += 1\n",
    "        #回報每個epoch訓練進度以及相關指標\n",
    "        trainer.summarize_training_progress()\n",
    "        #寫作測試\n",
    "        write_something(epoch,z)\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "執行訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 7402056 parameters in 24 parameter tensors.\n",
      "\n",
      "Learning rate per 1 samples: 0.001\n",
      " Minibatch[   1-  20]: loss = 7.654861 * 25600, metric = 95.98% * 25600;\n",
      " Minibatch[  21-  40]: loss = 7.598824 * 25600, metric = 92.89% * 25600;\n",
      " Minibatch[  41-  60]: loss = 7.583510 * 25600, metric = 92.00% * 25600;\n",
      " Minibatch[  61-  80]: loss = 7.590808 * 25600, metric = 93.12% * 25600;\n",
      " Minibatch[  81- 100]: loss = 7.571462 * 25600, metric = 90.58% * 25600;\n",
      " Minibatch[ 101- 120]: loss = 7.596580 * 25600, metric = 93.59% * 25600;\n",
      " Minibatch[ 121- 140]: loss = 7.584307 * 25600, metric = 92.45% * 25600;\n",
      " Minibatch[ 141- 160]: loss = 7.584023 * 25600, metric = 92.52% * 25600;\n",
      " Minibatch[ 161- 180]: loss = 7.587266 * 25600, metric = 92.80% * 25600;\n",
      " Minibatch[ 181- 200]: loss = 7.591128 * 25600, metric = 93.21% * 25600;\n",
      "\n",
      "----- 第Epoch: 0後自動寫詞\n",
      "----- diversity: 0.8\n",
      "----- 根據以下詞彙發想: \"巴不得我四季放暑假\n",
      "誰又不需要呵護 和自我競賽多痛苦\n",
      "沿途幸有你製造憧憬中所有夢\"\n",
      "巴不得我四季放暑假\n",
      "誰又不需要呵護 和自我競賽多痛苦\n",
      "沿途幸有你製造憧憬中所有夢                                                                                                                                                                                                        ----- diversity: 1.0\n",
      "----- 根據以下詞彙發想: \"巴不得我四季放暑假\n",
      "誰又不需要呵護 和自我競賽多痛苦\n",
      "沿途幸有你製造憧憬中所有夢\"\n",
      "巴不得我四季放暑假\n",
      "誰又不需要呵護 和自我競賽多痛苦\n",
      "沿途幸有你製造憧憬中所有夢                                                                                                                                                                                                         Minibatch[ 201- 220]: loss = 7.590815 * 25600, metric = 93.23% * 25600;\n",
      " Minibatch[ 221- 240]: loss = 7.583369 * 25600, metric = 92.08% * 25600;\n",
      " Minibatch[ 241- 260]: loss = 7.590293 * 25600, metric = 93.09% * 25600;\n",
      " Minibatch[ 261- 280]: loss = 7.578623 * 25600, metric = 91.96% * 25600;\n",
      " Minibatch[ 281- 300]: loss = 7.585107 * 25600, metric = 92.64% * 25600;\n",
      " Minibatch[ 301- 320]: loss = 7.577666 * 25600, metric = 91.71% * 25600;\n",
      " Minibatch[ 321- 340]: loss = 7.574561 * 25600, metric = 91.19% * 25600;\n",
      " Minibatch[ 341- 360]: loss = 7.552832 * 25600, metric = 89.03% * 25600;\n",
      " Minibatch[ 361- 380]: loss = 7.567520 * 25600, metric = 89.98% * 25600;\n",
      " Minibatch[ 381- 400]: loss = 7.574229 * 25600, metric = 91.23% * 25600;\n",
      "\n",
      "----- 第Epoch: 0後自動寫詞\n",
      "----- diversity: 0.8\n",
      "----- 根據以下詞彙發想: \"生最美麗的愛\n",
      "不管流言 流過蒼茫的人海\n",
      "等不到天昏地暗\n",
      "這就是我們命運最合理的安\"\n",
      "生最美麗的愛\n",
      "不管流言 流過蒼茫的人海\n",
      "等不到天昏地暗\n",
      "這就是我們命運最合理的安            屋\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- 根據以下詞彙發想: \"生最美麗的愛\n",
      "不管流言 流過蒼茫的人海\n",
      "等不到天昏地暗\n",
      "這就是我們命運最合理的安\"\n",
      "生最美麗的愛\n",
      "不管流言 流過蒼茫的人海\n",
      "等不到天昏地暗\n",
      "這就是我們命運最合理的安            轉\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Minibatch[ 401- 420]: loss = 7.592432 * 25600, metric = 93.06% * 25600;\n",
      " Minibatch[ 421- 440]: loss = 7.582656 * 25600, metric = 92.27% * 25600;\n",
      " Minibatch[ 441- 460]: loss = 7.587187 * 25600, metric = 92.86% * 25600;\n",
      " Minibatch[ 461- 480]: loss = 7.586250 * 25600, metric = 92.71% * 25600;\n",
      " Minibatch[ 481- 500]: loss = 7.581660 * 25600, metric = 92.16% * 25600;\n",
      " Minibatch[ 501- 520]: loss = 7.558223 * 25600, metric = 89.71% * 25600;\n",
      " Minibatch[ 521- 540]: loss = 7.557441 * 25600, metric = 89.41% * 25600;\n",
      " Minibatch[ 541- 560]: loss = 7.543750 * 25600, metric = 88.44% * 25600;\n",
      " Minibatch[ 561- 580]: loss = 7.559355 * 25600, metric = 90.01% * 25600;\n",
      " Minibatch[ 581- 600]: loss = 7.553965 * 25600, metric = 89.38% * 25600;\n",
      "\n",
      "----- 第Epoch: 0後自動寫詞\n",
      "----- diversity: 0.8\n",
      "----- 根據以下詞彙發想: \"開了也不再狼狽\n",
      "愛過的人不愛我也都無所謂\n",
      "我只是愛美\n",
      "你不喝的咖啡 留給我去品味\"\n",
      "開了也不再狼狽\n",
      "愛過的人不愛我也都無所謂\n",
      "我只是愛美\n",
      "你不喝的咖啡 留給我去品味\n",
      "\n",
      "\n",
      "嚼      \n",
      "\n",
      "\n",
      "乳      \n",
      "\n",
      "\n",
      "臨      \n",
      "\n",
      "\n",
      "要      \n",
      "\n",
      "\n",
      "捧      \n",
      "\n",
      "\n",
      "剛      \n",
      "\n",
      "\n",
      "＃      \n",
      "\n",
      "\n",
      "束      \n",
      "\n",
      "\n",
      "母      \n",
      "\n",
      "\n",
      "遺      \n",
      "\n",
      "\n",
      "舉      \n",
      "\n",
      "\n",
      "競      \n",
      "\n",
      "\n",
      "終      \n",
      "\n",
      "\n",
      "從      \n",
      "\n",
      "\n",
      "毒      \n",
      "\n",
      "\n",
      "幸      \n",
      "\n",
      "\n",
      "挫      \n",
      "\n",
      "\n",
      "每      \n",
      "\n",
      "\n",
      "征      \n",
      "\n",
      "\n",
      "濕      ----- diversity: 1.0\n",
      "----- 根據以下詞彙發想: \"開了也不再狼狽\n",
      "愛過的人不愛我也都無所謂\n",
      "我只是愛美\n",
      "你不喝的咖啡 留給我去品味\"\n",
      "開了也不再狼狽\n",
      "愛過的人不愛我也都無所謂\n",
      "我只是愛美\n",
      "你不喝的咖啡 留給我去品味\n",
      "\n",
      "衍話     \n",
      "\n",
      "\n",
      "血      \n",
      "\n",
      "\n",
      "諧      \n",
      "\n",
      "\n",
      "址      \n",
      "\n",
      "\n",
      "廚      \n",
      "\n",
      "\n",
      "跑      \n",
      "\n",
      "\n",
      "竟      \n",
      "\n",
      "\n",
      "柄      \n",
      "\n",
      "\n",
      "搶      \n",
      "\n",
      "\n",
      "陌      \n",
      "\n",
      "\n",
      "伏      \n",
      "\n",
      "\n",
      "先      \n",
      "\n",
      "\n",
      "亮      \n",
      "\n",
      "\n",
      "歷      \n",
      "\n",
      "\n",
      "啊      \n",
      "\n",
      "\n",
      "店      \n",
      "\n",
      "\n",
      "丈      \n",
      "\n",
      "\n",
      "造      \n",
      "\n",
      "\n",
      "這      \n",
      "\n",
      "\n",
      "髮      \n",
      " Minibatch[ 601- 620]: loss = 7.555156 * 25600, metric = 89.60% * 25600;\n",
      " Minibatch[ 621- 640]: loss = 7.554355 * 25600, metric = 89.43% * 25600;\n",
      " Minibatch[ 641- 660]: loss = 7.552812 * 25600, metric = 89.43% * 25600;\n",
      " Minibatch[ 661- 680]: loss = 7.554375 * 25600, metric = 89.53% * 25600;\n",
      " Minibatch[ 681- 700]: loss = 7.545215 * 25600, metric = 88.24% * 25600;\n",
      " Minibatch[ 701- 720]: loss = 7.553574 * 25600, metric = 89.45% * 25600;\n",
      " Minibatch[ 721- 740]: loss = 7.599414 * 25600, metric = 94.01% * 25600;\n",
      " Minibatch[ 741- 760]: loss = 7.553438 * 25600, metric = 89.39% * 25600;\n",
      " Minibatch[ 761- 780]: loss = 7.545898 * 25600, metric = 88.63% * 25600;\n",
      " Minibatch[ 781- 800]: loss = 7.549590 * 25600, metric = 88.95% * 25600;\n",
      "\n",
      "----- 第Epoch: 0後自動寫詞\n",
      "----- diversity: 0.8\n",
      "----- 根據以下詞彙發想: \"代\n",
      "我要你現在\n",
      "眼看你的擁抱剩下溫暖的指紋\n",
      "體驗一段感情如何變成了永恆\n",
      "我在你的\"\n",
      "代\n",
      "我要你現在\n",
      "眼看你的擁抱剩下溫暖的指紋\n",
      "體驗一段感情如何變成了永恆\n",
      "我在你的\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "   \n",
      "                           粗 \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      "                                \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      "                                 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "----- diversity: 1.0\n",
      "----- 根據以下詞彙發想: \"代\n",
      "我要你現在\n",
      "眼看你的擁抱剩下溫暖的指紋\n",
      "體驗一段感情如何變成了永恆\n",
      "我在你的\"\n",
      "代\n",
      "我要你現在\n",
      "眼看你的擁抱剩下溫暖的指紋\n",
      "體驗一段感情如何變成了永恆\n",
      "我在你的\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "                               \n",
      "  \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "  \n",
      "                                \n",
      "\n",
      "她\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "摩\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "                                  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "  Minibatch[ 801- 820]: loss = 7.548730 * 25600, metric = 89.00% * 25600;\n",
      " Minibatch[ 821- 840]: loss = 7.543965 * 25600, metric = 88.59% * 25600;\n",
      " Minibatch[ 841- 860]: loss = 7.546172 * 25600, metric = 88.77% * 25600;\n",
      " Minibatch[ 861- 880]: loss = 7.548164 * 25600, metric = 89.00% * 25600;\n",
      " Minibatch[ 881- 900]: loss = 7.553828 * 25600, metric = 89.34% * 25600;\n",
      " Minibatch[ 901- 920]: loss = 7.592969 * 25600, metric = 93.38% * 25600;\n",
      " Minibatch[ 921- 940]: loss = 7.599453 * 25600, metric = 93.52% * 25600;\n",
      " Minibatch[ 941- 960]: loss = 7.592109 * 25600, metric = 93.36% * 25600;\n",
      " Minibatch[ 961- 980]: loss = 7.579570 * 25600, metric = 92.03% * 25600;\n",
      " Minibatch[ 981-1000]: loss = 7.586563 * 25600, metric = 92.83% * 25600;\n",
      "Finished Epoch[1 of 300]: [Training] loss = 7.573521 * 1280000, metric = 91.23% * 1280000 150.841s (8485.8 samples/s);\n",
      "\n",
      "----- 第Epoch: 0後自動寫詞\n",
      "----- diversity: 0.8\n",
      "----- 根據以下詞彙發想: \"盡面前路\n",
      "夢想中的彼岸為何還未到\n",
      "明明我已奮力無間 天天上路\n",
      "我不死也為活得好\n",
      "\"\n",
      "盡面前路\n",
      "夢想中的彼岸為何還未到\n",
      "明明我已奮力無間 天天上路\n",
      "我不死也為活得好\n",
      "                                                                                                                                                                                                        ----- diversity: 1.0\n",
      "----- 根據以下詞彙發想: \"盡面前路\n",
      "夢想中的彼岸為何還未到\n",
      "明明我已奮力無間 天天上路\n",
      "我不死也為活得好\n",
      "\"\n",
      "盡面前路\n",
      "夢想中的彼岸為何還未到\n",
      "明明我已奮力無間 天天上路\n",
      "我不死也為活得好\n",
      "                                                                                                                                                                                                         Minibatch[   1-  20]: loss = 7.576434 * 25600, metric = 91.73% * 25600;\n",
      " Minibatch[  21-  40]: loss = 7.543663 * 25600, metric = 88.56% * 25600;\n",
      " Minibatch[  41-  60]: loss = 7.545846 * 25600, metric = 88.82% * 25600;\n",
      " Minibatch[  61-  80]: loss = 7.538079 * 25600, metric = 88.03% * 25600;\n",
      " Minibatch[  81- 100]: loss = 7.549653 * 25600, metric = 89.16% * 25600;\n",
      " Minibatch[ 101- 120]: loss = 7.546121 * 25600, metric = 88.78% * 25600;\n",
      " Minibatch[ 121- 140]: loss = 7.542183 * 25600, metric = 88.43% * 25600;\n",
      " Minibatch[ 141- 160]: loss = 7.547036 * 25600, metric = 88.86% * 25600;\n",
      " Minibatch[ 161- 180]: loss = 7.542529 * 25600, metric = 88.45% * 25600;\n",
      " Minibatch[ 181- 200]: loss = 7.531084 * 25600, metric = 87.29% * 25600;\n",
      "\n",
      "----- 第Epoch: 1後自動寫詞\n",
      "----- diversity: 0.8\n",
      "----- 根據以下詞彙發想: \"錯\n",
      "若我說我愿意誰在乎過\n",
      "你知道那答案快提示我\n",
      "總有天親耳聽到 我愛侶對天宣布\n",
      "\"\n",
      "錯\n",
      "若我說我愿意誰在乎過\n",
      "你知道那答案快提示我\n",
      "總有天親耳聽到 我愛侶對天宣布\n",
      "     \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    格\n",
      "    \n",
      "    \n",
      "     \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "     \n",
      "    \n",
      "    \n",
      "    \n",
      "     \n",
      "    \n",
      "    \n",
      "    \n",
      "     \n",
      "    \n",
      "     \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "     \n",
      "    \n",
      "    \n",
      "     \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      " ----- diversity: 1.0\n",
      "----- 根據以下詞彙發想: \"錯\n",
      "若我說我愿意誰在乎過\n",
      "你知道那答案快提示我\n",
      "總有天親耳聽到 我愛侶對天宣布\n",
      "\"\n",
      "錯\n",
      "若我說我愿意誰在乎過\n",
      "你知道那答案快提示我\n",
      "總有天親耳聽到 我愛侶對天宣布\n",
      "    賭\n",
      "     \n",
      "     \n",
      "    \n",
      "    \n",
      "    其\n",
      "     \n",
      "    \n",
      "    \n",
      "    \n",
      "     \n",
      "    \n",
      "    \n",
      "    \n",
      "     \n",
      "    \n",
      "    \n",
      "    \n",
      "     \n",
      "    \n",
      "    \n",
      "     \n",
      "    \n",
      "    氏\n",
      "     \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "     \n",
      "    \n",
      "    \n",
      "     \n",
      "    \n",
      "    \n",
      "討   \n",
      "   Minibatch[ 201- 220]: loss = 7.559609 * 25600, metric = 90.19% * 25600;\n",
      " Minibatch[ 221- 240]: loss = 7.541943 * 25600, metric = 88.33% * 25600;\n",
      " Minibatch[ 241- 260]: loss = 7.543779 * 25600, metric = 88.54% * 25600;\n",
      " Minibatch[ 261- 280]: loss = 7.557236 * 25600, metric = 89.93% * 25600;\n",
      " Minibatch[ 281- 300]: loss = 7.550615 * 25600, metric = 89.22% * 25600;\n",
      " Minibatch[ 301- 320]: loss = 7.540957 * 25600, metric = 88.24% * 25600;\n",
      " Minibatch[ 321- 340]: loss = 7.538672 * 25600, metric = 87.94% * 25600;\n",
      " Minibatch[ 341- 360]: loss = 7.536201 * 25600, metric = 87.84% * 25600;\n",
      " Minibatch[ 361- 380]: loss = 7.536611 * 25600, metric = 87.89% * 25600;\n",
      " Minibatch[ 381- 400]: loss = 7.541016 * 25600, metric = 88.18% * 25600;\n",
      "\n",
      "----- 第Epoch: 1後自動寫詞\n",
      "----- diversity: 0.8\n",
      "----- 根據以下詞彙發想: \"場\n",
      "好想好好的 好好的 想放浪\n",
      "但是沒事忙 想快樂 卻沒有蜜糖\n",
      "風雨過後不一定有\"\n",
      "場\n",
      "好想好好的 好好的 想放浪\n",
      "但是沒事忙 想快樂 卻沒有蜜糖\n",
      "風雨過後不一定有\n",
      "     \n",
      "    \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "    \n",
      "     \n",
      "    \n",
      "     \n",
      "    \n",
      "     \n",
      "    \n",
      "     \n",
      "    \n",
      "     \n",
      "     \n",
      "     \n",
      "\n",
      "      \n",
      "     \n",
      "    \n",
      "     \n",
      "     \n",
      "    \n",
      "     \n",
      "    \n",
      "     \n",
      "     \n",
      "田    \n",
      "     \n",
      "     \n",
      "    \n",
      "   ----- diversity: 1.0\n",
      "----- 根據以下詞彙發想: \"場\n",
      "好想好好的 好好的 想放浪\n",
      "但是沒事忙 想快樂 卻沒有蜜糖\n",
      "風雨過後不一定有\"\n",
      "場\n",
      "好想好好的 好好的 想放浪\n",
      "但是沒事忙 想快樂 卻沒有蜜糖\n",
      "風雨過後不一定有\n",
      "     \n",
      "  闊  \n",
      "     \n",
      "     \n",
      "\n",
      "      \n",
      "     \n",
      "    \n",
      "     \n",
      "    燈\n",
      "     \n",
      "     \n",
      "     \n",
      "    \n",
      "    \n",
      "     \n",
      "    \n",
      "     \n",
      "     \n",
      "     \n",
      "    運\n",
      "     \n",
      "  墨  \n",
      "    \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "    \n",
      "     \n",
      "    艷\n",
      "    \n",
      "     \n",
      "\n",
      "      \n",
      "     Minibatch[ 401- 420]: loss = 7.543330 * 25600, metric = 88.48% * 25600;\n",
      " Minibatch[ 421- 440]: loss = 7.547402 * 25600, metric = 88.92% * 25600;\n",
      " Minibatch[ 441- 460]: loss = 7.542813 * 25600, metric = 88.45% * 25600;\n",
      " Minibatch[ 461- 480]: loss = 7.552246 * 25600, metric = 89.38% * 25600;\n",
      " Minibatch[ 481- 500]: loss = 7.551543 * 25600, metric = 89.29% * 25600;\n",
      " Minibatch[ 501- 520]: loss = 7.551465 * 25600, metric = 89.34% * 25600;\n",
      " Minibatch[ 521- 540]: loss = 7.541699 * 25600, metric = 88.25% * 25600;\n",
      " Minibatch[ 541- 560]: loss = 7.550020 * 25600, metric = 89.18% * 25600;\n",
      " Minibatch[ 561- 580]: loss = 7.541367 * 25600, metric = 88.27% * 25600;\n",
      " Minibatch[ 581- 600]: loss = 7.534238 * 25600, metric = 87.61% * 25600;\n",
      "\n",
      "----- 第Epoch: 1後自動寫詞\n",
      "----- diversity: 0.8\n",
      "----- 根據以下詞彙發想: \"的臉 幽怨迷離像眼前\n",
      "從未試過這滋味 苦澀茫然又帶甜\n",
      "當你默默道別而不知是否會再\"\n",
      "的臉 幽怨迷離像眼前\n",
      "從未試過這滋味 苦澀茫然又帶甜\n",
      "當你默默道別而不知是否會再\n",
      "\n",
      "        \n",
      "     \n",
      "      \n",
      "      \n",
      "     \n",
      "     \n",
      "      \n",
      "      \n",
      "      \n",
      "\n",
      "       \n",
      "      \n",
      "      \n",
      "     \n",
      "      \n",
      "      \n",
      "     \n",
      "      \n",
      "       \n",
      "\n",
      "        \n",
      "\n",
      "        \n",
      "      \n",
      "      \n",
      "泳     \n",
      "     \n",
      "      \n",
      "      \n",
      "      \n",
      "    ----- diversity: 1.0\n",
      "----- 根據以下詞彙發想: \"的臉 幽怨迷離像眼前\n",
      "從未試過這滋味 苦澀茫然又帶甜\n",
      "當你默默道別而不知是否會再\"\n",
      "的臉 幽怨迷離像眼前\n",
      "從未試過這滋味 苦澀茫然又帶甜\n",
      "當你默默道別而不知是否會再\n",
      "\n",
      "        \n",
      "\n",
      "        \n",
      "      \n",
      "\n",
      "        \n",
      "     \n",
      "      \n",
      "\n",
      "        \n",
      "\n",
      "        \n",
      "     \n",
      "      \n",
      "      \n",
      "      \n",
      " 患    \n",
      "     \n",
      "      \n",
      "\n",
      "        \n",
      "\n",
      "        \n",
      "\n",
      "        \n",
      "\n",
      "        \n",
      "      \n",
      "\n",
      "        \n",
      "     \n",
      "      \n",
      "      \n",
      "      Minibatch[ 601- 620]: loss = 7.537656 * 25600, metric = 87.86% * 25600;\n",
      " Minibatch[ 621- 640]: loss = 7.546055 * 25600, metric = 88.72% * 25600;\n",
      " Minibatch[ 641- 660]: loss = 7.539219 * 25600, metric = 88.04% * 25600;\n",
      " Minibatch[ 661- 680]: loss = 7.548555 * 25600, metric = 89.02% * 25600;\n",
      " Minibatch[ 681- 700]: loss = 7.539961 * 25600, metric = 88.12% * 25600;\n",
      " Minibatch[ 701- 720]: loss = 7.544922 * 25600, metric = 88.64% * 25600;\n",
      " Minibatch[ 721- 740]: loss = 7.527148 * 25600, metric = 86.82% * 25600;\n",
      " Minibatch[ 741- 760]: loss = 7.553945 * 25600, metric = 89.59% * 25600;\n",
      " Minibatch[ 761- 780]: loss = 7.543262 * 25600, metric = 88.53% * 25600;\n",
      " Minibatch[ 781- 800]: loss = 7.539902 * 25600, metric = 88.10% * 25600;\n",
      "\n",
      "----- 第Epoch: 1後自動寫詞\n",
      "----- diversity: 0.8\n",
      "----- 根據以下詞彙發想: \"想念我\n",
      "証明你也明白 太愛一個人的寂寞\n",
      "隔著枕頭對我說\n",
      "你甚麼時候在哪裡吻我\n",
      "你\"\n",
      "想念我\n",
      "証明你也明白 太愛一個人的寂寞\n",
      "隔著枕頭對我說\n",
      "你甚麼時候在哪裡吻我\n",
      "你        \n",
      "        \n",
      "        \n",
      "\n",
      "          \n",
      "       \n",
      "        \n",
      "\n",
      "           \n",
      "\n",
      "          \n",
      "        \n",
      "\n",
      "           \n",
      "       \n",
      "        \n",
      "        \n",
      "\n",
      "           \n",
      "\n",
      "          \n",
      "       \n",
      "        \n",
      "\n",
      "          \n",
      "        \n",
      "        ----- diversity: 1.0\n",
      "----- 根據以下詞彙發想: \"想念我\n",
      "証明你也明白 太愛一個人的寂寞\n",
      "隔著枕頭對我說\n",
      "你甚麼時候在哪裡吻我\n",
      "你\"\n",
      "想念我\n",
      "証明你也明白 太愛一個人的寂寞\n",
      "隔著枕頭對我說\n",
      "你甚麼時候在哪裡吻我\n",
      "你         \n",
      "\n",
      "           \n",
      "\n",
      "           \n",
      "        \n",
      "\n",
      "          \n",
      "\n",
      "           \n",
      "\n",
      "           \n",
      "\n",
      "           \n",
      "\n",
      "          \n",
      "       \n",
      "        \n",
      "\n",
      "          \n",
      "       \n",
      "        \n",
      "\n",
      "晝          \n",
      "\n",
      "          \n",
      "       \n",
      "        \n",
      "     Minibatch[ 801- 820]: loss = 7.544219 * 25600, metric = 88.52% * 25600;\n",
      " Minibatch[ 821- 840]: loss = 7.554883 * 25600, metric = 89.71% * 25600;\n",
      " Minibatch[ 841- 860]: loss = 7.540039 * 25600, metric = 88.04% * 25600;\n",
      " Minibatch[ 861- 880]: loss = 7.527500 * 25600, metric = 86.85% * 25600;\n",
      " Minibatch[ 881- 900]: loss = 7.543828 * 25600, metric = 88.45% * 25600;\n",
      " Minibatch[ 901- 920]: loss = 7.531367 * 25600, metric = 87.24% * 25600;\n",
      " Minibatch[ 921- 940]: loss = 7.538555 * 25600, metric = 88.04% * 25600;\n",
      " Minibatch[ 941- 960]: loss = 7.543281 * 25600, metric = 88.33% * 25600;\n",
      " Minibatch[ 961- 980]: loss = 7.542422 * 25600, metric = 88.35% * 25600;\n",
      " Minibatch[ 981-1000]: loss = 7.543320 * 25600, metric = 88.47% * 25600;\n",
      "Finished Epoch[2 of 300]: [Training] loss = 7.543909 * 1280000, metric = 88.54% * 1280000 163.817s (7813.6 samples/s);\n",
      "\n",
      "----- 第Epoch: 1後自動寫詞\n",
      "----- diversity: 0.8\n",
      "----- 根據以下詞彙發想: \"回頭\n",
      "難道就這樣回不到我們最快樂時候\n",
      "無欲無求在鬧市夜游 成為終生戰友\n",
      "從來受慣\"\n",
      "回頭\n",
      "難道就這樣回不到我們最快樂時候\n",
      "無欲無求在鬧市夜游 成為終生戰友\n",
      "從來受慣    \n",
      "      \n",
      "\n",
      "        \n",
      "      \n",
      "      \n",
      "      \n",
      "      \n",
      "       \n",
      "     \n",
      "      \n",
      "      \n",
      "      \n",
      "      \n",
      "     \n",
      "       \n",
      "      \n",
      "      \n",
      "      \n",
      "      \n",
      "      \n",
      "     \n",
      "      \n",
      "      \n",
      "      \n",
      "      \n",
      "       \n",
      "      \n",
      "      \n",
      "   ----- diversity: 1.0\n",
      "----- 根據以下詞彙發想: \"回頭\n",
      "難道就這樣回不到我們最快樂時候\n",
      "無欲無求在鬧市夜游 成為終生戰友\n",
      "從來受慣\"\n",
      "回頭\n",
      "難道就這樣回不到我們最快樂時候\n",
      "無欲無求在鬧市夜游 成為終生戰友\n",
      "從來受慣    \n",
      "      \n",
      "      \n",
      "      \n",
      "      \n",
      "      \n",
      "      \n",
      "久     \n",
      "      \n",
      "      \n",
      "      \n",
      "      \n",
      "      \n",
      "      \n",
      "      \n",
      "      \n",
      "捧     \n",
      "      \n",
      "      \n",
      "      \n",
      "     另 \n",
      "      \n",
      "       \n",
      "      \n",
      "      \n",
      "      \n",
      "     \n",
      "      \n",
      "  裏   Minibatch[   1-  20]: loss = 7.543093 * 25600, metric = 88.52% * 25600;\n",
      " Minibatch[  21-  40]: loss = 7.550540 * 25600, metric = 89.15% * 25600;\n",
      " Minibatch[  41-  60]: loss = 7.553362 * 25600, metric = 89.54% * 25600;\n",
      " Minibatch[  61-  80]: loss = 7.541462 * 25600, metric = 88.26% * 25600;\n",
      " Minibatch[  81- 100]: loss = 7.550476 * 25600, metric = 89.23% * 25600;\n",
      " Minibatch[ 101- 120]: loss = 7.542971 * 25600, metric = 88.46% * 25600;\n",
      " Minibatch[ 121- 140]: loss = 7.531436 * 25600, metric = 87.26% * 25600;\n",
      " Minibatch[ 141- 160]: loss = 7.539048 * 25600, metric = 88.11% * 25600;\n",
      " Minibatch[ 161- 180]: loss = 7.541812 * 25600, metric = 88.29% * 25600;\n",
      " Minibatch[ 181- 200]: loss = 7.538496 * 25600, metric = 88.02% * 25600;\n",
      "\n",
      "----- 第Epoch: 2後自動寫詞\n",
      "----- diversity: 0.8\n",
      "----- 根據以下詞彙發想: \"這叫幸福 不怕流逝\n",
      "任他們多漂亮 未及你矜貴\n",
      "記憶無論最輕 輕不過脈搏聲\n",
      "靠你的\"\n",
      "這叫幸福 不怕流逝\n",
      "任他們多漂亮 未及你矜貴\n",
      "記憶無論最輕 輕不過脈搏聲\n",
      "靠你的   \n",
      "     \n",
      "      \n",
      "     \n",
      "     \n",
      "    塘\n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "\n",
      "       \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "      ----- diversity: 1.0\n",
      "----- 根據以下詞彙發想: \"這叫幸福 不怕流逝\n",
      "任他們多漂亮 未及你矜貴\n",
      "記憶無論最輕 輕不過脈搏聲\n",
      "靠你的\"\n",
      "這叫幸福 不怕流逝\n",
      "任他們多漂亮 未及你矜貴\n",
      "記憶無論最輕 輕不過脈搏聲\n",
      "靠你的   k\n",
      "      \n",
      "     \n",
      "     \n",
      "     \n",
      "\n",
      "       \n",
      "     \n",
      "     \n",
      "     \n",
      "魄    \n",
      "      \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "      \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "      \n",
      "     \n",
      "      \n",
      "      \n",
      "\n",
      "      \n",
      "     \n",
      "     \n",
      "      \n",
      "    Minibatch[ 201- 220]: loss = 7.546182 * 25600, metric = 88.83% * 25600;\n",
      " Minibatch[ 221- 240]: loss = 7.539160 * 25600, metric = 88.04% * 25600;\n",
      " Minibatch[ 241- 260]: loss = 7.544648 * 25600, metric = 88.61% * 25600;\n",
      " Minibatch[ 261- 280]: loss = 7.533525 * 25600, metric = 87.50% * 25600;\n",
      " Minibatch[ 281- 300]: loss = 7.536494 * 25600, metric = 87.77% * 25600;\n",
      " Minibatch[ 301- 320]: loss = 7.554951 * 25600, metric = 89.71% * 25600;\n",
      " Minibatch[ 321- 340]: loss = 7.534141 * 25600, metric = 87.59% * 25600;\n",
      " Minibatch[ 341- 360]: loss = 7.539404 * 25600, metric = 88.01% * 25600;\n",
      " Minibatch[ 361- 380]: loss = 7.558232 * 25600, metric = 90.00% * 25600;\n",
      " Minibatch[ 381- 400]: loss = 7.538252 * 25600, metric = 87.90% * 25600;\n",
      "\n",
      "----- 第Epoch: 2後自動寫詞\n",
      "----- diversity: 0.8\n",
      "----- 根據以下詞彙發想: \"要不是鐘擺 忽然停下來\n",
      "怎能體會 過去是這樣愉快\n",
      "走過人山人海\n",
      "眼看著煙火燦爛的\"\n",
      "要不是鐘擺 忽然停下來\n",
      "怎能體會 過去是這樣愉快\n",
      "走過人山人海\n",
      "眼看著煙火燦爛的 \n",
      "       \n",
      "       \n",
      "       \n",
      "       \n",
      "        \n",
      "       \n",
      "       \n",
      "      \n",
      "       \n",
      "       \n",
      "       \n",
      "       \n",
      "       \n",
      "       \n",
      "       \n",
      "       \n",
      "      \n",
      "       \n",
      "        \n",
      "      \n",
      "       \n",
      "       \n",
      "       \n",
      "       \n",
      "       ----- diversity: 1.0\n",
      "----- 根據以下詞彙發想: \"要不是鐘擺 忽然停下來\n",
      "怎能體會 過去是這樣愉快\n",
      "走過人山人海\n",
      "眼看著煙火燦爛的\"\n",
      "要不是鐘擺 忽然停下來\n",
      "怎能體會 過去是這樣愉快\n",
      "走過人山人海\n",
      "眼看著煙火燦爛的 \n",
      "       \n",
      "      \n",
      "       男\n",
      "       \n",
      "       \n",
      "       \n",
      "       \n",
      "       \n",
      "       \n",
      "        \n",
      "       \n",
      "       介\n",
      "       \n",
      "       \n",
      "       \n",
      "       \n",
      "       \n",
      "       \n",
      " 冒    \n",
      "       \n",
      " 武     \n",
      "      \n",
      "       \n",
      "       \n",
      "       Minibatch[ 401- 420]: loss = 7.538506 * 25600, metric = 87.98% * 25600;\n",
      " Minibatch[ 421- 440]: loss = 7.531836 * 25600, metric = 87.27% * 25600;\n",
      " Minibatch[ 441- 460]: loss = 7.534570 * 25600, metric = 87.51% * 25600;\n",
      " Minibatch[ 461- 480]: loss = 7.533223 * 25600, metric = 87.46% * 25600;\n",
      " Minibatch[ 481- 500]: loss = 7.541953 * 25600, metric = 88.36% * 25600;\n",
      " Minibatch[ 501- 520]: loss = 7.545273 * 25600, metric = 88.63% * 25600;\n",
      " Minibatch[ 521- 540]: loss = 7.544082 * 25600, metric = 88.49% * 25600;\n",
      " Minibatch[ 541- 560]: loss = 7.544551 * 25600, metric = 88.62% * 25600;\n",
      " Minibatch[ 561- 580]: loss = 7.547383 * 25600, metric = 88.92% * 25600;\n",
      " Minibatch[ 581- 600]: loss = 7.546680 * 25600, metric = 88.82% * 25600;\n",
      "\n",
      "----- 第Epoch: 2後自動寫詞\n",
      "----- diversity: 0.8\n",
      "----- 根據以下詞彙發想: \"便無力再轉動\n",
      "無奈你的愛是隨時失蹤\n",
      "在你眼光背後 是那些內容\n",
      "無謂再懂 無謂再動\"\n",
      "便無力再轉動\n",
      "無奈你的愛是隨時失蹤\n",
      "在你眼光背後 是那些內容\n",
      "無謂再懂 無謂再動\n",
      "\n",
      "\n",
      "        \n",
      "      \n",
      "     \n",
      "      \n",
      "\n",
      "        \n",
      "\n",
      "        \n",
      "      \n",
      "      \n",
      "\n",
      "        \n",
      "     \n",
      "      \n",
      "      \n",
      "      \n",
      "     \n",
      "      \n",
      "     \n",
      "      \n",
      "      \n",
      "     \n",
      "      \n",
      "     \n",
      "      \n",
      "      \n",
      "     \n",
      "      \n",
      "      \n",
      "\n",
      "        \n",
      " ----- diversity: 1.0\n",
      "----- 根據以下詞彙發想: \"便無力再轉動\n",
      "無奈你的愛是隨時失蹤\n",
      "在你眼光背後 是那些內容\n",
      "無謂再懂 無謂再動\"\n",
      "便無力再轉動\n",
      "無奈你的愛是隨時失蹤\n",
      "在你眼光背後 是那些內容\n",
      "無謂再懂 無謂再動\n",
      "穴     \n",
      " 璀    \n",
      "     \n",
      "     \n",
      "       \n",
      "\n",
      "       \n",
      "      \n",
      "     \n",
      "      \n",
      "     范\n",
      "   禮   \n",
      "\n",
      "       \n",
      "      \n",
      "      \n",
      "     \n",
      "      \n",
      "     \n",
      "      \n",
      "      \n",
      "     \n",
      "      \n",
      "     \n",
      "      \n",
      "      \n",
      "\n",
      "        \n",
      "      \n",
      "      \n",
      "      \n",
      "  Minibatch[ 601- 620]: loss = 7.544805 * 25600, metric = 88.64% * 25600;\n",
      " Minibatch[ 621- 640]: loss = 7.547168 * 25600, metric = 88.84% * 25600;\n",
      " Minibatch[ 641- 660]: loss = 7.544395 * 25600, metric = 88.57% * 25600;\n",
      " Minibatch[ 661- 680]: loss = 7.537598 * 25600, metric = 87.95% * 25600;\n",
      " Minibatch[ 681- 700]: loss = 7.535977 * 25600, metric = 87.82% * 25600;\n",
      " Minibatch[ 701- 720]: loss = 7.536699 * 25600, metric = 87.78% * 25600;\n",
      " Minibatch[ 721- 740]: loss = 7.541172 * 25600, metric = 88.18% * 25600;\n",
      " Minibatch[ 741- 760]: loss = 7.541934 * 25600, metric = 88.36% * 25600;\n",
      " Minibatch[ 761- 780]: loss = 7.538301 * 25600, metric = 87.99% * 25600;\n",
      " Minibatch[ 781- 800]: loss = 7.540879 * 25600, metric = 88.20% * 25600;\n",
      "\n",
      "----- 第Epoch: 2後自動寫詞\n",
      "----- diversity: 0.8\n",
      "----- 根據以下詞彙發想: \"地 仿似咬咬空氣\n",
      "事不關己\n",
      "一不清醒關起答錄機\n",
      "繼續聽著懷舊唱機\n",
      "明明過份依戀不\"\n",
      "地 仿似咬咬空氣\n",
      "事不關己\n",
      "一不清醒關起答錄機\n",
      "繼續聽著懷舊唱機\n",
      "明明過份依戀不\n",
      "    \n",
      "    \n",
      "     \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "     \n",
      "    \n",
      "     \n",
      "\n",
      "      \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "     \n",
      "    \n",
      "    \n",
      "    \n",
      "     \n",
      "\n",
      "      \n",
      "    \n",
      "    \n",
      "     \n",
      "    \n",
      "    \n",
      "    \n",
      "     \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      " ----- diversity: 1.0\n",
      "----- 根據以下詞彙發想: \"地 仿似咬咬空氣\n",
      "事不關己\n",
      "一不清醒關起答錄機\n",
      "繼續聽著懷舊唱機\n",
      "明明過份依戀不\"\n",
      "地 仿似咬咬空氣\n",
      "事不關己\n",
      "一不清醒關起答錄機\n",
      "繼續聽著懷舊唱機\n",
      "明明過份依戀不\n",
      "     \n",
      "\n",
      "      查\n",
      "\n",
      "      \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "     \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "     \n",
      "    \n",
      "    \n",
      "    \n",
      "     \n",
      "    \n",
      "    \n",
      "     \n",
      "    \n",
      "    \n",
      "     \n",
      "\n",
      "      \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "     \n",
      "\n",
      "       Minibatch[ 801- 820]: loss = 7.544375 * 25600, metric = 88.55% * 25600;\n",
      " Minibatch[ 821- 840]: loss = 7.524805 * 25600, metric = 86.59% * 25600;\n",
      " Minibatch[ 841- 860]: loss = 7.551328 * 25600, metric = 89.30% * 25600;\n",
      " Minibatch[ 861- 880]: loss = 7.545527 * 25600, metric = 88.70% * 25600;\n",
      " Minibatch[ 881- 900]: loss = 7.532461 * 25600, metric = 87.37% * 25600;\n",
      " Minibatch[ 901- 920]: loss = 7.550430 * 25600, metric = 89.10% * 25600;\n",
      " Minibatch[ 921- 940]: loss = 7.543906 * 25600, metric = 88.51% * 25600;\n",
      " Minibatch[ 941- 960]: loss = 7.541406 * 25600, metric = 88.31% * 25600;\n",
      " Minibatch[ 961- 980]: loss = 7.529961 * 25600, metric = 87.12% * 25600;\n",
      " Minibatch[ 981-1000]: loss = 7.543711 * 25600, metric = 88.53% * 25600;\n",
      "Finished Epoch[3 of 300]: [Training] loss = 7.541652 * 1280000, metric = 88.31% * 1280000 164.172s (7796.7 samples/s);\n",
      "\n",
      "----- 第Epoch: 2後自動寫詞\n",
      "----- diversity: 0.8\n",
      "----- 根據以下詞彙發想: \"依然廣闊\n",
      "還要什麼 這一段關係 走這段距離\n",
      "已足夠讓我燃燒 那更燦爛的煙火\n",
      "只有\"\n",
      "依然廣闊\n",
      "還要什麼 這一段關係 走這段距離\n",
      "已足夠讓我燃燒 那更燦爛的煙火\n",
      "只有    \n",
      "       \n",
      "      \n",
      "       \n",
      "       \n",
      "      \n",
      "       \n",
      "       \n",
      "       \n",
      "       \n",
      "       \n",
      "       \n",
      "      \n",
      "       \n",
      "       \n",
      "       \n",
      "      \n",
      "      \n",
      "       \n",
      "       \n",
      "      \n",
      "       \n",
      "       \n",
      "       \n",
      "       \n",
      "       \n",
      " ----- diversity: 1.0\n",
      "----- 根據以下詞彙發想: \"依然廣闊\n",
      "還要什麼 這一段關係 走這段距離\n",
      "已足夠讓我燃燒 那更燦爛的煙火\n",
      "只有\"\n",
      "依然廣闊\n",
      "還要什麼 這一段關係 走這段距離\n",
      "已足夠讓我燃燒 那更燦爛的煙火\n",
      "只有    \n",
      "      \n",
      "       \n",
      "      \n",
      "       \n",
      "       東\n",
      "       \n",
      "       \n",
      "      \n",
      "       \n",
      "      \n",
      "       \n",
      "      \n",
      "       \n",
      "      \n",
      "       \n",
      "       \n",
      "       \n",
      "       \n",
      "     粉 \n",
      "       \n",
      "       \n",
      "       \n",
      "      \n",
      "       \n",
      "       \n",
      "  Minibatch[   1-  20]: loss = 7.524231 * 25600, metric = 86.62% * 25600;\n",
      " Minibatch[  21-  40]: loss = 7.539004 * 25600, metric = 88.02% * 25600;\n",
      " Minibatch[  41-  60]: loss = 7.543535 * 25600, metric = 88.40% * 25600;\n",
      " Minibatch[  61-  80]: loss = 7.542441 * 25600, metric = 88.32% * 25600;\n",
      " Minibatch[  81- 100]: loss = 7.539172 * 25600, metric = 88.08% * 25600;\n",
      " Minibatch[ 101- 120]: loss = 7.543677 * 25600, metric = 88.50% * 25600;\n",
      " Minibatch[ 121- 140]: loss = 7.548438 * 25600, metric = 88.96% * 25600;\n",
      " Minibatch[ 141- 160]: loss = 7.555166 * 25600, metric = 89.73% * 25600;\n",
      " Minibatch[ 161- 180]: loss = 7.537866 * 25600, metric = 87.88% * 25600;\n",
      " Minibatch[ 181- 200]: loss = 7.545488 * 25600, metric = 88.72% * 25600;\n",
      "\n",
      "----- 第Epoch: 3後自動寫詞\n",
      "----- diversity: 0.8\n",
      "----- 根據以下詞彙發想: \"未遇過你聲音\n",
      "多動人〔多麼傷感的笑聲〕\n",
      "明日再會 我的身軀搜索你的身影\n",
      "如下一站\"\n",
      "未遇過你聲音\n",
      "多動人〔多麼傷感的笑聲〕\n",
      "明日再會 我的身軀搜索你的身影\n",
      "如下一站   \n",
      "     \n",
      "      \n",
      "     \n",
      "      \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "      \n",
      "     \n",
      "      \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "      \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "    ----- diversity: 1.0\n",
      "----- 根據以下詞彙發想: \"未遇過你聲音\n",
      "多動人〔多麼傷感的笑聲〕\n",
      "明日再會 我的身軀搜索你的身影\n",
      "如下一站\"\n",
      "未遇過你聲音\n",
      "多動人〔多麼傷感的笑聲〕\n",
      "明日再會 我的身軀搜索你的身影\n",
      "如下一站  \n",
      "      \n",
      "   皂  \n",
      "      \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "      \n",
      "     \n",
      "     \n",
      "      \n",
      "\n",
      "       \n",
      "減    \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "      \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "      \n",
      "     \n",
      "     進 \n",
      "\n",
      "        \n",
      "\n",
      "服      \n",
      "     Minibatch[ 201- 220]: loss = 7.547495 * 25600, metric = 88.89% * 25600;\n",
      " Minibatch[ 221- 240]: loss = 7.530615 * 25600, metric = 87.31% * 25600;\n",
      " Minibatch[ 241- 260]: loss = 7.538369 * 25600, metric = 87.98% * 25600;\n",
      " Minibatch[ 261- 280]: loss = 7.537666 * 25600, metric = 87.86% * 25600;\n",
      " Minibatch[ 281- 300]: loss = 7.531943 * 25600, metric = 87.36% * 25600;\n",
      " Minibatch[ 301- 320]: loss = 7.552881 * 25600, metric = 89.43% * 25600;\n",
      " Minibatch[ 321- 340]: loss = 7.534033 * 25600, metric = 87.55% * 25600;\n",
      " Minibatch[ 341- 360]: loss = 7.546563 * 25600, metric = 88.82% * 25600;\n",
      " Minibatch[ 361- 380]: loss = 7.526406 * 25600, metric = 86.72% * 25600;\n",
      " Minibatch[ 381- 400]: loss = 7.545410 * 25600, metric = 88.74% * 25600;\n",
      "\n",
      "----- 第Epoch: 3後自動寫詞\n",
      "----- diversity: 0.8\n",
      "----- 根據以下詞彙發想: \"人亮相的場地 我不能不在\n",
      "每個人都懂得搞怪 我也算不賴\n",
      "狠下了心腸一天談個新的戀\"\n",
      "人亮相的場地 我不能不在\n",
      "每個人都懂得搞怪 我也算不賴\n",
      "狠下了心腸一天談個新的戀\n",
      "\n",
      "          \n",
      "         \n",
      "        \n",
      "    "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
