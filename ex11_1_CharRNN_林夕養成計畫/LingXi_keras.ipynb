{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 詞神林夕養成計畫(keras)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "林夕是公認的華語流行歌的詞神，他寫了許多膾炙人口的好詞，那麼機器有沒有辦法模仿他寫詞的功夫呢?我們在此使用最簡單的Char-RNN，也就是利用LSTM預測下一個字，利用這樣的模型來模仿林夕的用字遣詞。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![md_images](../Images/charrnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "RNN是個耗費計算力的算法，tensorflow的壞習慣是自己會獨吞所有gpu，很容易一執行就會出現failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED 錯誤，因此要修改KERAS後台，只准他使用90% gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "\n",
    "def get_session(gpu_fraction=0.9):\n",
    "    \"\"\"\n",
    "    This function is to allocate GPU memory a specific fraction\n",
    "    Assume that you have 6GB of GPU memory and want to allocate ~2GB\n",
    "    \"\"\"\n",
    "    num_threads = os.environ.get('OMP_NUM_THREADS')\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n",
    "\n",
    "    if num_threads:\n",
    "        return tf.Session(config=tf.ConfigProto(\n",
    "            gpu_options=gpu_options, intra_op_parallelism_threads=num_threads))\n",
    "    else:\n",
    "        return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "KTF.set_session(get_session(0.9))  # using 40% of total GPU Memory"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我們在此提供了林夕(lingxi.txt)以及方文山(jay.txt)的歌詞語料，首先讀取語料。為何編碼不使用utf-8要改用utf-8-sig，這是因為在windows環境的utf-8格式與python不同，為了避免錯誤，因此使用utf-8-sig比較保險。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 52647\n",
      "corpus rows: 4117\n",
      "你說你 從\n",
      "['你說你 從來未愛戀過', '但很珍惜 跟我在消磨', '我笑我 原來是我的錯', '裂開的心 還未算清楚', '如此天真 竟得我一個']\n"
     ]
    }
   ],
   "source": [
    "with io.open('lingxi.txt', encoding='utf-8-sig') as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "print('corpus length:', len(text))\n",
    "texts=text.split('\\n')\n",
    "texts=[t for t in texts if len(t.replace(' ',''))>0]\n",
    "print('corpus rows:', len(texts))\n",
    "print(text[:5])\n",
    "print(texts[:5])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "為了要把文字轉換為onehot編碼，首先我們需要統計他們所使用過的去重複字數。使用.strip()去除沒有意義的符號，接著生成字轉索引(char_indices)以及反向的索引轉字(indices_char)的字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 2114\n",
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '1']\n"
     ]
    }
   ],
   "source": [
    "#把每個字去重複\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "print(chars[:10])\n",
    "\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Keras的序列定義仍然是向量，向量形狀為[句子數,序列最大長度,總字數]，長度超過最大序列長度時截斷。由於是要預測下一個字，因此label等於feature位移一個字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "(26302, 40, 2114)\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "\n",
    "print('Vectorization...')\n",
    "x_features=[]\n",
    "y_labels=[]\n",
    "curr_chars=text[:-1]\n",
    "next_chars=text[1:]\n",
    "for i in range((len(curr_chars)-maxlen-1)//2):\n",
    "    x_features.append(curr_chars[2*i:2*i+maxlen])\n",
    "    y_labels.append(next_chars[2*i:2*i+maxlen])\n",
    "\n",
    "x= np.zeros((len(x_features),maxlen,len(chars)), dtype=np.float32)\n",
    "y = np.zeros((len(x_features),maxlen,len(chars)), dtype=np.float32)\n",
    "    \n",
    "for i in range(len(x_features)):\n",
    "    for m in range(maxlen):\n",
    "        x[i,m,char_indices[x_features[i][m]]]=1\n",
    "        y[i,m,char_indices[y_labels[i][m]]]=1\n",
    "  \n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "定義模型結構，模型結構如下：\n",
    "     +---------------------------+              +------------+\n",
    "x -->| LSTM(2 layer, hidden=512) |--> dropout-->| DenseLayer |--> y\n",
    "     +---------------------------+              +------------+\n",
    "     \n",
    "這裡的輸入x是一個(批次,序列長，字數)的張量(64, 40, 2114])，輸入至一個2層的單向lstm(隱藏層形狀為512)，最後透過dropout後送入全連接層，輸出為長度為字數的onehot向量([64, 40, 2114])。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "load model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 40, 512)           5380096   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 40, 512)           2099200   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 40, 512)           0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 40, 2114)          1084482   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 40, 2114)          0         \n",
      "=================================================================\n",
      "Total params: 8,563,778\n",
      "Trainable params: 8,563,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, input_shape=(maxlen, len(chars)), return_sequences=True))\n",
    "model.add(LSTM(512, input_shape=(maxlen, len(chars)), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "optimizer =Adam(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "if not os.path.exists('Models'):\n",
    "    os.mkdir('Models')\n",
    "    print(\"Directory Models Created \")\n",
    "    \n",
    "if os.path.exists('Models/lingxi_keras.h5'):\n",
    "    model.load_weights('Models/lingxi_keras.h5')\n",
    "    print('load model')\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "定義自動寫詞函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds+10e-14) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / (np.sum(exp_preds)+10e-14)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- 第Epoch: %d後自動寫詞' % epoch)\n",
    "    model.save_weights('Models/lingxi_keras.h5',True)\n",
    "    start_index = random.randint(0, len(texts))\n",
    "    for diversity in [1.0]:\n",
    "        print('----- diversity:', diversity)\n",
    "        \n",
    "        generated = ''\n",
    "        sentence=''\n",
    "        input_sentence=''\n",
    "        sentence = texts[start_index]\n",
    "        if len(sentence)>5:\n",
    "            sentence=sentence[:5]\n",
    "        generated += sentence\n",
    "        print('----- 根據以下詞彙發想:「{0}」'.format(sentence ))\n",
    "        sys.stdout.write(generated)\n",
    "        is_finished=False\n",
    "        row=0\n",
    "        while not is_finished:\n",
    "            x_pred = np.zeros((1, maxlen,len(chars)))\n",
    "            if len(sentence)>maxlen-1:\n",
    "                input_sentence=sentence[-1*(maxlen-1):]\n",
    "            else:\n",
    "                input_sentence=sentence\n",
    "            for t, char in enumerate(input_sentence):\n",
    "                x_pred[0, t,char_indices[char]] =1\n",
    "            \n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "          \n",
    "            next_index =sample(preds[len(input_sentence)-1],1.0)\n",
    "            next_char = indices_char[next_index]\n",
    "            if next_char==' ' and len(generated)>5 and generated[-2:]=='  ':\n",
    "                is_finished =True\n",
    "                break\n",
    "            generated += next_char\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "            sentence = sentence+ next_char\n",
    " \n",
    "            row+=1\n",
    "            if row>100:\n",
    "                is_finished =True\n",
    "        print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "每次epoch結束後進行一次寫作測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "26302/26302 [==============================] - 158s 6ms/step - loss: 1.2772 - acc: 0.6790\n",
      "\n",
      "----- 第Epoch: 0後自動寫詞\n",
      "----- diversity: 1.0\n",
      "----- 根據以下詞彙發想:「跟你相對時」\n",
      "跟你相對時分開\n",
      "如果你愛我的清角 並有總是好 沒有心火\n",
      "無謂一生 難得〕\n",
      "不管你 我要眼睛\n",
      "漆黑中的找到我\n",
      "沒法想這個話 看不到\n",
      "誰比難道會\n",
      "但每一句的愛 你不會的眼前\n",
      "對你結尾的傳笑\n",
      "回頭動心的貼過\n",
      "留下風光世\n",
      "Epoch 2/5\n",
      "26302/26302 [==============================] - 157s 6ms/step - loss: 1.1898 - acc: 0.6968\n",
      "\n",
      "----- 第Epoch: 1後自動寫詞\n",
      "----- diversity: 1.0\n",
      "----- 根據以下詞彙發想:「一夜之間化」\n",
      "一夜之間化做話 不要緊 你你去做證 你與你抱 我的你\n",
      "怕不知 當你那份浪漫和寧靜\n",
      "才可以端莊 也可以放浪\n",
      "造型隨你幻想\n",
      "你不愛你要 我要心得不好\n",
      "我要給我最美滿太高興\n",
      "現在就開始哭進在大國南\n",
      "皇全你手 快樂地 隨\n",
      "Epoch 3/5\n",
      "26302/26302 [==============================] - 177s 7ms/step - loss: 1.1069 - acc: 0.7155\n",
      "\n",
      "----- 第Epoch: 2後自動寫詞\n",
      "----- diversity: 1.0\n",
      "----- 根據以下詞彙發想:「每日要飲幾」\n",
      "每日要飲幾罐汽水\n",
      "你未與到掌聲之後我能選擇關呼一一方不覺\n",
      "亦有任何事回怪兩點 從來給我自己\n",
      "心在最後 為了會更深心\n",
      "溫柔一然來相擁\n",
      "從不可 貼過的人\n",
      "牽著手 是吻別\n",
      "但你前在為何沒有醒\n",
      "我的身情 轉天怎不必懂\n",
      "為\n",
      "Epoch 4/5\n",
      "26302/26302 [==============================] - 229s 9ms/step - loss: 1.0483 - acc: 0.7280\n",
      "\n",
      "----- 第Epoch: 3後自動寫詞\n",
      "----- diversity: 1.0\n",
      "----- 根據以下詞彙發想:「然後一起分」\n",
      "然後一起分離 看過了個世顧\n",
      "不要的感動 不要你\n",
      "有了一切 不要再親我\n",
      "我只怕愛情 不要怕只要心問\n",
      "為何愛你的心火\n",
      "有多少人 提前同走\n",
      "在同波一掃\n",
      "曾愛一剎的愛過\n",
      "一個一個 竟將不知 更去看不起\n",
      "如此一般竟會 我不\n",
      "Epoch 5/5\n",
      "26302/26302 [==============================] - 657s 25ms/step - loss: 0.9961 - acc: 0.7391\n",
      "\n",
      "----- 第Epoch: 4後自動寫詞\n",
      "----- diversity: 1.0\n",
      "----- 根據以下詞彙發想:「天一方 海」\n",
      "天一方 海亦無別 在世間在背後\n",
      "溫馨的感覺 看 講 來真懂\n",
      "當多自戀愛你\n",
      "如此 夜人的加期待 為你還留記著了\n",
      "不必把東邊從偏知\n",
      "只想在乎與你 在你身邊\n",
      "假使你 請不可抱緊什麼\n",
      "每人都知\n",
      "否定再過什麼\n",
      "有誰和誰和哪\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b129034eb8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=64,\n",
    "          epochs=5,\n",
    "          callbacks=[print_callback])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
