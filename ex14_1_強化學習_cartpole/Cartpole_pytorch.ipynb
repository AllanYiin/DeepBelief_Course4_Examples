{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 強化學習Cartpole (pytorch)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "相較於之前學習過的卷積神經網路與遞迴神經網路，強化學習是非常不一樣的，他通常不會有事先整理好成對的輸入特徵與標籤，而是不斷地與外界環境交互，透過交互所累積的獎賞或懲罰來修正行動策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"Images/polecart.gif\" width=\"300\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Figure 1\n",
    "Image(url=\"Images/polecart.gif\", width=300, height=300)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我們將使用OpenAI's gym CartPole環境模擬器，來教導台車如何平衡上方的木棒。在 CartPole環境中，給定木棒的角度和台車位置及其兩者的變化量，決定讓台車向左或向右來平衡木棒。決策基本上就只有+1(向右)或-1(向左)移動台車，每多保持平衡一刻，就會獎賞+1，當棒子距離垂直角度大於15度或者台車移動距離中心超過2.4單位，則該epoch結束。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Goal**\n",
    "\n",
    "訓練的目的就是希望台車能夠學會正確的移動，好讓木棒持續停留在平衡位置。如果用RL的術語來說，目標就是要找尋 _策略_ $a$, 藉由與環境的互動(讓木棒保持平衡)好讓 _獎賞_ $r$ 最大化。於是給定一系列的實驗 $$s \\xrightarrow{a} r, s'$$ 讓後讓智能體學會在給定的狀態下 $s$ 找出最佳的行動策略 $a$ 以將跨時間累積獎賞最大化 $r$ :\n",
    "\n",
    "$$\n",
    "Q(s,a) = r_0 + \\gamma r_1 + \\gamma^2 r_2 + \\ldots = r_0 + \\gamma \\max_a Q^*(s',a)\n",
    "$$\n",
    "\n",
    "此處的 $\\gamma \\in [0,1)$ 是用來控制評估獎賞的折價因子，我們也稱之為 [*Bellmann*-equation](https://en.wikipedia.org/wiki/Bellman_equation).\n",
    "\n",
    "在接下來的範例中，我們將示範如何針對狀態空間建模，以及如何根據收到的獎賞，來轉化為取得未來最大獎賞的行動。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此我們將採用兩種常見的技術:\n",
    "\n",
    "**Deep Q-Networks (DQN)**: DQNs在2015年只透過遊戲畫面像素數據就能訓練智能體玩Atari電動遊戲而聲名大噪。我們訓練神經網路學習 $Q(s,a)$ 值 (也就是 $Q$-Network )。根據這 $Q$ 函數直，我們可以選擇最佳策略。\n",
    "\n",
    "**Policy gradient (策略梯度)**: 這個方法是在神經網路中直接估計策略(行動組合)，結果是學習一組有序的動作，通過機率選擇動作的子集來最大化獎勵。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請注意，由於這個實作的目的是為了要理解RL的概念，因此我們網路部分都使用簡單的淺層網路，當然這個部分是可以日後再擴充與使用結構更複雜的網路。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "執行實作前需要安裝 OpenAI gym包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import gym\n",
    "except:\n",
    "    !pip install gym\n",
    "    import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## CartPole: 數據與環境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們將使用OpenAI's [gym](https://github.com/openai/gym)模擬器中的[CartPole](https://gym.openai.com/envs/CartPole-v0)環境以訓練台車學會維持木棒的平衡。\n",
    "\n",
    "在每個時間點, 智能體(agent)\n",
    " * 取得環境觀察值 $(x, \\dot{x}, \\theta, \\dot{\\theta})$, 分別表示 *台車位置*, *台車速度*, *木棒對垂直交角*, *木棒角速度(落下速度)*,\n",
    " * 採取行動 `向左` 或 `向右`, 以及\n",
    " * 獲得\n",
    "  * 每多存活一刻就會獲得獎賞 +1 \n",
    "  * 新狀態 $(x', \\dot{x}', \\theta', \\dot{\\theta}')$\n",
    "\n",
    "如果觸發以下條件，則episode終止\n",
    " * 木棒距垂直角度大於15度\n",
    " * 臺車移動距離中心超過2.4個單位\n",
    "\n",
    "任務完成認定條件\n",
    " * 智能體在過去50個episodes期間累積獎賞值200以上\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法1: DQN\n",
    "\n",
    "在每次轉換後 $(s,a,r,s')$, 我們試圖將我們的價值函數 $Q(s,a)$ 移近我們的目標 $r+\\gamma \\max_{a'}Q(s',a')$, 其中 $\\gamma$ 是未來獎勵的折扣因子，值介於0和1之間。\n",
    "\n",
    "DQNs\n",
    " * 學習能將觀察 (狀態, 行動)對應至 `score`的 _Q-函數_ \n",
    " * 使用記憶體重放（以前記錄的 $Q$ 值對應不同的 $(s,a)$ 來解相關經驗（序列狀態轉換）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: DQN\n",
    "\n",
    "$$\n",
    "l_1 = relu( x W_1 + b_1) \\\\\n",
    "Q(s,a) = l_1 W_2 + b_2 \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "來源參考自Keras版本實現, https://github.com/jaara/AI-blog/blob/master/CartPole-basic.py, 作者是Jaromír Janisch 首見於他的 [AI blog](https://jaromiru.com/2016/09/27/lets-make-a-dqn-theory/)\n",
    "\n",
    "我們使用簡單的兩層全連接層網路來做練習。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import argparse\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Bernoulli\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STATE_COUNT = 4 (corresponding to $(x, \\dot{x}, \\theta, \\dot{\\theta})$),\n",
    "\n",
    "ACTION_COUNT = 2 (corresponding to `LEFT` or `RIGHT`)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "利用gym建置CartPole-v0模擬器環境，狀態空間長度為4，行動策略長度為2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "STATE_COUNT  = env.observation_space.shape[0]\n",
    "ACTION_COUNT = env.action_space.n\n",
    "\n",
    "STATE_COUNT, ACTION_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `記憶(Memory)` 類別是用來儲存不同的狀態、行動以及獎賞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.head = nn.Linear(448, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        #print(x.shape)\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`智能體` 使用 `Brain` 以及 `Memory` 來重放(replay)過去的行動以選擇能讓獎賞最大化的行動集合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADXCAYAAAAKnKqnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSugiIjmhhC4ikhNK6CIiOaGELiKSE/8fOx51I/dsbNoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "# This is based on the code from gym.\n",
    "screen_width = 600\n",
    "\n",
    "\n",
    "def get_cart_location():\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "\n",
    "def get_screen():\n",
    "    screen = env.render(mode='rgb_array').transpose(\n",
    "        (2, 0, 1))  # transpose into torch order (CHW)\n",
    "    # Strip off the top and bottom of the screen\n",
    "    screen = screen[:, 160:320]\n",
    "    view_width = 320\n",
    "    cart_location = get_cart_location()\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescare, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練\n",
    "\n",
    "如何一般的學習過程，我們希望一開始的行動可以較具探索性，然後隨著學習過程逐漸掌握哪些行動比較夠獲得較高報酬。 The tutorial below implements the [epsilon-greedy](https://en.wikipedia.org/wiki/Reinforcement_learning) approach (a.k.a. $\\epsilon$-greedy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "num_episodes = 3000\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    # Update the target network\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法 2: 策略梯度 (PG, Policy gradient)\n",
    "**目標:**\n",
    "$$\n",
    "\\text{maximize } E [R | \\pi_\\theta]\n",
    "$$\n",
    "\n",
    "**思路:**\n",
    "1. 收集經驗 (透過 $(s,a)$ 空間，取樣一系列的軌跡)\n",
    "2. 更新策略，讓好 _good_ 經驗變得更有可能\n",
    "\n",
    "**與DQN的差異:**\n",
    " * 不用考慮單一 $(s,a,r,s')$ 轉換,而在梯度更新時考慮整個episodes\n",
    " * 模型參數直接對策略建模(輸出是行動的機率),而DQN是針對價值函數建模(輸出是原始分數)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 獎賞\n",
    "\n",
    "停留在遊戲中的每多一刻，獎賞+1\n",
    "\n",
    "問題: 我們通常一開始不知道, 甚麼樣的行動可以讓遊戲玩久一點。簡單的思路：在批次開始時的動作很好，而那些最終的動作可能很糟糕（畢竟他們導致了遊戲失敗）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma=0.999):\n",
    "    \"\"\"使用1D rewards向量以及計算折價後獎賞 \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "discounted_epr = discount_rewards(np.ones(10))\n",
    "f, ax = plt.subplots(1, figsize=(5,2))\n",
    "sns.barplot(list(range(10)), discounted_epr, color=\"steelblue\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我們將獎勵正歸化，使得後半的獎勵都處於小於零的狀態。gamma 控制獎勵遞減的延遲時間。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "discounted_epr_cent = discounted_epr - np.mean(discounted_epr)\n",
    "discounted_epr_norm = discounted_epr_cent/np.std(discounted_epr_cent)\n",
    "f, ax = plt.subplots(1, figsize=(5,2))\n",
    "sns.barplot(list(range(10)), discounted_epr_norm, color=\"steelblue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "discounted_epr = discount_rewards(np.ones(10), gamma=0.5)\n",
    "discounted_epr_cent = discounted_epr - np.mean(discounted_epr)\n",
    "discounted_epr_norm = discounted_epr_cent/np.std(discounted_epr_cent)\n",
    "f, ax = plt.subplots(2, figsize=(5,3))\n",
    "sns.barplot(list(range(10)), discounted_epr, color=\"steelblue\", ax=ax[0])\n",
    "sns.barplot(list(range(10)), discounted_epr_norm, color=\"steelblue\", ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: Policy Gradient\n",
    "\n",
    "$$\n",
    "l_1 = relu( x W_1 + b_1) \\\\\n",
    "l_2 = l_1 W_2 + b_2 \\\\\n",
    "\\pi(a|s) = sigmoid(l_2)\n",
    "$$\n",
    "\n",
    "請注意: 在使用策略梯度法時,全連接層的輸出要加上sigmoid，以轉換為0~1之間。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(STATE_COUNT, 24)\n",
    "        self.fc2 = nn.Linear(24, 36)\n",
    "        self.fc3 = nn.Linear(36, 1)  # Prob of Left\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 執行 PG 模型\n",
    "\n",
    "**策略搜索(Policy Search)**: 最優策略搜索可以使用無梯度方法或通過計算由$ \\ theta $參數化的策略空間（$ \\ pi_ \\ theta $）上的梯度下降來執行。在本實作中, 我們在參數化空間 $\\theta$使用經典正向 (`loss.forward`) 和反向 (`loss.backward`) 傳播方式處理偏差。在這案例中, $\\theta = \\{W_1, b_1, W_2, b_2\\}$, 也就是我們的模型參數。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.FloatTensor(episode_durations)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "        plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "# Parameters\n",
    "num_episode =10000\n",
    "batch_size =  50\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "policy_net = PolicyNet()\n",
    "optimizer = torch.optim.RMSprop(policy_net.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch History\n",
    "state_pool = []\n",
    "action_pool = []\n",
    "reward_pool = []\n",
    "steps = 0\n",
    "\n",
    "\n",
    "for e in range(num_episode):\n",
    "    state  = env.reset()\n",
    "    state = torch.from_numpy(state).float()\n",
    "    state = Variable(state)\n",
    "    for t in count():\n",
    "        probs = policy_net(state)\n",
    "        m = Bernoulli(probs)\n",
    "        action = m.sample()\n",
    "\n",
    "        action = action.data.numpy().astype(int)[0]\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "\n",
    "        # To mark boundarys between episodes\n",
    "        if done:\n",
    "            reward = 0\n",
    "        else:\n",
    "            env.render()\n",
    "\n",
    "        state_pool.append(state)\n",
    "        action_pool.append(float(action))\n",
    "        reward_pool.append(reward)\n",
    "\n",
    "        state = next_state\n",
    "        state = torch.from_numpy(state).float()\n",
    "        state = Variable(state)\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "        else:\n",
    "            env.render()\n",
    "            \n",
    "\n",
    "    # Update policy\n",
    "    if e > 0 and e % batch_size == 0:\n",
    "        # Discount reward\n",
    "        running_add = 0\n",
    "        for i in reversed(range(steps)):\n",
    "            if reward_pool[i] == 0:\n",
    "                running_add = 0\n",
    "            else:\n",
    "                running_add = running_add * gamma + reward_pool[i]\n",
    "                reward_pool[i] = running_add\n",
    "\n",
    "        # Normalize reward\n",
    "        reward_mean = np.mean(reward_pool)\n",
    "        reward_std = np.std(reward_pool)\n",
    "        for i in range(steps):\n",
    "            reward_pool[i] = (reward_pool[i] - reward_mean) / reward_std\n",
    "\n",
    "        # Gradient Desent\n",
    "        optimizer.zero_grad()\n",
    "        print('Episode: %d. Average reward for episode %f.' % (e ,reward_mean))\n",
    "        for i in range(steps):\n",
    "            state = state_pool[i]\n",
    "            action = Variable(torch.FloatTensor([action_pool[i]]))\n",
    "            reward = reward_pool[i]\n",
    "\n",
    "            probs = policy_net(state)\n",
    "            m = Bernoulli(probs)\n",
    "            loss = -m.log_prob(action) * reward  # Negtive score function x reward\n",
    "            loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        state_pool = []\n",
    "        action_pool = []\n",
    "        reward_pool = []\n",
    "        steps = 0"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
