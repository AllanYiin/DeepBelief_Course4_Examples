{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 強化學習Cartpole (cntk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相較於之前學習過的機器視覺與自然語言，強化學習是另一種不同的人工智能方向。機器視覺與語言注重的是如何透過神經網路結構來實現人類視覺以及語言理解的內部機制。但是強化學習卻不關心這些，強化學習的本質是一種行為控制的科學，它關心的是如何能夠控制智能體的行為模式，但卻不需要關注於智能體實現這個行為模式的內部結構。舉例來說，傳統的技術如果要讓智能體懂得「跑」這件事，他需要對於人類的運動機制、肌肉控制等過程需要有巨細靡遺理解，但是也正是這些阻礙地讓機器跑起來的可能性，因為這裡涉及到太多的內部機制。而強化學習的目標則是透過外部的獎賞與懲罰，透過智能體與環境間的交互，來誘導智能體自然產生「跑」這個行為。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![md_images](../Images/rl.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole: 數據與環境\n",
    "\n",
    "在本次實作中，我們將會來介紹強化學習的經典案例Cartpole。我們將使用OpenAI's [gym](https://github.com/openai/gym)模擬器中的[CartPole](https://gym.openai.com/envs/CartPole-v0)環境來做為我們智能體所存在以及交互的世界，它會不斷的反饋給我們木棒與台車的狀態。\n",
    "\n",
    "環境給予我們的狀態(state)是一個長度為4的向量 $(x, \\dot{x}, \\theta, \\dot{\\theta})$, 分別表示 *台車位置*, *台車速度*, *木棒對垂直交角*, *木棒角速度(落下速度)*,\n",
    "\n",
    "### CartPole: 行動策略\n",
    "至於台車為了不讓棒子掉落，必須採取對應行動，它能夠執行的行動只有兩種: `向左` 或 `向右`\n",
    "\n",
    "### CartPole: 環境交互\n",
    "當台車執行了某個行動後，它可從環境中獲得\n",
    "  * 每多存活一刻就會獲得獎賞 +1 \n",
    "  * 新狀態 $(x', \\dot{x}', \\theta', \\dot{\\theta}')$\n",
    "\n",
    "### CartPole: 失敗的定義\n",
    "這也正是「活得越久，領得越多」，那麼機器的目標很明確，就是希望採取正確的行動，來讓木棒可以撐久一點不要掉下去。那至於甚麼樣的情況會導致木棒掉落呢？如果觸發以下條件，則episode終止\n",
    " * 木棒距垂直角度大於15度\n",
    " * 臺車移動距離中心超過2.4個單位\n",
    "\n",
    "### CartPole: 成功的定義\n",
    "我們對於台車的任務給予一個任務完成認定條件如下：\n",
    " * 智能體在過去50個episodes期間累積獎賞值200以上\n",
    "\n",
    "### CartPole: 優化的目標\n",
    "如果用RL的術語來說，整個模型的目標就是要找尋 _行動策略(向左向右)_ $a$, 藉由與環境的互動(讓木棒保持平衡)好讓 _獎賞_ $r$ 最大化。於是給定一系列的實驗 $$s \\xrightarrow{a} r, s'$$ 讓後讓智能體學會在給定的狀態下 $s$ 找出最佳的行動策略 $a$ 以將跨時間累積獎賞最大化 $r$ :\n",
    "\n",
    "$$\n",
    "Q(s,a) = r_0 + \\gamma r_1 + \\gamma^2 r_2 + \\ldots = r_0 + \\gamma \\max_a Q^*(s',a)\n",
    "$$\n",
    "\n",
    "此處的 $\\gamma \\in [0,1)$ 是波爾曼方程式 [*Bellmann*-equation] 用來控制評估未來獎賞的折價因子，我們也稱之為(https://en.wikipedia.org/wiki/Bellman_equation).\n",
    "\n",
    "在接下來的範例中，我們將示範如何針對狀態空間建模，以及如何根據收到的獎賞，來轉化為取得未來最大獎賞的行動。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![md_images](../Images/polecart.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此我們將採用兩種常見的技術:\n",
    "\n",
    "**Deep Q-Networks (DQN)**: DQNs在2015年只透過遊戲畫面像素數據就能訓練智能體玩Atari電動遊戲而聲名大噪。我們訓練神經網路學習 $Q(s,a)$ 值 (也就是 $Q$-Network )，你可以把Q函數想像為一個可以根據目前狀態去查詢各個行動策略對應的可能獎賞值(目前獎賞與預估未來獎賞)的對應表，因為這個表可能很複雜或是根本不存在，所以我們需要用一個網路去近似它。根據這 $Q$ 函數值，我們交會逐一評估所有可能的策略，選擇獎賞最高的為最佳策略。(我常開玩笑說DQN是最視錢如命的模型，它的輸入是視覺，輸出是怎麼做會最賺錢)，它的輸出既然是獎賞的期望值，所以它的本質是一種迴歸。\n",
    "\n",
    "**Policy gradient (策略梯度)**: 這個方法是在神經網路中直接估計策略(行動組合)的機率分布，通過機率選擇動作的子集來最大化獎勵，所以你可以把它視為是一種分類模型，輸出的是各個行動的分布機率。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請注意，由於這個實作的目的是為了要理解RL的概念，因此我們網路部分都使用簡單的淺層網路，當然這個部分是可以日後再擴充與使用結構更複雜的網路。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy\n",
    "import math \n",
    "import os \n",
    "import random\n",
    "import cntk as C\n",
    "from cntk.device import try_set_default_device, cpu, gpu\n",
    "\n",
    "style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# 是否使用GPU\n",
    "is_gpu = True\n",
    "\n",
    "if is_gpu:\n",
    "    try_set_default_device(gpu(0))\n",
    "else:\n",
    "    try_set_default_device(cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "執行實作前需要安裝 OpenAI gym包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import gym\n",
    "except:\n",
    "    !pip install gym\n",
    "    import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 強化學習方法1: DQN\n",
    "\n",
    "DQN的主要核心在於收集歷史案例的四元組$(s,a,r,s')$ ，這四個分別是之前的狀態，執行的行動、得到的獎賞以及執行後的新狀態，透過這四元祖，我們可以利用深度學習網路去近似價值函數$Q(s,a)$，這個價值函數$Q(s,a)$的實際值應該要接近 $r+\\gamma \\max_{a'}Q(s',a')$, 其中 $\\gamma$ 是未來獎勵的折扣因子，值介於0和1之間。這種先收集樣本，然後在採樣後建模訓練的手法稱之為「回放(replay)」，可以有效地換解如果我們按照數據的時序訓練時，容易偏重當下的案例，反而造成樣本的不均衡。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: DQN\n",
    "\n",
    "$$\n",
    "l_1 = relu( x W_1 + b_1) \\\\\n",
    "Q(s,a) = l_1 W_2 + b_2 \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "來源參考自Keras版本實現, https://github.com/jaara/AI-blog/blob/master/CartPole-basic.py, 作者是Jaromír Janisch 首見於他的 [AI blog](https://jaromiru.com/2016/09/27/lets-make-a-dqn-theory/)。\n",
    "\n",
    "首先利用gym建置CartPole-v0模擬器環境，狀態空間長度為4，行動策略長度為2\n",
    "\n",
    "STATE_COUNT = 4 (對應至 $(x, \\dot{x}, \\theta, \\dot{\\theta})$),\n",
    "ACTION_COUNT = 2 (對應至`向左` or `向右`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "STATE_COUNT  = env.observation_space.shape[0]\n",
    "ACTION_COUNT = env.action_space.n\n",
    "\n",
    "STATE_COUNT, ACTION_COUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此，我們使用簡單的兩層全連接層網路來做示範，用來逼近價值函數$Q(s,a)$。如果你覺得兩層太簡單了，你也可以自行設計或者是置換為更複雜的網路結構。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![md_images](../Images/dqn_network.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 目標獎賞\n",
    "REWARD_TARGET =  200\n",
    "# 平均episodes數\n",
    "BATCH_SIZE_BASELINE = 50\n",
    "\n",
    "\n",
    "\n",
    "#大腦類別\n",
    "class Brain:\n",
    "    def __init__(self):\n",
    "        self.params = {}\n",
    "        self.model, self.trainer, self.loss = self._create()\n",
    "    #大腦中主要是透過兩層全連階層來近似Q函數\n",
    "    def _create(self):\n",
    "        observation = C.sequence.input_variable(STATE_COUNT, np.float32, name=\"s\")  #狀態\n",
    "        q_target = C.sequence.input_variable(ACTION_COUNT, np.float32, name=\"q\") #行動價值\n",
    "        \n",
    "        #使用兩層全連階層建構DQN\n",
    "        l1 = C.layers.Dense(24, activation=C.relu)\n",
    "        l2 = C.layers.Dense(64, activation=C.relu)\n",
    "        l3 = C.layers.Dense(ACTION_COUNT)\n",
    "        unbound_model = C.layers.Sequential([l1,l2,l3])\n",
    "        model = unbound_model(observation)\n",
    "\n",
    "        # loss='mse'\n",
    "        #模型預測的各個行動的r值應該要接近真實\n",
    "        loss = C.reduce_mean(C.square(model - q_target), axis=0)\n",
    "       \n",
    "        # 最佳化\n",
    "        lr = 0.001\n",
    "        lr_schedule = C.learning_parameter_schedule(lr)\n",
    "        learner = C.sgd(model.parameters, lr_schedule, gradient_clipping_threshold_per_sample=10)\n",
    "        trainer = C.Trainer(model, (loss, loss), learner)\n",
    "        return model, trainer, loss\n",
    "\n",
    "    def train(self, x, y, epoch=1, verbose=0):\n",
    "        arguments = dict(zip(self.loss.arguments, [x,y]))\n",
    "        updated, results =self.trainer.train_minibatch(arguments, outputs=[self.loss.output])\n",
    "\n",
    "    def predict(self, s):\n",
    "        return self.model.eval([s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `記憶(Memory)` 類別是專門用來儲存歷史的四元組紀錄。(之前的狀態，執行的行動、得到的獎賞以及執行後的新狀態)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Memory:   #儲存 ( s, a, r, s_ )\n",
    "    samples = []\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def add(self, sample):\n",
    "        self.samples.append(sample)\n",
    "\n",
    "        if len(self.samples) > self.capacity:\n",
    "            self.samples.pop(0)\n",
    "    \n",
    "    #從記憶中隨機取最近n筆\n",
    "    def sample(self, n):\n",
    "        n = min(n, len(self.samples))\n",
    "        return random.sample(self.samples, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`智能體` 使用 `Brain` 以及 `Memory` 來重放(replay)過去的行動以訓練出能讓獎賞最大化的行動集合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MEMORY_CAPACITY = 100000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "GAMMA =  0.95 # 折價因子\n",
    "\n",
    "MAX_EPSILON = 1  #初期還沒有案例可以供建模，因此100%根據隨機案例\n",
    "MIN_EPSILON = 0.01 # 即使模型準確率越來越高，還是必須保留部分比例基於隨機案例\n",
    "LAMBDA = 0.0001    # 衰減速度\n",
    "\n",
    "#智能體類別\n",
    "class Agent:\n",
    "    steps = 0\n",
    "    epsilon = MAX_EPSILON\n",
    "\n",
    "    def __init__(self):\n",
    "        #裡面主要有大腦以及記憶\n",
    "        #大腦裡具有輸入為狀態，輸出為各個行動的獎賞數值的神經網路\n",
    "        #記憶負責儲存四元組以及抽取樣本\n",
    "        self.brain = Brain()\n",
    "        self.memory = Memory(MEMORY_CAPACITY)\n",
    "\n",
    "    def act(self, s):\n",
    "        if random.random() < self.epsilon:\n",
    "            #部分基於隨機行動\n",
    "            return random.randint(0, ACTION_COUNT-1)\n",
    "        else:\n",
    "            #部分基於模型預測(這部分比率會逐漸增高)\n",
    "            return numpy.argmax(self.brain.predict(s))\n",
    "\n",
    "    def observe(self, sample):  # in (s, a, r, s_) format\n",
    "        #當觀察到新案例先把他加入記憶中\n",
    "        self.memory.add(sample)\n",
    "        self.steps += 1\n",
    "        # 一開始需要更多的探索，所以動作偏隨機，慢慢的我們開始能夠掌握訣竅，因此減少隨機。\n",
    "        self.epsilon = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * math.exp(-LAMBDA * self.steps)\n",
    "\n",
    "    #回放\n",
    "    def replay(self):\n",
    "        #取最近n筆作為minibatch\n",
    "        batch = self.memory.sample(BATCH_SIZE)\n",
    "        batchLen = len(batch)\n",
    "        no_state = numpy.zeros(STATE_COUNT)\n",
    "\n",
    "        #從四元組中抽取「初始狀態」\n",
    "        states = numpy.array([ o[0] for o in batch ], dtype=np.float32)\n",
    "        #從四元組中抽取「行動後新狀態」\n",
    "        states_ = np.array([(no_state if o[3] is None else o[3]) for o in batch ], dtype=np.float32)\n",
    "\n",
    "        #根據目前狀態預測的目前獎賞\n",
    "        p = agent.brain.predict(states)\n",
    "        #根據行動後新狀態預測的未來獎賞\n",
    "        p_ = agent.brain.predict(states_)\n",
    "\n",
    "        x = np.zeros((batchLen, STATE_COUNT)).astype(np.float32)\n",
    "        y = np.zeros((batchLen, ACTION_COUNT)).astype(np.float32)\n",
    "\n",
    "        #組出批次的x,y\n",
    "        for i in range(batchLen):\n",
    "            s, a, r, s_ = batch[i]\n",
    "\n",
    "            t = p[0][i]\n",
    "            \n",
    "            if s_ is None:\n",
    "                t[a] = r\n",
    "            else:\n",
    "                t[a] = r + GAMMA * np.amax(p_[0][i])\n",
    "\n",
    "            x[i] = s\n",
    "            y[i] = t\n",
    "\n",
    "        self.brain.train(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下來就可以開始訓練智能體的 **DQN**。請注意，若要達到 50 batches平均獎賞值達200以上，需要耗時約10分鐘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TOTAL_EPISODES = 3000\n",
    "\n",
    "#定義執行智能體函數\n",
    "def run(agent):\n",
    "    #將環境重置\n",
    "    s = env.reset()\n",
    "    #獎賞歸零\n",
    "    R = 0\n",
    "\n",
    "    while True:\n",
    "        #env.render()指令會幫我們渲染出目前台車狀態圖形\n",
    "        env.render()\n",
    "        #agent.act(s.astype(np.float32))表示是根據目前狀態產生對應行動\n",
    "        #在這邊act函數初期是基於隨機行動，後面隨機行動站筆會逐漸下降\n",
    "        a = agent.act(s.astype(np.float32))\n",
    "        #根據此行動，產出對應的獎賞以及新的狀態\n",
    "        s_, r, done, info = env.step(a)\n",
    "\n",
    "        if done: # 若最後是終止狀態(倒下)，則新狀態為None\n",
    "            s_ = None\n",
    "        \n",
    "        #構成四元組儲存在記憶中\n",
    "        agent.observe((s, a, r, s_))\n",
    "        #根據隨機抽樣進行回放訓練\n",
    "        agent.replay()\n",
    "        \n",
    "        #將狀態更新至新狀態\n",
    "        s = s_\n",
    "        #將獎賞累加\n",
    "        R += r\n",
    "\n",
    "        # 若最後是終止狀態(倒下)，則獎賞不再更新\n",
    "        if done:\n",
    "            return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下為訓練語法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50, episode的平均獎賞為21.24 \n",
      "Episode: 100, episode的平均獎賞為22.06 \n",
      "Episode: 150, episode的平均獎賞為20.98 \n",
      "Episode: 200, episode的平均獎賞為40.78 \n",
      "Episode: 250, episode的平均獎賞為66.06 \n",
      "Episode: 300, episode的平均獎賞為149.68 \n",
      "Episode: 350, episode的平均獎賞為200.0 \n"
     ]
    }
   ],
   "source": [
    "agent = Agent()#宣告智能體\n",
    "\n",
    "episode_number = 0\n",
    "reward_sum = 0\n",
    "while episode_number < TOTAL_EPISODES:\n",
    "    #執行智能體一輪一直到倒下為止，記錄這一輪的累計獎賞\n",
    "    reward_sum += run(agent)\n",
    "    episode_number += 1\n",
    "    if episode_number % BATCH_SIZE_BASELINE == 0:\n",
    "        print('Episode: {0}, episode的平均獎賞為{1} '.format(episode_number,reward_sum / BATCH_SIZE_BASELINE))\n",
    "        reward_sum = 0\n",
    "        agent.brain.model.save('Models/dqn_cntk.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 執行DQN模型\n",
    "\n",
    "前面將模型訓練好之後我們就可以開始來測試訓練好的DQN模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "num_episodes = 10  #欲執行的批次數\n",
    "\n",
    "#換原訓練好的模型\n",
    "root = C.load_model('Models/dqn_cntk.model')\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    observation = env.reset()  #每次episode開始歸零重置\n",
    "    done = False\n",
    "    n=0\n",
    "    tot_reward=0\n",
    "    while not done: \n",
    "        env.render()\n",
    "        #只根據模型預測結果進行act決定\n",
    "        action = np.argmax(root.eval([observation.astype(np.float32)]))\n",
    "        observation, reward, done, info  = env.step(action)  \n",
    "        #累加獎賞\n",
    "        tot_reward+=reward\n",
    "        n+=1\n",
    "    print('Episode {0} 累積獎賞:{0}'.format(i_episode,tot_reward))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
