{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文分詞(cntk)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "許多人初學自然語言處理時，面對道第一個棘手的問題就是中文分詞，是的，中文是這世界上少數沒有自帶分隔符號的語言，所以為了理解正確的語意，傳統的自然語言的第一步就是分詞(不少人用的都是JIEBA這個庫吧)，先不管分詞的合理性(人類看中文也沒分詞，機器一定要分詞嗎?之後我們會推出全程不分詞的中文分析範例)，那麼既然來到深度學習的世界，難道我們不能用深度學習模型來取代結巴分詞嗎?如此一來不但是端到端的處理，而且可以透過語料的補充與修正持續學習，比起只能透過自定義辭典來擴充的結巴來的有彈性許多，在這次的實作範例中我們就會來介紹如何使用深度學習模型來處理中文分詞。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在這個範例中，我們是使用SIGHAN Bakeoff的語料。SIGHAN是國際計算語言學會（ACL）中文語言處理小組的簡稱，其英文全稱為“Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics”。而Bakeoff則是SIGHAN所主辦的國際中文語言處理競賽，各位可以在以下網址下載語料。\n",
    "\n",
    "http://sighan.cs.uchicago.edu/bakeoff2005/\n",
    "\n",
    "特別需要說明的是這些中文分詞語料庫分別由臺灣中央研究院、香港城市大學、北京大學及微軟亞洲研究院提供，其中前二者是繁體中文，後二者是簡體中文,我們在此將只使用前兩份語料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import os\n",
    "import pickle\n",
    "import codecs\n",
    "import glob\n",
    "import math\n",
    "import builtins\n",
    "import string\n",
    "import cntk as C\n",
    "from cntk.ops import *\n",
    "from cntk.layers import *\n",
    "from cntk.initializer import *\n",
    "from cntk.logging import *\n",
    "from cntk.train import *\n",
    "from cntk.learners import *\n",
    "from cntk.losses import *\n",
    "from cntk.metrics import *\n",
    "from cntk.device import *\n",
    "import random\n",
    "\n",
    "# 是否使用GPU\n",
    "is_gpu = True\n",
    "if is_gpu:\n",
    "    try_set_default_device(gpu(0))\n",
    "else:\n",
    "    try_set_default_device(cpu())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在數據清洗邏輯中，將所有的全形符號轉成半形符號是很重要的(主要是數字、英文與標點符號)，全半形的編碼原則如下：\n",
    "    全形字元unicode編碼從65281~65374 （十六進位制 0xFF01 ~ 0xFF5E）\n",
    "    半形字元unicode編碼從33~126 （十六進位制 0x21~ 0x7E）\n",
    "    空格比較特殊,全形為 12288（0x3000）,半形為 32 （0x20）\n",
    "    而且除空格外,全形/半形按unicode編碼排序在順序上是對應的\n",
    "    \n",
    "所以我們可以把轉換原則寫成以下的str_full_to_half函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str_full_to_half(in_str):\n",
    "    out_str = []\n",
    "    for char in in_str:\n",
    "        inside_code = ord(char)\n",
    "        if inside_code == 0x3000 or inside_code == 12288 or char==string.whitespace: # 全形空格直接轉換\n",
    "             out_str.append(' ')\n",
    "        elif inside_code >= 65281 and inside_code <= 65374:\n",
    "            inside_code -= 0xfee0\n",
    "            out_str.append(chr(inside_code))\n",
    "        else:\n",
    "            out_str.append(char)\n",
    "        \n",
    "    return ''.join(out_str)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我們將兩份語料的分隔符號置換為「|」後合併，然後清除無效字元以及把所有全形轉半形後進行分行，即完成處理語料的過程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['時間|:', '三月|十日|(|星期四|)|上午|十時|。', '地點|:', '學術|活動|中心|一樓|簡報室|。', '主講|:', '民族所|所長|莊英章|先生|。', '講題|:', '閩|、|台|漢人|社會|研究|的|若干|考察|。', '李|院長|於|二月|二十六日|至|三月|十五日|赴|美|訪問|,', '期間|將|與|在|美|院士|商討|院務|,', '與|美國|大學|聯繫|商討|長期|合作|事宜|,', '並|辦理|加州|大學|退休|等|手續|。', '出國|期間|院務|由|羅|副院長|代行|。', '總辦事處|秘書組|主任|戴政|先生|請辭|獲准|,', '所|遺|職務|自|三月|一日|起|由|近代史|研究所|研究員|陶英惠|先生|兼任|。', '植物|研究所|所長|周昌弘|先生|當選|第三世界|科學院|(|The|Third|World|Academy|of|Sciences|,', '簡稱|TWAS|)|院士|。', 'TWAS|係|一九八三年|由|Prof|Adbus|Salam|(|巴基斯坦籍|,', '曾|獲|諾貝爾獎|)|發起|成立|,', '會員|遍佈|63|個|國家|,']\n"
     ]
    }
   ],
   "source": [
    "as_train=codecs.open('../Data/ex12_train/as_training.utf8',encoding='utf-8-sig').read()\n",
    "cityu_train=codecs.open('../Data/ex12_train/cityu_training.utf8',encoding='utf-8-sig').read()\n",
    "\n",
    "#兩個數據集的分割符號不太一樣\n",
    "as_train=as_train.replace('\\u3000','|').replace(' ','|')   #把分詞分隔號置換為'|'，否則會被視為空白被處理掉\n",
    "cityu_train=cityu_train.replace(' ','|')   #把分詞分隔號置換為'|'，否則會被視為空白被處理掉\n",
    "\n",
    "data=as_train+'\\r\\n'+cityu_train #把兩個語料合併\n",
    "data=data.strip() #去除無效的字元\n",
    "#as_train=as_train.translate(str.maketrans('０１２３４５６７８９', '0123456789')) #把全形數字轉半形(使用translate)\n",
    "data=str_full_to_half(data) #把所有全形轉半形\n",
    "\n",
    "raw_data_train=data.split('\\r\\n')#分行\n",
    "\n",
    "raw_data_train=[row.strip('\\n').strip('\\r').replace(\"\\x08\",'').replace(\"\\x80\",'') for row in raw_data_train] #移除分行字元\n",
    "\n",
    "print(raw_data_train[:20])\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我們可以利用一樣的流程整理驗證集數據。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['許多|社區|長青|學苑|多|開設|有|書法|、|插花|、|土風舞班|,', '文山區|長青|學苑|則|有|個|十分|特別|的|「|英文|歌唱班|」|,', '成員|年齡|均|超過|六十|歲|,', '這|群|白髮蒼蒼|,', '爺爺|、|奶奶級|的|學員|唱起|英文|歌|來|字正腔圓|,', '有模有樣|。', '對|他們|來說|,', '能|與|一|群|志同道合|的|朋友|共同|回味|年少|時期|流行|的|歌曲|,', '才|是|參加|英文|歌唱班|最|大|樂趣|。', '長青|學苑|英文|歌唱班|昨日|在|社教館|舉行|「|英文|懷念|金曲|演唱會|」|,', '曲目|包括|「|大江東去|」|、|「|月河|」|、|「|真善美|」|等|大眾|耳熟能詳|的|英文|歌曲|。', '難得|公開|演唱|,', '這些|有|著|豐富|人生|閱歷|的|學員|絲毫|不|覺得|緊張|怯場|,', '只|見|台|上|唱|得|盡興|,', '台|下|不少|聽眾|也|一時|技癢|跟|著|唱和|起來|。', '長青|學苑|英文|歌唱班|成立|至今|已|兩|年|,', '目前|成員|約|廿五|人|,', '年齡|都|在|六十|歲|以上|,', '其中|以|軍公教|退休|人員|居多|,', '並|有|現任|大學|教授|,']\n"
     ]
    }
   ],
   "source": [
    "as_test=codecs.open('../Data/ex12_train/as_testing_gold.utf8',encoding='utf-8-sig').read()\n",
    "cityu_test=codecs.open('../Data/ex12_train/cityu_test_gold.utf8',encoding='utf-8-sig').read()\n",
    "\n",
    "#兩個數據集的分割符號不太一樣\n",
    "as_test=as_test.replace('\\u3000','|').replace(' ','|')   #把分詞分隔號置換為'|'，否則會被視為空白被處理掉\n",
    "cityu_test=cityu_test.replace(' ','|')   #把分詞分隔號置換為'|'，否則會被視為空白被處理掉\n",
    "\n",
    "data_test=as_test+'\\r\\n'+cityu_test #把兩個語料合併\n",
    "data_test=data_test.strip() #去除無效的字元\n",
    "#as_train=as_train.translate(str.maketrans('０１２３４５６７８９', '0123456789')) #把全形數字轉半形(使用translate)\n",
    "data_test=str_full_to_half(data_test) #把所有全形轉半形\n",
    "\n",
    "raw_data_test=data_test.split('\\r\\n')#分行\n",
    "\n",
    "raw_data_test=[row.strip('\\n').strip('\\r').replace(\"\\x08\",'').replace(\"\\x80\",'') for row in raw_data_test] #移除分行字元\n",
    "\n",
    "print(raw_data_test[:20])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "接下來我們將語料去重複後排序，得到所有去重複的字典集合vocabs，各位可以在他的後面看到'\\ueb78', '\\uec95', '\\uecd4'這幾個字元，那其實是表情符號(emoji)，只是python無法正常顯示，但是我認為那些還是有意義地所以予以保留，此外，為了避免出現不在字典裡的字，因此我們在字典的一開始額外插入了未知代表字元('/Unknow')，所有不再字典裡的字都編碼成'/Unknow'。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6295\n",
      "['/Unknow', ' ', '!', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '\\\\', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', '°', '·', '×', 'ˊ', 'ˋ', '˙', 'Α', 'Β', 'Γ', 'Ε', 'Η', 'Ι']\n"
     ]
    }
   ],
   "source": [
    "vocabs=sorted(set(list(''.join(raw_data_train))))\n",
    "vocabs.remove('|')\n",
    "vocabs.insert(0,'/Unknow')\n",
    "\n",
    "print(len(vocabs))\n",
    "print(vocabs[:100])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "接下來我們要把vocabs的字以及索引順序編成字元轉索引(char_to_index)，以及索引轉字元(index_to_char)兩種dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'駁': 6009, '攤': 2265, '喂': 904, '沒': 2837, '癬': 3585, '汀': 2802, '疏': 3512, '逢': 5393, '杏': 2465, '商': 879, '安': 1398, '孛': 1376, '抨': 2020, '朱': 2456, '協': 686, 'ㄤ': 186, '嵨': 1553, '敗': 2284, '攸': 2272, '康': 1655, '雉': 5814, '傣': 449, '標': 2657, '隰': 5802, '值': 422, '綺': 4121, '嚼': 1031, '們': 402, '答': 3944, '鄂': 5482, '醫': 5530, '冼': 544, '℃': 115, '村': 2467, '停': 432, '孽': 1392, '稷': 3862, '洌': 2881, 'r': 77, '植': 2593, '慟': 1897, '哺': 854, '柏': 2502, '夸': 1221, '捂': 2081, '企': 301, '博': 688, '基': 1123, '敖': 2283, '笪': 3933, '傘': 445, '巢': 1578, '寮': 1445, '臻': 4416, '濮': 3109, '寺': 1450, '忞': 1767, '湎': 2988, '棶': 2588, '右': 742, '箴': 3967, 'b': 61, '線': 4131, '鎔': 5657, '礙': 3785, '嚏': 1019, '叵': 743, '疙': 3515, '梅': 2554, '寨': 1441, '澈': 3086, '壤': 1191, '推': 2126, '壆': 1181, '慰': 1904, '輸': 5322, '春': 2360, '溼': 3015, '骨': 6051, '嗟': 952, '乎': 231, '柺': 2522, '諙': 5029, '窟': 3892, '悟': 1831, '孳': 1387, '都': 5480, '莉': 4547, '擎': 2234, '彪': 1718, '美': 4215, 'i': 68, '決': 2825, '箬': 3964, '鑷': 5707, '磅': 3766, '遂': 5406, '慎': 1891, '勝': 639, '掃': 2106}\n"
     ]
    }
   ],
   "source": [
    "char_to_index=dict((w, i) for i, w in enumerate(vocabs))\n",
    "index_to_char=dict((i, w) for i, w in enumerate(vocabs))\n",
    "\n",
    "print({key:char_to_index[key] for key in list(char_to_index.keys())[:100]})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "接下來就是設計可以讀取minibatch數據的函數了。cntk跟其他家深度學習框架很不一樣的地方在於，它雖然是靜態的分析圖框架，卻有能力處理不定長度的序列，在cntk中，把序列視為一個包含多個形狀一致向量的清單。所以我們依照這個原則來設計數據讀取函數。將一個句子的文字轉換成文字的onehot向量，輸出則是變成對應的BMES(詞的開頭、詞的中間、詞的結尾以及單詞)\n",
    "\n",
    "你會發現在產生輸出的BMES的onehot向量時，我不是將對應索引的向量值設為1，而是利用np.random.choice(np.arange(0.8, 1.2, 0.01))將它指定為0.8~1.2之間的一個隨機數這個技巧稱之為label smoothing，屬於數據增強的技巧，有助於模型的泛化與穩定性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['而', '湯姆斯', '‧'], ['空門', '是', '禪', '的', '修行', '根基', ',']]\n"
     ]
    }
   ],
   "source": [
    "idx_train=0\n",
    "idxs_train=np.arange(len(raw_data_train))\n",
    "np.random.shuffle(idxs_train)\n",
    "\n",
    "idx_test=0\n",
    "idxs_test=np.arange(len(raw_data_test))\n",
    "np.random.shuffle(idxs_test)\n",
    "\n",
    "def get_next_minibatch(minibatch_size,is_train=True):\n",
    "    global idx_train,idxs_train,raw_data_train,idx_test,idxs_test,raw_data_test\n",
    "    features=[]\n",
    "    labels=[]\n",
    "    groundtruths=[]\n",
    "    idx=idx_train\n",
    "    idxs=idxs_train\n",
    "    raw_data=raw_data_train\n",
    "    if is_train==False:\n",
    "        idx=idx_test\n",
    "        idxs=idxs_test\n",
    "        raw_data=raw_data_test\n",
    "    np.random.shuffle(raw_data)\n",
    "    \n",
    "    while len(features)<minibatch_size:\n",
    "        features_seq=[]\n",
    "        labels_seq=[]\n",
    "        groundtruth_seq=[]\n",
    "        words=raw_data[idxs[idx]].split('|') #轉換成分詞後的詞清單\n",
    "        #BMES=>[0,1,2,3]\n",
    "        for word in words:\n",
    "            for i in range(len(word)):\n",
    "                feature_arr=np.zeros(len(vocabs),dtype=np.float32)\n",
    "                label_arr=np.zeros(4,dtype=np.float32)\n",
    "                #如果在字典中則取出其索引\n",
    "                if word[i] in char_to_index:\n",
    "                    feature_arr[char_to_index[word[i]]]=1\n",
    "                #否則定為未知\n",
    "                else:\n",
    "                    feature_arr[char_to_index['/Unknow']]=1\n",
    "                \n",
    "                #轉換為BMES\n",
    "                if len(word)==1 and i==0: #S 自己就是一個單詞\n",
    "                    label_arr[3]=np.random.choice(np.arange(0.8, 1.2, 0.01))\n",
    "                elif i==0: #B 是一個詞的開始\n",
    "                    label_arr[0]=np.random.choice(np.arange(0.8, 1.2, 0.01)) \n",
    "                elif i==len(word)-1:  #E 是一個詞的結束\n",
    "                    label_arr[2]=np.random.choice(np.arange(0.8, 1.2, 0.01))\n",
    "                else: #M 是一個詞的中間\n",
    "                    label_arr[1]=np.random.choice(np.arange(0.8, 1.2, 0.01))\n",
    "                #測試集不進行label smoothing\n",
    "                if is_train==False:\n",
    "                    label_arr[label_arr>0]=1\n",
    "                    \n",
    "                features_seq.append(feature_arr)\n",
    "                labels_seq.append(label_arr)\n",
    "            groundtruth_seq.append(word)\n",
    "        idx+=1\n",
    "        \n",
    "        if idx>len(idxs):\n",
    "            idx=0\n",
    "            np.random.shuffle(idxs)\n",
    "            \n",
    "        features.append(features_seq)\n",
    "        labels.append(labels_seq)\n",
    "        groundtruths.append(groundtruth_seq)\n",
    "        \n",
    "    idx_train=idx\n",
    "    if is_train==False:\n",
    "        idx_test=idx\n",
    "    return features,labels,groundtruths\n",
    "        \n",
    "x_feature,y_label,ground_truths=get_next_minibatch(2)\n",
    "\n",
    "print(ground_truths)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在準備好數據之後，我們來說明一下模型的架構，在這邊我們預計要使用的是雙向的lstm，因為對於分詞來說，上下文都是很重要的語意線索來源，模型結構如下：\n",
    "\n",
    "     +-----------+   +------------------+              +------------+\n",
    "x -->| Embedding |-->| BiRecurrent LSTM |--> dropout-->| DenseLayer |--> y\n",
    "     +-----------+   +------------------+              +------------+\n",
    "     \n",
    "這裡的輸入x是一個長度為len(vocabs)的onehot向量(請注意cntk是動態處理批次軸以及序列軸，因此不會呈現在形狀上)，透過embedding層將稀疏的onehot濃縮為長度256的字嵌入，然後輸入一個兩層的雙向lstm(隱藏層形狀為512)，最後透過dropout後送入全連接層，輸出為長度為4的onehot向量(BMES)。\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定義序列軸\n",
    "input_seq_axis = Axis('inputAxis')\n",
    "\n",
    "#定義輸入變數\n",
    "input_sequence = sequence.input_variable(shape=len(vocabs), sequence_axis=input_seq_axis)\n",
    "label_sequence = sequence.input_variable(shape=4, sequence_axis=input_seq_axis) #BMES四種\n",
    "\n",
    "\n",
    "#雙向遞迴\n",
    "def BiRecurrence(fwd, bwd):\n",
    "    F = C.layers.Recurrence(fwd)\n",
    "    G = C.layers.Recurrence(bwd, go_backwards=True)\n",
    "    x = C.placeholder()\n",
    "    apply_x = C.splice(F(x), G(x))\n",
    "    return apply_x \n",
    "\n",
    "def create_model(num_layers=2,hidden_dim=512):\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        return C.layers.Sequential([\n",
    "            C.layers.Embedding(256),\n",
    "            For(range(num_layers), lambda: \n",
    "                Sequential([Stabilizer(), \n",
    "                            BiRecurrence(LSTM(hidden_dim//2),LSTM(hidden_dim//2)),\n",
    "                            BatchNormalization()])),\n",
    "            Dropout(0.5),\n",
    "            C.layers.Dense(4)\n",
    "        ])\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "最後我們定義模型結構與損失函數，就可以開始進行訓練了。每隔500次Minibatch就將預測結果分詞與實際答案列印出來比較。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model recovered!\n",
      "Training 4241158 parameters in 21 parameter tensors.\n",
      "Learning rate per 1 samples: 0.0001\n",
      " Minibatch[   1- 100]: loss = -0.249303 * 44659, metric = 92.75% * 44659;\n",
      " Minibatch[ 101- 200]: loss = -0.296430 * 44848, metric = 92.44% * 44848;\n",
      " Minibatch[ 201- 300]: loss = -0.281151 * 44827, metric = 92.31% * 44827;\n",
      " Minibatch[ 301- 400]: loss = -0.268336 * 44810, metric = 92.51% * 44810;\n",
      " Minibatch[ 401- 500]: loss = -0.413194 * 45160, metric = 92.34% * 45160;\n",
      "-----測試集驗證--------\n",
      "Finished Evaluation [1]: Minibatch[1-5]: metric = 91.28% * 2854;\n",
      "predict:嚴格|考核|GDP|增長|中|的|環境|成本|,|將|“|環境|資產|增值|”|工作|及|其|成效|作為|考核|政府|行政|績效|的|重要|指標|;|\n",
      "answer :嚴格|考核|GDP|增長|中|的|環境|成本|,|將|“|環境|資產|增值|”|工作|及其|成效|作為|考核|政府|行政|績效|的|重要|指標|;|\n",
      "predict:也|早已|拉下|厚重|的|鐵門|,|\n",
      "answer :也|早已|拉下|厚重|的|鐵門|,|\n",
      "predict:向|客戶|、|學術界|與|媒體|展示|其|最|新|幾|項|研發|成果|。|\n",
      "answer :向|客戶|、|學術界|與|媒體|展示|其|最|新|幾|項|研發|成果|。|\n",
      "-----------------\n",
      " Minibatch[ 501- 600]: loss = -0.296743 * 44840, metric = 92.29% * 44840;\n",
      " Minibatch[ 601- 700]: loss = -0.296157 * 44403, metric = 91.70% * 44403;\n",
      " Minibatch[ 701- 800]: loss = -0.275664 * 45460, metric = 92.16% * 45460;\n",
      " Minibatch[ 801- 900]: loss = -0.334444 * 44554, metric = 92.27% * 44554;\n",
      " Minibatch[ 901-1000]: loss = -0.371350 * 45644, metric = 92.10% * 45644;\n",
      "-----測試集驗證--------\n",
      "Finished Evaluation [2]: Minibatch[1-5]: metric = 92.42% * 2507;\n",
      "predict:竟|偽造|莊某|印章|,|\n",
      "answer :竟|偽造|莊某|印章|,|\n",
      "predict:下午|到|萬華區|,|\n",
      "answer :下午|到|萬華區|,|\n",
      "predict:該|處|執行|人員|前後|三|次|發出|傳繳|通知書促|義務人|到處|繳納|稅款|,|\n",
      "answer :該|處|執行|人員|前後|三|次|發出|傳繳|通知書|促|義務人|到處|繳納|稅款|,|\n",
      "-----------------\n",
      " Minibatch[1001-1100]: loss = -0.222328 * 45462, metric = 92.48% * 45462;\n",
      " Minibatch[1101-1200]: loss = -0.397321 * 45540, metric = 92.41% * 45540;\n",
      " Minibatch[1201-1300]: loss = -0.342464 * 45760, metric = 92.61% * 45760;\n",
      " Minibatch[1301-1400]: loss = -0.389917 * 45022, metric = 92.20% * 45022;\n",
      " Minibatch[1401-1500]: loss = -0.288609 * 45949, metric = 92.29% * 45949;\n",
      "-----測試集驗證--------\n",
      "Finished Evaluation [3]: Minibatch[1-5]: metric = 91.43% * 2837;\n",
      "predict:但|畢竟|未|能|真正|深入|原住民|文化|的|底層|。|\n",
      "answer :但|畢竟|未|能|真正|深入|原住民|文化|的|底層|。|\n",
      "predict:其後|全|由|Socket478|接手|。|\n",
      "answer :其後|全|由|Socket478|接手|。|\n",
      "predict:還是|到|居家|修繕店|買|塗料|和|刷子|,|\n",
      "answer :還是|到|居家|修繕|店|買|塗料|和|刷子|,|\n",
      "-----------------\n",
      " Minibatch[1501-1600]: loss = -0.170576 * 46203, metric = 92.60% * 46203;\n"
     ]
    }
   ],
   "source": [
    "learning_rate=0.0001\n",
    "minibatch_size=32\n",
    "num_epochs=20\n",
    "\n",
    "#宣告模型結構\n",
    "rnn=create_model()\n",
    "z=rnn(input_sequence)\n",
    "if os.path.exists(\"Models/word_segment_cntk.lstm\"):\n",
    "    model=Function.load(\"Models/word_segment_cntk.lstm\")\n",
    "    z = model(input_sequence)\n",
    "    print('model recovered!')\n",
    "\n",
    "#定義進行訓練的損失函數以及錯誤率計算\n",
    "loss= cross_entropy_with_softmax(z,label_sequence)#+0.1*focal_loss(z,label_sequence)\n",
    "errs = classification_error(z, label_sequence)\n",
    "\n",
    "# 列印模型參數\n",
    "log_number_of_parameters(z);\n",
    "\n",
    "# 定義訓練器\n",
    "\n",
    "progress_printer = ProgressPrinter(freq=100, tag='Training', num_epochs=num_epochs)\n",
    "learner = adam(z.parameters,\n",
    "                lr=learning_rate_schedule([learning_rate], UnitType.sample, num_epochs),\n",
    "                momentum=momentum_as_time_constant_schedule([minibatch_size / -math.log(0.95)], epoch_size=num_epochs),\n",
    "                l2_regularization_weight=5e-4)\n",
    "trainer = Trainer(z, (loss, 1-errs), learner, progress_printer)\n",
    "for epoch in range(num_epochs):\n",
    "    mbs = 0\n",
    "    progress_printer.update_with_trainer(trainer, with_metric=True)\n",
    "    num_trained_samples = 0\n",
    "    while mbs<2000:\n",
    "        features, labels,ground_truths=get_next_minibatch(minibatch_size)\n",
    "        #進行訓練\n",
    "        trainer.train_minibatch({input_sequence: features, label_sequence: labels})\n",
    "        if mbs%100==0 and mbs>0:\n",
    "            z.save(\"Models/word_segment_cntk.lstm\")\n",
    "            features_test, labels_test,ground_truths_test=get_next_minibatch(minibatch_size,False)\n",
    "            trainer.test_minibatch({input_sequence: features_test, label_sequence: labels_test})\n",
    "            if mbs%500==0 :\n",
    "\n",
    "                print('-----測試集驗證--------')\n",
    "                trainer.summarize_test_progress()\n",
    "                result=z(features_test)\n",
    "                for i in range(3):\n",
    "                    answer='|'.join(ground_truths_test[i])+'|'\n",
    "                    pred=[]\n",
    "                    words=list(''.join(ground_truths_test[i]))\n",
    "                    for j in range(len(words)):\n",
    "                        word=words[j]\n",
    "                        onehot=np.argmax(result[i][j])\n",
    "                        if onehot>=2:\n",
    "                            pred.append(word+'|')\n",
    "                        else:\n",
    "                            pred.append(word)\n",
    "\n",
    "                    pred=''.join(pred)  \n",
    "                    print('predict:'+pred)\n",
    "                    print('answer :'+answer)\n",
    "                print('-----------------')\n",
    "            \n",
    "\n",
    "        if mbs%1000==0 and mbs>0:\n",
    "            learning_rate*=0.75\n",
    "\n",
    "        mbs += 1\n",
    "    #回報每個epoch訓練進度以及相關指標\n",
    "    trainer.summarize_training_progress()\n",
    "   \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
