{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文分詞(Pytorch)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "許多人初學自然語言處理時，面對道第一個棘手的問題就是中文分詞，是的，中文是這世界上少數沒有自帶分隔符號的語言，所以為了理解正確的語意，傳統的自然語言的第一步就是分詞(不少人用的都是JIEBA這個庫吧)，先不管分詞的合理性(人類看中文也沒分詞，機器一定要分詞嗎?之後我們會推出全程不分詞的中文分析範例)，那麼既然來到深度學習的世界，難道我們不能用深度學習模型來取代結巴分詞嗎?如此一來不但是端到端的處理，而且可以透過語料的補充與修正持續學習，比起只能透過自定義辭典來擴充的結巴來的有彈性許多，在這次的實作範例中我們就會來介紹如何使用深度學習模型來處理中文分詞。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在這個範例中，我們是使用SIGHAN Bakeoff的語料。SIGHAN是國際計算語言學會（ACL）中文語言處理小組的簡稱，其英文全稱為“Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics”。而Bakeoff則是SIGHAN所主辦的國際中文語言處理競賽，各位可以在以下網址下載語料。\n",
    "\n",
    "http://sighan.cs.uchicago.edu/bakeoff2005/\n",
    "\n",
    "特別需要說明的是這些中文分詞語料庫分別由臺灣中央研究院、香港城市大學、北京大學及微軟亞洲研究院提供，其中前二者是繁體中文，後二者是簡體中文,我們在此將只使用前兩份語料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import PIL  \n",
    "import os\n",
    "import pickle\n",
    "import codecs\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import builtins\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在數據清洗邏輯中，將所有的全形符號轉成半形符號是很重要的(主要是數字、英文與標點符號)，全半形的編碼原則如下：\n",
    "    全形字元unicode編碼從65281~65374 （十六進位制 0xFF01 ~ 0xFF5E）\n",
    "    半形字元unicode編碼從33~126 （十六進位制 0x21~ 0x7E）\n",
    "    空格比較特殊,全形為 12288（0x3000）,半形為 32 （0x20）\n",
    "    而且除空格外,全形/半形按unicode編碼排序在順序上是對應的\n",
    "    \n",
    "所以我們可以把轉換原則寫成以下的str_full_to_half函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str_full_to_half(in_str):\n",
    "    out_str = []\n",
    "    for char in in_str:\n",
    "        inside_code = ord(char)\n",
    "        if inside_code == 0x3000 or inside_code == 12288 or char==string.whitespace: # 全形空格直接轉換\n",
    "             out_str.append(' ')\n",
    "        elif inside_code >= 65281 and inside_code <= 65374:\n",
    "            inside_code -= 0xfee0\n",
    "            out_str.append(chr(inside_code))\n",
    "        else:\n",
    "            out_str.append(char)\n",
    "        \n",
    "    return ''.join(out_str)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我們將兩份語料的分隔符號置換為「|」後合併，然後清除無效字元以及把所有全形轉半形後進行分行，即完成處理語料的過程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['時間|:', '三月|十日|(|星期四|)|上午|十時|。', '地點|:', '學術|活動|中心|一樓|簡報室|。', '主講|:', '民族所|所長|莊英章|先生|。', '講題|:', '閩|、|台|漢人|社會|研究|的|若干|考察|。', '李|院長|於|二月|二十六日|至|三月|十五日|赴|美|訪問|,', '期間|將|與|在|美|院士|商討|院務|,', '與|美國|大學|聯繫|商討|長期|合作|事宜|,', '並|辦理|加州|大學|退休|等|手續|。', '出國|期間|院務|由|羅|副院長|代行|。', '總辦事處|秘書組|主任|戴政|先生|請辭|獲准|,', '所|遺|職務|自|三月|一日|起|由|近代史|研究所|研究員|陶英惠|先生|兼任|。', '植物|研究所|所長|周昌弘|先生|當選|第三世界|科學院|(|The|Third|World|Academy|of|Sciences|,', '簡稱|TWAS|)|院士|。', 'TWAS|係|一九八三年|由|Prof|Adbus|Salam|(|巴基斯坦籍|,', '曾|獲|諾貝爾獎|)|發起|成立|,', '會員|遍佈|63|個|國家|,']\n",
      "211\n"
     ]
    }
   ],
   "source": [
    "as_train=codecs.open('../Data/ex12_train/as_training.utf8',encoding='utf-8-sig').read()\n",
    "cityu_train=codecs.open('../Data/ex12_train/cityu_training.utf8',encoding='utf-8-sig').read()\n",
    "\n",
    "#兩個數據集的分割符號不太一樣\n",
    "as_train=as_train.replace('\\u3000','|').replace(' ','|')   #把分詞分隔號置換為'|'，否則會被視為空白被處理掉\n",
    "cityu_train=cityu_train.replace(' ','|')   #把分詞分隔號置換為'|'，否則會被視為空白被處理掉\n",
    "\n",
    "data=as_train+'\\r\\n'+cityu_train #把兩個語料合併\n",
    "data=data.strip() #去除無效的字元\n",
    "#as_train=as_train.translate(str.maketrans('０１２３４５６７８９', '0123456789')) #把全形數字轉半形(使用translate)\n",
    "data=str_full_to_half(data) #把所有全形轉半形\n",
    "\n",
    "raw_data_train=data.split('\\r\\n')#分行\n",
    "\n",
    "raw_data_train=[row.strip('\\n').strip('\\r').replace(\"\\x08\",'').replace(\"\\x80\",'') for row in raw_data_train] #移除分行字元\n",
    "\n",
    "print(raw_data_train[:20])\n",
    "\n",
    "train_data_len=[len(txt.split('|'))  for txt in  raw_data_train]\n",
    "print(max(train_data_len))\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我們可以利用一樣的流程整理驗證集數據。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['許多|社區|長青|學苑|多|開設|有|書法|、|插花|、|土風舞班|,', '文山區|長青|學苑|則|有|個|十分|特別|的|「|英文|歌唱班|」|,', '成員|年齡|均|超過|六十|歲|,', '這|群|白髮蒼蒼|,', '爺爺|、|奶奶級|的|學員|唱起|英文|歌|來|字正腔圓|,', '有模有樣|。', '對|他們|來說|,', '能|與|一|群|志同道合|的|朋友|共同|回味|年少|時期|流行|的|歌曲|,', '才|是|參加|英文|歌唱班|最|大|樂趣|。', '長青|學苑|英文|歌唱班|昨日|在|社教館|舉行|「|英文|懷念|金曲|演唱會|」|,', '曲目|包括|「|大江東去|」|、|「|月河|」|、|「|真善美|」|等|大眾|耳熟能詳|的|英文|歌曲|。', '難得|公開|演唱|,', '這些|有|著|豐富|人生|閱歷|的|學員|絲毫|不|覺得|緊張|怯場|,', '只|見|台|上|唱|得|盡興|,', '台|下|不少|聽眾|也|一時|技癢|跟|著|唱和|起來|。', '長青|學苑|英文|歌唱班|成立|至今|已|兩|年|,', '目前|成員|約|廿五|人|,', '年齡|都|在|六十|歲|以上|,', '其中|以|軍公教|退休|人員|居多|,', '並|有|現任|大學|教授|,']\n",
      "104\n"
     ]
    }
   ],
   "source": [
    "as_test=codecs.open('../Data/ex12_train/as_testing_gold.utf8',encoding='utf-8-sig').read()\n",
    "cityu_test=codecs.open('../Data/ex12_train/cityu_test_gold.utf8',encoding='utf-8-sig').read()\n",
    "\n",
    "#兩個數據集的分割符號不太一樣\n",
    "as_test=as_test.replace('\\u3000','|').replace(' ','|')   #把分詞分隔號置換為'|'，否則會被視為空白被處理掉\n",
    "cityu_test=cityu_test.replace(' ','|')   #把分詞分隔號置換為'|'，否則會被視為空白被處理掉\n",
    "\n",
    "data_test=as_test+'\\r\\n'+cityu_test #把兩個語料合併\n",
    "data_test=data_test.strip() #去除無效的字元\n",
    "#as_train=as_train.translate(str.maketrans('０１２３４５６７８９', '0123456789')) #把全形數字轉半形(使用translate)\n",
    "data_test=str_full_to_half(data_test) #把所有全形轉半形\n",
    "\n",
    "raw_data_test=data_test.split('\\r\\n')#分行\n",
    "\n",
    "raw_data_test=[row.strip('\\n').strip('\\r').replace(\"\\x08\",'').replace(\"\\x80\",'') for row in raw_data_test] #移除分行字元\n",
    "\n",
    "print(raw_data_test[:20])\n",
    "test_data_len=[len(txt.split('|'))  for txt in  raw_data_test]\n",
    "print(max(test_data_len))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "接下來我們將語料去重複後排序，得到所有去重複的字典集合vocabs，各位可以在他的後面看到'\\ueb78', '\\uec95', '\\uecd4'這幾個字元，那其實是表情符號(emoji)，只是python無法正常顯示，但是我認為那些還是有意義地所以予以保留，此外，為了避免出現不在字典裡的字，因此我們在字典的一開始額外插入了未知代表字元('/Unknow')，所有不再字典裡的字都編碼成'/Unknow'。同時我們也加入了<PAD>表示將序列填充時的無意義字元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6296\n",
      "['<PAD>', '/Unknow', ' ', '!', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '\\\\', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', '°', '·', '×', 'ˊ', 'ˋ', '˙', 'Α', 'Β', 'Γ', 'Ε', 'Η']\n"
     ]
    }
   ],
   "source": [
    "vocabs=sorted(set(list(''.join(raw_data_train))))\n",
    "vocabs.remove('|')\n",
    "vocabs.insert(0,'/Unknow')\n",
    "vocabs.insert(0,'<PAD>')\n",
    "\n",
    "print(len(vocabs))\n",
    "print(vocabs[:100])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "接下來我們要把vocabs的字以及索引順序編成字元轉索引(char_to_index)，以及索引轉字元(index_to_char)兩種dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'凸': 560, '聘': 4279, '夜': 1208, '嫩': 1348, 'H': 39, '隉': 5789, '驥': 6050, '鷺': 6202, '健': 434, '訪': 4965, '酒': 5504, '胥': 4328, '過': 5413, '邱': 5464, '晝': 2378, '戶': 1980, '鄰': 5493, '筑': 3943, '吐': 760, '荼': 4546, '謝': 5061, '欞': 2710, '勇': 633, '臺': 4416, '噌': 994, '偏': 430, '些': 257, '噸': 1014, '綸': 4120, '繆': 4164, '娉': 1298, '呲': 798, '盾': 3641, '逑': 5381, '槐': 2642, '悌': 1826, '捉': 2085, '羅': 4211, '充': 499, '酬': 5513, '民': 2784, '嬤': 1361, '馨': 6003, '邐': 5453, '伺': 317, '粳': 4029, '毽': 2780, '煽': 3213, '密': 1428, '嚴': 1029, '滁': 3018, '衩': 4852, '卜': 690, '換': 2143, '鍬': 5650, '仄': 275, '壹': 1197, '橫': 2677, '斂': 2299, '膨': 4389, '真': 3650, '遵': 5435, '迓': 5354, '冊': 529, 'Ⅰ': 117, '鴃': 6144, '佶': 350, '銅': 5591, '礽': 3793, '斯': 2317, '咆': 810, '轍': 5332, '訶': 4969, '猥': 3325, '題': 5926, '殞': 2751, '憲': 1927, '察': 1436, '袞': 4867, '地': 1074, '凰': 556, '冇': 527, '麥': 6224, '段': 2758, '揹': 2154, '殼': 2761, '●': 137, '榫': 2630, '托': 1995, '勾': 652, '薯': 4685, '廛': 1670, '槌': 2639, '詬': 4988, '紂': 4056, '季': 1382, '頤': 5916, '危': 697, '勃': 632, '骰': 6054}\n"
     ]
    }
   ],
   "source": [
    "char_to_index=dict((w, i) for i, w in enumerate(vocabs))\n",
    "index_to_char=dict((i, w) for i, w in enumerate(vocabs))\n",
    "\n",
    "print({key:char_to_index[key] for key in list(char_to_index.keys())[:100]})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "接下來就是設計可以讀取minibatch數據的函數了。pytorch方便(一開始學覺得奇怪)的地方在於如果輸入或是輸出是onehot，其實是不需要真的使用onehot向量，而是直接傳索引即可，也因此我們的輸入的形狀不需要是(批次,序列長,len(vocabs))只要是(批次,序列長)就可以了，同樣的，我們的輸出是變成對應的BMES(詞的開頭、詞的中間、詞的結尾以及單詞)，它的形狀不需要是(批次,序列長,4)只要(批次,序列長)就夠了。\n",
    "\n",
    "Pytorch其實是動態計算圖，所以它不需要固定長度的序列，在這個時作當中我們將會介紹如何根據他動態計算圖的特性，來自動做到動態序列長度的模型。雖然式動態計算圖，他還是要設定一個序列長度上限，在此我們設定為128，長度超過的句子就捨棄，長度為0的空字串必須排除。\n",
    "\n",
    "動態計算圖如何處理一個minibatch的數據呢?動態計算圖的一個minibatch內的序列長度仍然必須一致，但是minibatch間的長度則可以不同。所以在一個批次內我們將會以裡面最大的長度為基準，其餘補滿零。那除了最長的那個序列之外，其他較短的序列欠缺不足之處仍就是透過補零(<PAD>)補滿，那我們該如何知道實際的序列長度呢?，所以我們必須準備另外一個形狀為(批次)的一維向量來存放每個序列的長度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 128)\n",
      "(2, 128)\n",
      "[30 11]\n",
      "[['進入', '西堤島', '大都', '為', '了', '觀賞', '聖母院', '及', 'La', 'Conciergerie', ','], ['要', '化解', '臺灣', '的', '教育', '問題', ',']]\n"
     ]
    }
   ],
   "source": [
    "idx_train=0\n",
    "idxs_train=np.arange(len(raw_data_train))\n",
    "np.random.shuffle(idxs_train)\n",
    "\n",
    "idx_test=0\n",
    "idxs_test=np.arange(len(raw_data_test))\n",
    "np.random.shuffle(idxs_test)\n",
    "\n",
    "def get_next_minibatch(minibatch_size,is_train=True):\n",
    "    global idx_train,idxs_train,raw_data_train,idx_test,idxs_test,raw_data_test\n",
    "\n",
    "    groundtruths=[]\n",
    "    idx=idx_train\n",
    "    idxs=idxs_train\n",
    "    raw_data=raw_data_train\n",
    "    if is_train==False:\n",
    "        idx=idx_test\n",
    "        idxs=idxs_test\n",
    "        raw_data=raw_data_test\n",
    "    np.random.shuffle(raw_data)\n",
    "    #定義輸出向量形狀\n",
    "    feature_arr=np.zeros((minibatch_size,128))\n",
    "    label_arr=np.zeros((minibatch_size,128))\n",
    "    length_arr=np.zeros((minibatch_size))\n",
    "    batch=0\n",
    "    while batch<minibatch_size:\n",
    "        seq_len=len(raw_data[idxs[idx]].replace('|',''))\n",
    "        if seq_len<=128 and seq_len>0:\n",
    "            length_arr[batch]=seq_len\n",
    "            \n",
    "            groundtruth_seq=[]\n",
    "            words=raw_data[idxs[idx]].split('|') #轉換成分詞後的詞清單\n",
    "\n",
    "            pos=0\n",
    "            #BMES=>[0,1,2,3]\n",
    "            for word in words:\n",
    "                for i in range(len(word)):\n",
    "                    #如果在字典中則取出其索引\n",
    "                    if word[i] in char_to_index:\n",
    "                        feature_arr[batch,pos]=char_to_index[word[i]]\n",
    "                    #否則定為未知\n",
    "                    else:\n",
    "                        feature_arr[batch,pos]=char_to_index['/Unknow']\n",
    "\n",
    "                    #轉換為BMES\n",
    "                    if len(word)==1 and i==0: #S 自己就是一個單詞\n",
    "                        label_arr[batch,pos]=3\n",
    "                    elif i==0: #B 是一個詞的開始\n",
    "                        label_arr[batch,pos]=0\n",
    "                    elif i==len(word)-1:  #E 是一個詞的結束\n",
    "                        label_arr[batch,pos]=2\n",
    "                    else: #M 是一個詞的中間\n",
    "                        label_arr[batch,pos]=1  \n",
    "                    pos+=1\n",
    "\n",
    "                groundtruth_seq.append(word)\n",
    "            groundtruths.append(groundtruth_seq)\n",
    "            batch+=1\n",
    "        idx+=1\n",
    "        if idx>len(idxs):\n",
    "            idx=0\n",
    "            np.random.shuffle(idxs)\n",
    "    idx_train=idx\n",
    "    if is_train==False:\n",
    "        idx_test=idx\n",
    "    return feature_arr.astype(np.int64),label_arr.astype(np.int64),length_arr.astype(np.int64),groundtruths\n",
    "        \n",
    "x_feature,y_label,x_len,ground_truths=get_next_minibatch(2)\n",
    "print(x_feature.shape)\n",
    "print(y_label.shape)\n",
    "print(x_len)\n",
    "print(ground_truths)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在準備好數據之後，我們來說明一下模型的架構，在這邊我們預計要使用的是雙向的lstm，因為對於分詞來說，上下文都是很重要的語意線索來源，模型結構如下：\n",
    "\n",
    "     +-----------+   +------------------+              +------------+\n",
    "x -->| Embedding |-->| BiRecurrent LSTM |--> dropout-->| DenseLayer |--> y\n",
    "     +-----------+   +------------------+              +------------+\n",
    "     \n",
    "這裡的輸入x是一個(批次,序列長)的張量，透過embedding層將輸入內部展開為onehot後再濃縮為長度256的字嵌入，然後輸入一個兩層的雙向lstm(隱藏層形狀為256，由於是雙向所以實際輸出是512)，最後透過dropout後送入全連接層，輸出為長度為4的onehot向量(BMES)。在這個模型中，內部數據張量的形狀變化比較複雜，建議各位學習時，可以把裡面print語法的註解拿掉，可以方便各位學習裡面張量的變動。\n",
    "\n",
    "在模型中需要注意的是，pytorch裡面提供了pack_padded_sequence以及pad_packed_sequence這兩個函數(天殺的名字取得像繞口令一樣!!)\n",
    "其中， pack_padded_sequence 來確保 LSTM 模型不會處理用於填充的序列成員\n",
    "而pad_packed_sequence 則是用來解包（unpack）pack_padded_sequence 操作後的序列\n",
    "\n",
    "由於我們的批次內的序列長度是不固定的，因此我們會故意將他reshape變成(批次*序列長度,隱藏層*2)，把序列長度趕到批次維度那邊去，這樣我們後續的全連接層就還是可以維持固定的形狀定義不受影響。請注意，這裡的隱藏層*2主要是因為我們使用的是雙向lstm，因此將兩個方向的向量concate在一起就會變成兩倍大。\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, batch_size,vocab_size,embed_size, hidden_size, output_size):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.batch_size=batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embed_size = embed_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size,padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embed_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.liner = nn.Linear(hidden_size*2, output_size) \n",
    "    def init_hidden(self):\n",
    "        #2*2的意思是指:lstm層數*雙向lstm\n",
    "        hidden_a = torch.randn(2*2, self.batch_size, self.hidden_size).to(device)\n",
    "        hidden_b = torch.randn(2*2, self.batch_size, self.hidden_size).to(device)\n",
    "        hidden_a = Variable(hidden_a)\n",
    "        hidden_b = Variable(hidden_b)\n",
    "        return (hidden_a, hidden_b)\n",
    "    def forward(self,x, x_lengths):\n",
    "        #Lstmt初始的隱藏狀態\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        #print(x.shape)\n",
    "        output = self.embedding(x)\n",
    "        #print(output.shape)#(批次，序列長度,嵌入層)\n",
    "        \n",
    "        output = torch.nn.utils.rnn.pack_padded_sequence(output, x_lengths, batch_first=True)\n",
    "        \n",
    "        output, self.hidden = self.lstm(output, self.hidden)\n",
    "        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        #print(output.shape)#(批次，序列長度,隱藏層*2)\n",
    "        \n",
    "        output = output.contiguous()\n",
    "        output = output.view(-1, output.shape[2]) \n",
    "        #print(output.shape)#(批次*序列長度,隱藏層*2)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        \n",
    "        output = self.liner(output)\n",
    "        #print(output.shape)#(批次*序列長度,隱藏層*2)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        output = output.view(self.batch_size, output.size(0)//self.batch_size, self.output_size)\n",
    "        #print(output.shape)#(批次,序列長度,輸出)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "模型這邊透過了內建函數解決了不處理填充序列成員的問題，但是我們在計算損失函數的時候也別忘記了不要把填充成員納入計算，因此我們需要寫一個自動億的損失函數，根據input_x是否為零來判斷要被遮罩的區域，然後將填充成員蓋住後再計算損失函數。\n",
    "\n",
    "在這邊需要注意的是這裡的 target_y以及input_x的序列長度仍舊是原本定義的最大長度128，但是pred_y卻變成了該minibatch內的最大長度，所以計算損失函數時要記得將這兩個都要透過切片([:pred_y.shape[0]])來讓三者長度對齊。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def masked_loss(pred_y, target_y,input_x,num_class=5):\n",
    "        # flatten all predictions\n",
    "        pred_y = pred_y.view(-1,num_class)\n",
    "        #print('pred_y'+str(pred_y.shape))\n",
    "        # flatten all the labels\n",
    "        target_y = target_y.view(-1)[:pred_y.shape[0]]\n",
    "        #print('target_y'+str(target_y.shape))\n",
    "        input_x = input_x.view(-1)[:pred_y.shape[0]]\n",
    "        #print('input_x'+str(input_x.shape))\n",
    "        \n",
    "        \n",
    "\n",
    "        # create a mask by filtering out all tokens that ARE NOT the padding token\n",
    "        mask = (input_x>0).float()\n",
    "        #print('mask'+str(mask.shape))\n",
    "        # count how many tokens we have\n",
    "        nb_tokens = int(torch.sum(mask).data.item())\n",
    "        # pick the values for the label and zero out the rest with the mask\n",
    "        pred_y = (1-pred_y[range(pred_y.shape[0]),target_y]) * mask\n",
    "\n",
    "        # compute cross entropy loss which ignores all <PAD> tokens\n",
    "        ce_loss = torch.sum(pred_y) / nb_tokens\n",
    "        return ce_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "最後我們定義模型結構與損失函數，就可以開始進行訓練了。每隔500次Minibatch就將預測結果分詞與實際答案列印出來比較。\n",
    "\n",
    "\n",
    "還有一點要注意的是，使用 PyTorch 的 PackedSequence 雖然可以較快速的處理長短不一的序列1資料，但是用起來有個不方便的地方。就是同一個 batch 裡的資料，長度必須由長到短排列。所以在訓練的語法中我加入了處理排序的語法，同時我們可以根據sorted_idx追溯原本的排序結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recovered!!\n",
      "start epoch!\n",
      "Epoch: 1/30... Step: 0... Loss: 0.6486... Accuracy: 70.877%...\n",
      "-----測試集驗證--------\n",
      "predict:然而|,|中策|組|不單是|特首|的|智囊|和|政治|顧問|,|它|也|是|協助|特區|政府|制訂|公共|政策|的|研究|機關|,|它要|經常|接觸|不同|界別|人士|,|吸收|不同|派別|的|社會精英和|專家|學者|的|意見|,|作為|特區|政府|制訂|政策|的|參考|,|這個|角色|和|協助|執政|黨派|與|反對|黨|派作|政治|鬥爭|是|明顯|有|衝突|的|。|\n",
      "answer :然而|,|中策組|不單|是|特首|的|智囊|和|政治|顧問|,|它|也是|協助|特區|政府|制訂|公共|政策|的|研究|機關|,|它|要|經常|接觸|不同|界別|人士|,|吸收|不同|派別|的|社會|精英|和|專家|學者|的|意見|,|作為|特區|政府|制訂|政策|的|參考|,|這個|角色|和|協助|執政黨派|與|反對黨派|作|政治|鬥爭|是|明顯|有|衝突|的|。|\n",
      "predict:由於誠|泰隊|的|日籍|投手|教練|中本|與|中華奧|運隊|的|日籍|投手|教練|酒井|光|次|郎有|連絡|,|商量|過|如何|幫林|英傑|進行|奧|運行|的|調整|,|因此|誠泰隊|將|盡量|讓林|英傑|不要|太|勞累|,|先發|的|投球|數|控制|在|10|0|球|之內|。|\n",
      "answer :由於|誠泰隊|的|日籍|投手|教練|中本|與|中華|奧運隊|的|日籍|投手|教練|酒井光次郎|有|連絡|,|商量|過|如何|幫|林英傑|進行|奧運行|的|調整|,|因此|誠泰隊|將|盡量|讓|林英傑|不要|太|勞累|,|先發|的|投球數|控制|在|100|球|之內|。|\n",
      "predict:也|歡迎|本籍|看護|、|家屬|及|助理|護士|、|居家|服務|員|等|相關|醫療|人員|報名|參加|。|\n",
      "answer :也|歡迎|本籍|看護|、|家屬|及|助理|護士|、|居家|服務員|等|相關|醫療|人員|報名|參加|。|\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:250: UserWarning: Couldn't retrieve source code for container of type CharRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30... Step: 100... Loss: 0.6427... Accuracy: 71.785%...\n"
     ]
    }
   ],
   "source": [
    "learning_rate=0.0005\n",
    "minibatch_size=64\n",
    "num_epochs=20\n",
    "\n",
    "#宣告模型結構\n",
    "#output_size還是4，不要計算pad\n",
    "model = CharRNN(batch_size=minibatch_size,vocab_size=len(vocabs),embed_size=256, hidden_size=256, output_size=4)\n",
    "model.to(device)\n",
    "if os.path.exists('Models/word_segment_pytorch.lstm'):\n",
    "    model=torch.load('Models/word_segment_pytorch.lstm')\n",
    "    print('recovered!!')\n",
    "\n",
    "model.train()\n",
    "model_optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.95, 0.999))\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "\n",
    "\n",
    "epochs=30\n",
    "accuracy_numerator=[]\n",
    "accuracy_denominator=[]\n",
    "print('start epoch!')\n",
    "for epoch in range(epochs):\n",
    "    mbs=0\n",
    "    while mbs<2000:\n",
    "        features, labels,lens,ground_truths=get_next_minibatch(minibatch_size)\n",
    "        inputs, targets,seq_len = torch.from_numpy(features.astype(np.int64)), torch.from_numpy(labels.astype(np.int64)), torch.from_numpy(lens.astype(np.int64))\n",
    "\n",
    "        inputs, targets,seq_len = Variable(inputs), Variable(targets), Variable(seq_len).to(device)\n",
    "        \n",
    "        #所有輸入的sequence必須由長到短排序才可以輸入\n",
    "        seq_len_sorted, sorted_idx = seq_len.sort(descending=True)\n",
    "        inputs_sorted, targets_sorted,seq_len_sorted=inputs[sorted_idx].to(device), targets[sorted_idx].to(device),seq_len_sorted.to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        \n",
    "        output= model.forward(inputs_sorted,seq_len_sorted)\n",
    "        loss= masked_loss(output,targets_sorted,inputs_sorted,4)\n",
    "        loss.backward()\n",
    "        \n",
    "        output_arr=np.argmax(output.data.cpu().numpy(),-1)\n",
    "        target_arr=targets_sorted.data.cpu().numpy()[:,:output_arr.shape[1]]\n",
    "        mask=np.greater(inputs_sorted.data.cpu().numpy(),0)[:,:output_arr.shape[1]]\n",
    "        \n",
    "        accuracy_numerator.append((np.equal(output_arr,target_arr)*mask).sum())\n",
    "        accuracy_denominator.append(mask.sum())\n",
    "        \n",
    "\n",
    "        model_optimizer.step()\n",
    "        if mbs % 100 == 0:\n",
    "            #列印訓練狀態\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "                      \"Step: {}...\".format(mbs),\n",
    "                      \"Loss: {:.4f}...\".format(loss.data.item()),\n",
    "                      \"Accuracy: {:.3%}...\".format(sum(accuracy_numerator)/sum(accuracy_denominator)))\n",
    "            torch.save(model, 'Models/word_segment_pytorch.lstm')\n",
    "            accuracy_numerator=[]\n",
    "            accuracy_denominator=[]\n",
    "        if mbs % 500 == 0:\n",
    "            learning_rate*=0.75\n",
    "            test_features, test_labels,test_lens,test_ground_truths=get_next_minibatch(minibatch_size,False)\n",
    "            inputs, targets,seq_len = torch.from_numpy(test_features.astype(np.int64)), torch.from_numpy(test_labels.astype(np.int64)), torch.from_numpy(test_lens.astype(np.int64))\n",
    "            inputs, targets,seq_len = Variable(inputs), Variable(targets), Variable(seq_len).to(device)\n",
    "            #所有輸入的sequence必須由長到短排序才可以輸入\n",
    "            seq_len_sorted, sorted_idx = seq_len.sort(descending=True)\n",
    "            inputs_sorted, targets_sorted,seq_len_sorted=inputs[sorted_idx].to(device), targets[sorted_idx].to(device),seq_len_sorted.to(device)\n",
    "            output= model.forward(inputs_sorted,seq_len_sorted)\n",
    "            sorted_idx_arr=sorted_idx.data.cpu().numpy()\n",
    "            result=np.argmax(output.data.cpu().numpy(),-1)\n",
    "            print('-----測試集驗證--------')\n",
    "            for i in range(3):\n",
    "                answer='|'.join(test_ground_truths[sorted_idx_arr[i]])+'|'\n",
    "                pred=[]\n",
    "                words=list(''.join(test_ground_truths[sorted_idx_arr[i]]))\n",
    "                for j in range(len(words)):\n",
    "                    word=words[j]\n",
    "                    onehot=result[i][j]\n",
    "                    if onehot>=2:\n",
    "                        pred.append(word+'|')\n",
    "                    else:\n",
    "                        pred.append(word)\n",
    "\n",
    "                pred=''.join(pred)  \n",
    "                print('predict:'+pred)\n",
    "                print('answer :'+answer)\n",
    "            print('-----------------')\n",
    "            \n",
    "            \n",
    "    \n",
    "        mbs += 1\n",
    "   \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
