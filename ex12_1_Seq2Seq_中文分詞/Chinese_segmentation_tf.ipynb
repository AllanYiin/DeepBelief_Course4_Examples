{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文分詞(Tensorflow+Keras)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "許多人初學自然語言處理時，面對道第一個棘手的問題就是中文分詞，是的，中文是這世界上少數沒有自帶分隔符號的語言，所以為了理解正確的語意，傳統的自然語言的第一步就是分詞(不少人用的都是JIEBA這個庫吧)，先不管分詞的合理性(人類看中文也沒分詞，機器一定要分詞嗎?之後我們會推出全程不分詞的中文分析範例)，那麼既然來到深度學習的世界，難道我們不能用深度學習模型來取代結巴分詞嗎?如此一來不但是端到端的處理，而且可以透過語料的補充與修正持續學習，比起只能透過自定義辭典來擴充的結巴來的有彈性許多，在這次的實作範例中我們就會來介紹如何使用深度學習模型來處理中文分詞。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在這個範例中，我們是使用SIGHAN Bakeoff的語料。SIGHAN是國際計算語言學會（ACL）中文語言處理小組的簡稱，其英文全稱為“Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics”。而Bakeoff則是SIGHAN所主辦的國際中文語言處理競賽，各位可以在以下網址下載語料。\n",
    "\n",
    "http://sighan.cs.uchicago.edu/bakeoff2005/\n",
    "\n",
    "特別需要說明的是這些中文分詞語料庫分別由臺灣中央研究院、香港城市大學、北京大學及微軟亞洲研究院提供，其中前二者是繁體中文，後二者是簡體中文,我們在此將只使用前兩份語料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import PIL  \n",
    "import os\n",
    "import pickle\n",
    "import codecs\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import builtins\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.losses import *\n",
    "from keras import backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.applications import *\n",
    "from keras.applications.imagenet_utils import *\n",
    "from keras.optimizers import  *\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "#如果你使用tensorflow+keras總是異常掛掉，請務必加入以下語法(貪心的tf把所有gpu全吃光一點不留給keras，變成全掛的慘劇)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "config.gpu_options.visible_device_list = \"0\"\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.90\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "#run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在數據清洗邏輯中，將所有的全形符號轉成半形符號是很重要的(主要是數字、英文與標點符號)，全半形的編碼原則如下：\n",
    "    全形字元unicode編碼從65281~65374 （十六進位制 0xFF01 ~ 0xFF5E）\n",
    "    半形字元unicode編碼從33~126 （十六進位制 0x21~ 0x7E）\n",
    "    空格比較特殊,全形為 12288（0x3000）,半形為 32 （0x20）\n",
    "    而且除空格外,全形/半形按unicode編碼排序在順序上是對應的\n",
    "    \n",
    "所以我們可以把轉換原則寫成以下的str_full_to_half函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str_full_to_half(in_str):\n",
    "    out_str = []\n",
    "    for char in in_str:\n",
    "        inside_code = ord(char)\n",
    "        if inside_code == 0x3000 or inside_code == 12288 or char==string.whitespace: # 全形空格直接轉換\n",
    "             out_str.append(' ')\n",
    "        elif inside_code >= 65281 and inside_code <= 65374:\n",
    "            inside_code -= 0xfee0\n",
    "            out_str.append(chr(inside_code))\n",
    "        else:\n",
    "            out_str.append(char)\n",
    "        \n",
    "    return ''.join(out_str)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我們將兩份語料的分隔符號置換為「|」後合併，然後清除無效字元以及把所有全形轉半形後進行分行，即完成處理語料的過程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['時間|:', '三月|十日|(|星期四|)|上午|十時|。', '地點|:', '學術|活動|中心|一樓|簡報室|。', '主講|:', '民族所|所長|莊英章|先生|。', '講題|:', '閩|、|台|漢人|社會|研究|的|若干|考察|。', '李|院長|於|二月|二十六日|至|三月|十五日|赴|美|訪問|,', '期間|將|與|在|美|院士|商討|院務|,', '與|美國|大學|聯繫|商討|長期|合作|事宜|,', '並|辦理|加州|大學|退休|等|手續|。', '出國|期間|院務|由|羅|副院長|代行|。', '總辦事處|秘書組|主任|戴政|先生|請辭|獲准|,', '所|遺|職務|自|三月|一日|起|由|近代史|研究所|研究員|陶英惠|先生|兼任|。', '植物|研究所|所長|周昌弘|先生|當選|第三世界|科學院|(|The|Third|World|Academy|of|Sciences|,', '簡稱|TWAS|)|院士|。', 'TWAS|係|一九八三年|由|Prof|Adbus|Salam|(|巴基斯坦籍|,', '曾|獲|諾貝爾獎|)|發起|成立|,', '會員|遍佈|63|個|國家|,']\n",
      "211\n"
     ]
    }
   ],
   "source": [
    "as_train=codecs.open('../Data/ex12_train/as_training.utf8',encoding='utf-8-sig').read()\n",
    "cityu_train=codecs.open('../Data/ex12_train/cityu_training.utf8',encoding='utf-8-sig').read()\n",
    "\n",
    "#兩個數據集的分割符號不太一樣\n",
    "as_train=as_train.replace('\\u3000','|').replace(' ','|')   #把分詞分隔號置換為'|'，否則會被視為空白被處理掉\n",
    "cityu_train=cityu_train.replace(' ','|')   #把分詞分隔號置換為'|'，否則會被視為空白被處理掉\n",
    "\n",
    "data=as_train+'\\r\\n'+cityu_train #把兩個語料合併\n",
    "data=data.strip() #去除無效的字元\n",
    "#as_train=as_train.translate(str.maketrans('０１２３４５６７８９', '0123456789')) #把全形數字轉半形(使用translate)\n",
    "data=str_full_to_half(data) #把所有全形轉半形\n",
    "\n",
    "raw_data_train=data.split('\\r\\n')#分行\n",
    "\n",
    "raw_data_train=[row.strip('\\n').strip('\\r').replace(\"\\x08\",'').replace(\"\\x80\",'') for row in raw_data_train] #移除分行字元\n",
    "\n",
    "print(raw_data_train[:20])\n",
    "\n",
    "train_data_len=[len(txt.split('|'))  for txt in  raw_data_train]\n",
    "print(max(train_data_len))\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我們可以利用一樣的流程整理驗證集數據。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['許多|社區|長青|學苑|多|開設|有|書法|、|插花|、|土風舞班|,', '文山區|長青|學苑|則|有|個|十分|特別|的|「|英文|歌唱班|」|,', '成員|年齡|均|超過|六十|歲|,', '這|群|白髮蒼蒼|,', '爺爺|、|奶奶級|的|學員|唱起|英文|歌|來|字正腔圓|,', '有模有樣|。', '對|他們|來說|,', '能|與|一|群|志同道合|的|朋友|共同|回味|年少|時期|流行|的|歌曲|,', '才|是|參加|英文|歌唱班|最|大|樂趣|。', '長青|學苑|英文|歌唱班|昨日|在|社教館|舉行|「|英文|懷念|金曲|演唱會|」|,', '曲目|包括|「|大江東去|」|、|「|月河|」|、|「|真善美|」|等|大眾|耳熟能詳|的|英文|歌曲|。', '難得|公開|演唱|,', '這些|有|著|豐富|人生|閱歷|的|學員|絲毫|不|覺得|緊張|怯場|,', '只|見|台|上|唱|得|盡興|,', '台|下|不少|聽眾|也|一時|技癢|跟|著|唱和|起來|。', '長青|學苑|英文|歌唱班|成立|至今|已|兩|年|,', '目前|成員|約|廿五|人|,', '年齡|都|在|六十|歲|以上|,', '其中|以|軍公教|退休|人員|居多|,', '並|有|現任|大學|教授|,']\n",
      "104\n"
     ]
    }
   ],
   "source": [
    "as_test=codecs.open('../Data/ex12_train/as_testing_gold.utf8',encoding='utf-8-sig').read()\n",
    "cityu_test=codecs.open('../Data/ex12_train/cityu_test_gold.utf8',encoding='utf-8-sig').read()\n",
    "\n",
    "#兩個數據集的分割符號不太一樣\n",
    "as_test=as_test.replace('\\u3000','|').replace(' ','|')   #把分詞分隔號置換為'|'，否則會被視為空白被處理掉\n",
    "cityu_test=cityu_test.replace(' ','|')   #把分詞分隔號置換為'|'，否則會被視為空白被處理掉\n",
    "\n",
    "data_test=as_test+'\\r\\n'+cityu_test #把兩個語料合併\n",
    "data_test=data_test.strip() #去除無效的字元\n",
    "#as_train=as_train.translate(str.maketrans('０１２３４５６７８９', '0123456789')) #把全形數字轉半形(使用translate)\n",
    "data_test=str_full_to_half(data_test) #把所有全形轉半形\n",
    "\n",
    "raw_data_test=data_test.split('\\r\\n')#分行\n",
    "\n",
    "raw_data_test=[row.strip('\\n').strip('\\r').replace(\"\\x08\",'').replace(\"\\x80\",'') for row in raw_data_test] #移除分行字元\n",
    "\n",
    "print(raw_data_test[:20])\n",
    "test_data_len=[len(txt.split('|'))  for txt in  raw_data_test]\n",
    "print(max(test_data_len))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "接下來我們將語料去重複後排序，得到所有去重複的字典集合vocabs，各位可以在他的後面看到'\\ueb78', '\\uec95', '\\uecd4'這幾個字元，那其實是表情符號(emoji)，只是python無法正常顯示，但是我認為那些還是有意義地所以予以保留，此外，為了避免出現不在字典裡的字，因此我們在字典的一開始額外插入了未知代表字元('/Unknow')，所有不再字典裡的字都編碼成'/Unknow'。同時我們也加入了＜PAD＞表示將序列填充時的無意義字元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6296\n",
      "['<PAD>', '/Unknow', ' ', '!', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '\\\\', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', '°', '·', '×', 'ˊ', 'ˋ', '˙', 'Α', 'Β', 'Γ', 'Ε', 'Η']\n"
     ]
    }
   ],
   "source": [
    "vocabs=sorted(set(list(''.join(raw_data_train))))\n",
    "vocabs.remove('|')\n",
    "vocabs.insert(0,'/Unknow')\n",
    "vocabs.insert(0,'<PAD>')\n",
    "\n",
    "print(len(vocabs))\n",
    "print(vocabs[:100])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "接下來我們要把vocabs的字以及索引順序編成字元轉索引(char_to_index)，以及索引轉字元(index_to_char)兩種dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'蜢': 4777, '碉': 3747, '諾': 5047, '逍': 5378, '衍': 4842, '湖': 2991, '敉': 2280, '鈞': 5566, '遞': 5425, '裟': 4883, '妨': 1267, '讞': 5095, '炘': 3155, '哩': 850, '借': 412, '宴': 1419, '意': 1871, '蹦': 5261, '旬': 2339, '磨': 3777, '凌': 550, '賠': 5161, '撞': 2210, '●': 137, '戛': 1968, '惴': 1861, '囝': 1046, '魄': 6088, '楣': 2614, '寓': 1433, '篋': 3976, '洫': 2891, '瀋': 3119, '嚨': 1027, '壯': 1196, '蟬': 4819, '蕈': 4656, '卒': 685, 'ㄟ': 182, '捭': 2097, '潔': 3067, '枺': 2501, '捻': 2104, '鉅': 5577, '搓': 2159, '暝': 2403, '霑': 5843, '尾': 1475, '棋': 2572, '幻': 1634, '迸': 5368, '圓': 1065, '噬': 1008, '凝': 553, '熗': 3219, '益': 3619, '胳': 4333, '鐺': 5696, '到': 584, '嚅': 1017, '催': 451, '下': 207, '蟳': 4821, 'z': 86, '好': 1250, '鷸': 6200, '荷': 4543, '件': 298, '鞠': 5882, '膾': 4394, '推': 2127, '鏈': 5673, '赧': 5186, '蠅': 4825, '萄': 4582, '筵': 3952, '呷': 802, '﹑': 6286, '鯤': 6109, '箐': 3958, '滅': 3021, '奘': 1235, '武': 2733, '瘴': 3570, '瀨': 3132, '絞': 4094, '封': 1452, '玕': 3357, '珍': 3376, '風': 5943, '鯰': 6112, '祛': 3806, '薈': 4672, '管': 3964, '俗': 381, '輅': 5309, '夕': 1203, '忽': 1779, '蟹': 4822, '莘': 4554}\n"
     ]
    }
   ],
   "source": [
    "char_to_index=dict((w, i) for i, w in enumerate(vocabs))\n",
    "index_to_char=dict((i, w) for i, w in enumerate(vocabs))\n",
    "\n",
    "print({key:char_to_index[key] for key in list(char_to_index.keys())[:100]})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "接下來就是設計可以讀取minibatch數據的函數了。由於我們模型的第一層是個嵌入層，因此我們的輸入不需要先變成onehot，直接提供各個文字對應的索引即可，因為Embedding層會幫我們內部自動展開成onehot，然後映射至想要的低維空間(256維)。\n",
    "\n",
    "tensorflow是靜態計算圖，所以需要固定序列長度，在此設定為128，長度不足的部分補零(在vocabs的第0個字元就是＜PAD＞)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 128)\n",
      "(2, 128, 4)\n",
      "[13. 12.]\n",
      "[['可是', '我們', '現在', '不', '知道', '為什麼', '﹖'], ['就', '能', '注意到', '這', '座', '山', '的', '特色', ',']]\n"
     ]
    }
   ],
   "source": [
    "idx_train=0\n",
    "idxs_train=np.arange(len(raw_data_train))\n",
    "np.random.shuffle(idxs_train)\n",
    "\n",
    "idx_test=0\n",
    "idxs_test=np.arange(len(raw_data_test))\n",
    "np.random.shuffle(idxs_test)\n",
    "\n",
    "def get_next_minibatch(minibatch_size,is_train=True):\n",
    "    global idx_train,idxs_train,raw_data_train,idx_test,idxs_test,raw_data_test\n",
    "\n",
    "    groundtruths=[]\n",
    "    idx=idx_train\n",
    "    idxs=idxs_train\n",
    "    raw_data=raw_data_train\n",
    "    if is_train==False:\n",
    "        idx=idx_test\n",
    "        idxs=idxs_test\n",
    "        raw_data=raw_data_test\n",
    "    np.random.shuffle(raw_data)\n",
    "    #定義輸出向量形狀\n",
    "    feature_arr=np.zeros((minibatch_size,128))\n",
    "    label_arr=np.zeros((minibatch_size,128,4))\n",
    "    length_arr=np.zeros((minibatch_size))\n",
    "    batch=0\n",
    "    while batch<minibatch_size:\n",
    "        seq_len=len(raw_data[idxs[idx]].replace('|',''))\n",
    "        if seq_len<=128 and seq_len>0:\n",
    "            length_arr[batch]=seq_len\n",
    "            \n",
    "            groundtruth_seq=[]\n",
    "            words=raw_data[idxs[idx]].split('|') #轉換成分詞後的詞清單\n",
    "\n",
    "            pos=0\n",
    "            #BMES=>[0,1,2,3]\n",
    "            for word in words:\n",
    "                for i in range(len(word)):\n",
    "                    #如果在字典中則取出其索引\n",
    "                    if word[i] in char_to_index:\n",
    "                        feature_arr[batch,pos]=char_to_index[word[i]]\n",
    "                    #否則定為未知\n",
    "                    else:\n",
    "                        feature_arr[batch,pos]=char_to_index['/Unknow']\n",
    "\n",
    "                    #轉換為BMES\n",
    "                    if len(word)==1 and i==0: #S 自己就是一個單詞\n",
    "                        label_arr[batch,pos,3]=1\n",
    "                    elif i==0: #B 是一個詞的開始\n",
    "                        label_arr[batch,pos,0]=1\n",
    "                    elif i==len(word)-1:  #E 是一個詞的結束\n",
    "                        label_arr[batch,pos,2]=1\n",
    "                    else: #M 是一個詞的中間\n",
    "                        label_arr[batch,pos,1]=1  \n",
    "                    pos+=1\n",
    "\n",
    "                groundtruth_seq.append(word)\n",
    "            groundtruths.append(groundtruth_seq)\n",
    "            batch+=1\n",
    "        idx+=1\n",
    "        if idx>len(idxs):\n",
    "            idx=0\n",
    "            np.random.shuffle(idxs)\n",
    "    idx_train=idx\n",
    "    if is_train==False:\n",
    "        idx_test=idx\n",
    "    return feature_arr.astype(np.float32),label_arr.astype(np.float32),length_arr.astype(np.float32),groundtruths\n",
    "        \n",
    "x_feature,y_label,x_len,ground_truths=get_next_minibatch(2)\n",
    "print(x_feature.shape)\n",
    "print(y_label.shape)\n",
    "print(x_len)\n",
    "print(ground_truths)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在準備好數據之後，我們來說明一下模型的架構，在這邊我們預計要使用的是雙向的lstm，因為對於分詞來說，上下文都是很重要的語意線索來源，模型結構如下：\n",
    "\n",
    "     +-----------+   +------------------+              +------------+\n",
    "x -->| Embedding |-->| BiRecurrent LSTM |--> dropout-->| DenseLayer |--> y\n",
    "     +-----------+   +------------------+              +------------+\n",
    "     \n",
    "這裡的輸入x是一個(批次,序列長)的張量，透過embedding層將輸入內部展開為onehot後再濃縮為長度256的字嵌入，然後輸入一個兩層的雙向lstm(隱藏層形狀為256，由於是雙向所以實際輸出是512)，最後透過dropout後送入全連接層，輸出為長度為4的onehot向量(BMES)。\n",
    "\n",
    "至於如何在固定的128長度序列中不至於誤把補零的區域納入損失函數計算呢?在KERAS中只要於Embedding層指定mask_zero=True就可以了。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 128, 256)          1611776   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128, 512)          1050624   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128, 512)          1574912   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128, 512)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128, 4)            2052      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128, 4)            0         \n",
      "=================================================================\n",
      "Total params: 4,239,364\n",
      "Trainable params: 4,239,364\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Build model...')\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vocabs),256,input_length=128,mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "if not os.path.exists('Models'):\n",
    "    os.mkdir('Models')\n",
    "    print(\"Directory Models Created \")\n",
    "    \n",
    "if os.path.exists('Models/word_segment_keras.h5'):\n",
    "    model.load_weights('Models/word_segment_keras.h5')\n",
    "    print('load model')\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----測試集驗證--------\n",
      "predict:長|實|(|0|0|0|1|)執|行|董|事|趙|國|雄|指|出|,|樓|市目前仍處鞏固期,發展商再度提供優|惠|或|二|按等促銷樓|盤|是|可以|理|解|的|。|\n",
      "answer :長實|(|0001|)|執行|董事|趙國雄|指出|,|樓市|目前|仍|處|鞏固期|,|發展商|再度|提供|優惠|或|二按|等|促銷|樓盤|是|可以|理解|的|。|\n",
      "predict:十|一|年|之|間|成|長|五|點|三|倍|;|\n",
      "answer :十一|年|之間|成長|五點三|倍|;|\n",
      "predict:「小森|先|生|,|\n",
      "answer :「|小森|先生|,|\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "num_epochs=3\n",
    "minibatch_size=32\n",
    "for epoch in range(num_epochs):\n",
    "    mbs = 0\n",
    "    rows = 0\n",
    "    while mbs <1000:\n",
    "        try:\n",
    "            raw_features, raw_label,raw_len,raw_ground_truths = get_next_minibatch(minibatch_size)\n",
    "            \n",
    "            loss=model.train_on_batch(raw_features, raw_label)\n",
    "            #每100次存一次檔\n",
    "            if (mbs+1) % 50 == 0 :\n",
    "                print('epoch {0}/{1} {2}-{3}  loss:{4:.3} accuracy:{5:.2%}  '.format(epoch,num_epochs,mbs+1-50,mbs+1,loss[0],loss[1]))\n",
    "                model.save_weights('Models/word_segment_keras.hdf5')\n",
    "            if mbs%200==0 :\n",
    "                features_test, labels_test,len_test,ground_truths_test=get_next_minibatch(minibatch_size,False)\n",
    "                print('-----測試集驗證--------')\n",
    "                result=model.predict(features_test)\n",
    "                for i in range(3):\n",
    "                    answer='|'.join(ground_truths_test[i])+'|'\n",
    "                    pred=[]\n",
    "                    words=list(''.join(ground_truths_test[i]))\n",
    "                    for j in range(len(words)):\n",
    "                        word=words[j]\n",
    "                        onehot=np.argmax(result[i][j])\n",
    "                        if onehot>=2:\n",
    "                            pred.append(word+'|')\n",
    "                        else:\n",
    "                            pred.append(word)\n",
    "\n",
    "                    pred=''.join(pred)  \n",
    "                    print('predict:'+pred)\n",
    "                    print('answer :'+answer)\n",
    "                print('-----------------')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        mbs += 1\n",
    "\n",
    "    \n",
    "    model.save_weights('Models/word_segment_keras.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
